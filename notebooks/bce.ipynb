{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLAB\n",
    "!pip install neptune-client\n",
    "# pip install torch-tensorrt -f https://github.com/NVIDIA/Torch-TensorRT/releases\n",
    "!unzip data.zip\n",
    "!mkdir artifacts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "import torch\n",
    "from torch import einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from torchvision import transforms as T\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import neptune.new as neptune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/victorcallejas/Belluga/e/BEL-162\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"victorcallejas/Belluga\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlNDRlNTJiNC00OTQwLTQxYjgtYWZiNS02OWQ0MDcwZmU5N2YifQ==\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1070 with Max-Q Design _CudaDeviceProperties(name='NVIDIA GeForce GTX 1070 with Max-Q Design', major=6, minor=1, total_memory=8191MB, multi_processor_count=16)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(torch.cuda.get_device_name(0), torch.cuda.get_device_properties(device))\n",
    "\n",
    "fp16 = False\n",
    "input_dtype = torch.float16 if fp16 else torch.float32\n",
    "\n",
    "scaler =  torch.cuda.amp.GradScaler(enabled=fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCORING\n",
    "PREDICTION_LIMIT = 20\n",
    "QUERY_ID_COL = \"query_id\"\n",
    "DATABASE_ID_COL = \"database_image_id\"\n",
    "SCORE_COL = \"score\"\n",
    "\n",
    "SCORE_THRESHOLD = 0.5\n",
    "\n",
    "class MeanAveragePrecision:\n",
    "    @classmethod\n",
    "    def score(cls, predicted: pd.DataFrame, actual: pd.DataFrame, prediction_limit: int):\n",
    "        \"\"\"Calculates mean average precision for a ranking task.\n",
    "        :param predicted: The predicted values as a dataframe with specified column names\n",
    "        :param actual: The ground truth values as a dataframe with specified column names\n",
    "        \"\"\"\n",
    "        if not predicted[SCORE_COL].between(0.0, 1.0).all():\n",
    "            raise ValueError(\"Scores must be in range [0, 1].\")\n",
    "        if predicted.index.name != QUERY_ID_COL:\n",
    "            raise ValueError(\n",
    "                f\"First column of submission must be named '{QUERY_ID_COL}', \"\n",
    "                f\"got {predicted.index.name}.\"\n",
    "            )\n",
    "        if predicted.columns.to_list() != [DATABASE_ID_COL, SCORE_COL]:\n",
    "            raise ValueError(\n",
    "                f\"Columns of submission must be named '{[DATABASE_ID_COL, SCORE_COL]}', \"\n",
    "                f\"got {predicted.columns.to_list()}.\"\n",
    "            )\n",
    "\n",
    "        unadjusted_aps, predicted_n_pos, actual_n_pos = cls._score_per_query(\n",
    "            predicted, actual, prediction_limit\n",
    "        )\n",
    "        adjusted_aps = unadjusted_aps.multiply(predicted_n_pos).divide(actual_n_pos)\n",
    "        return adjusted_aps.mean()\n",
    "\n",
    "    @classmethod\n",
    "    def _score_per_query(\n",
    "        cls, predicted: pd.DataFrame, actual: pd.DataFrame, prediction_limit: int\n",
    "    ):\n",
    "        \"\"\"Calculates per-query mean average precision for a ranking task.\"\"\"\n",
    "        merged = predicted.merge(\n",
    "            right=actual.assign(actual=1.0),\n",
    "            how=\"left\",\n",
    "            on=[QUERY_ID_COL, DATABASE_ID_COL],\n",
    "        ).fillna({\"actual\": 0.0})\n",
    "        # Per-query raw average precisions based on predictions\n",
    "        unadjusted_aps = merged.groupby(QUERY_ID_COL).apply(\n",
    "            lambda df: average_precision_score(df[\"actual\"].values, df[SCORE_COL].values)\n",
    "            if df[\"actual\"].sum()\n",
    "            else 0.0\n",
    "        )\n",
    "        # Total ground truth positive counts for rescaling\n",
    "        predicted_n_pos = merged[\"actual\"].groupby(QUERY_ID_COL).sum().astype(\"int64\").rename()\n",
    "        actual_n_pos = actual.groupby(QUERY_ID_COL).size().clip(upper=prediction_limit)\n",
    "        return unadjusted_aps, predicted_n_pos, actual_n_pos\n",
    "    \n",
    "    \n",
    "def map_score(dataloader, model, threshold=SCORE_THRESHOLD):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    sub = []\n",
    "    \n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    with torch.no_grad():        \n",
    "    \n",
    "        for query, reference, query_id, reference_id in tqdm(dataloader):\n",
    "            \n",
    "            query = query.to(device, non_blocking=True, dtype=input_dtype)\n",
    "            reference = reference.to(device, non_blocking=True, dtype=input_dtype)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled = fp16):\n",
    "                logits = sigmoid(model(query=query, reference=reference)).cpu().squeeze().tolist()\n",
    "                \n",
    "            sub.extend(zip(query_id, reference_id, logits))\n",
    "            \n",
    "    sub = pd.DataFrame(sub, columns=['query_id', 'database_image_id', 'score'])\n",
    "    sub = sub[sub.score > threshold]\n",
    "    sub = sub.set_index(['database_image_id']).groupby('query_id')['score'].nlargest(20).reset_index()\n",
    "    sub = sub.set_index('query_id')\n",
    "    \n",
    "    mean_avg_prec = MeanAveragePrecision.score(\n",
    "        predicted=sub, actual=dataloader.dataset.gt, prediction_limit=PREDICTION_LIMIT\n",
    "    )\n",
    "    \n",
    "    print('MaP: ',mean_avg_prec)\n",
    "    return mean_avg_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5902/5902 [02:23<00:00, 40.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# DATA\n",
    "\n",
    "IMG_SIZE = 224\n",
    "ROOT_DIR = '../data/'\n",
    "NORM_TRANSFORMS = torch.nn.Sequential(\n",
    "    T.Resize([IMG_SIZE, IMG_SIZE]),\n",
    "    T.ConvertImageDtype(input_dtype),\n",
    "    T.Normalize(mean = (0.4234, 0.4272, 0.4641),\n",
    "                std  = (0.2037, 0.2027, 0.2142)),\n",
    ")\n",
    "\n",
    "VAL_SPLIT = 0.05\n",
    "\n",
    "METADATA = pd.read_csv('../data/metadata.csv')\n",
    "\n",
    "TRAIN, VAL = train_test_split(METADATA, test_size=0.05, random_state=42)\n",
    "TRAIN, VAL = TRAIN.reset_index(), VAL.reset_index()\n",
    "#TRAIN, VAL = METADATA, METADATA\n",
    "#TRAIN = METADATA\n",
    "\n",
    "def getImages(metadata):\n",
    "    IMAGES = {}\n",
    "    for image_id, path in tqdm(zip(metadata.image_id, metadata.path), total=metadata.shape[0]):\n",
    "        IMAGES[image_id] = NORM_TRANSFORMS(read_image(ROOT_DIR + path))\n",
    "    return IMAGES\n",
    "\n",
    "IMAGES = getImages(METADATA)\n",
    "\n",
    "class PreTrain_BellugaDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, metadata):\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.metadata.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return IMAGES[self.metadata.image_id[idx]]\n",
    "\n",
    "\n",
    "class Eval_BellugaDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, metadata):\n",
    "        self.metadata = metadata\n",
    "    \n",
    "        # GROUND TRUTH\n",
    "        gt = []\n",
    "        for wid in self.metadata.whale_id: # query\n",
    "            tmp = self.metadata[self.metadata.whale_id == wid].image_id.tolist() # get all images id\n",
    "            gt.extend(list(itertools.permutations(tmp, 2)))\n",
    "        self.gt = pd.DataFrame(gt,columns=['query_id','database_image_id'])\n",
    "        self.gt = self.gt.set_index('query_id')\n",
    "        \n",
    "        # ALL QUERIES\n",
    "        self.query_reference = list(itertools.permutations(self.metadata.image_id, 2))\n",
    "            \n",
    "    def getimage(self, image_id):\n",
    "        return IMAGES[image_id]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.query_reference)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        query_id = self.query_reference[idx][0]\n",
    "        reference_id = self.query_reference[idx][1]\n",
    "        \n",
    "        query = self.getimage(query_id)\n",
    "        reference = self.getimage(reference_id)\n",
    "        \n",
    "        return query, reference, query_id, reference_id\n",
    "    \n",
    "    \n",
    "class Train_BellugaDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, metadata):\n",
    "        self.metadata = metadata\n",
    "        self.aug = T.RandomErasing(p=0.4, scale=(0.12, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n",
    "            \n",
    "    def getimage(self, image_id):\n",
    "        return IMAGES[image_id]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.metadata.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        anchor = self.aug(self.getimage(self.metadata.image_id[idx]))\n",
    "        label = self.metadata.whale_id[idx]\n",
    "        \n",
    "        pos = self.aug(self.getimage(self.metadata[self.metadata.whale_id == label].sample()['image_id'].values[0]))\n",
    "        neg = self.aug(self.getimage(self.metadata[self.metadata.whale_id != label].sample()['image_id'].values[0]))\n",
    "\n",
    "        return anchor, pos, neg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATALOADERS \n",
    "\n",
    "PRETRAIN_BS = 6\n",
    "TRAIN_BS = 64\n",
    "INFER_BS = TRAIN_BS\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "\n",
    "#pretrain_dataset = PreTrain_BellugaDataset(METADATA)\n",
    "train_train_dataset = Train_BellugaDataset(TRAIN)\n",
    "#train_eval_dataset = Eval_BellugaDataset(TRAIN)\n",
    "valid_eval_dataset = Eval_BellugaDataset(VAL)\n",
    "\n",
    "'''\n",
    "pretrain_dataloader = torch.utils.data.DataLoader(\n",
    "                        pretrain_dataset, \n",
    "                        batch_size=PRETRAIN_BS,\n",
    "                        shuffle=True, \n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        pin_memory=True\n",
    "                    )\n",
    "'''\n",
    "train_train_dataloader = torch.utils.data.DataLoader(\n",
    "                        train_train_dataset, \n",
    "                        batch_size=TRAIN_BS,\n",
    "                        shuffle=True, \n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        pin_memory=True\n",
    "                    )\n",
    "'''\n",
    "train_eval_dataloader = torch.utils.data.DataLoader(\n",
    "                        train_eval_dataset, \n",
    "                        batch_size=INFER_BS,\n",
    "                        shuffle=True, \n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        pin_memory=True\n",
    "                    )\n",
    "'''\n",
    "valid_eval_dataloader = torch.utils.data.DataLoader(\n",
    "                        valid_eval_dataset, \n",
    "                        batch_size=INFER_BS,\n",
    "                        shuffle=False, \n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        pin_memory=True\n",
    "                    )   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "# pre-layernorm\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "# feedforward\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context = None, kv_include_self = False):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        context = default(context, x)\n",
    "\n",
    "        if kv_include_self:\n",
    "            context = torch.cat((x, context), dim = 1) # cross attention requires CLS token includes itself as key / value\n",
    "\n",
    "        qkv = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "# transformer encoder, for qall and large patches\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)\n",
    "\n",
    "# projecting CLS tokens, in the case that qall and large patch tokens have different dimensions\n",
    "class ProjectInOut(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "        need_projection = dim_in != dim_out\n",
    "        self.project_in = nn.Linear(dim_in, dim_out) if need_projection else nn.Identity()\n",
    "        self.project_out = nn.Linear(dim_out, dim_in) if need_projection else nn.Identity()\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self.project_in(x)\n",
    "        x = self.fn(x, *args, **kwargs)\n",
    "        x = self.project_out(x)\n",
    "        return x\n",
    "\n",
    "# cross attention transformer\n",
    "class CrossTransformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(ProjectInOut(dim, dim, PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout))))\n",
    "\n",
    "    def forward(self, q_tokens, ref_tokens):\n",
    "        (q_cls, q_patch_tokens), (ref_cls, ref_patch_tokens) = map(lambda t: (t[:, :1], t[:, 1:]), (q_tokens, ref_tokens))\n",
    "        for attend in self.layers:\n",
    "            q_cls = attend(q_cls, context = ref_patch_tokens, kv_include_self = True) + q_cls\n",
    "            ref_cls = attend(ref_cls, context = q_patch_tokens, kv_include_self = True) + ref_cls\n",
    "            \n",
    "        q_tokens = torch.cat((q_cls, q_patch_tokens), dim = 1)\n",
    "        ref_tokens = torch.cat((ref_cls, ref_patch_tokens), dim = 1)\n",
    "        \n",
    "        return q_tokens, ref_tokens\n",
    "\n",
    "# multi-scale encoder\n",
    "class MultiScaleEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        enc_depth,\n",
    "        dim,\n",
    "        enc_params,\n",
    "        cross_attn_heads,\n",
    "        cross_attn_depth,\n",
    "        cross_attn_dim_head = 64,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for enc_d, cross_d in zip(enc_depth, cross_attn_depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Transformer(dim = dim, dropout = dropout, depth = enc_d, **enc_params),\n",
    "                CrossTransformer(dim = dim, depth = cross_d, heads = cross_attn_heads, dim_head = cross_attn_dim_head, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, q_tokens, ref_tokens):\n",
    "        for enc, cross_attend in self.layers:\n",
    "            q_tokens, ref_tokens = enc(q_tokens), enc(ref_tokens)\n",
    "            q_tokens, ref_tokens = cross_attend(q_tokens, ref_tokens)\n",
    "\n",
    "        return q_tokens, ref_tokens\n",
    "\n",
    "# patch-based image to token embedder\n",
    "class ImageEmbedder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        image_size,\n",
    "        patch_size,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = 3 * patch_size ** 2\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "\n",
    "        return self.dropout(x)\n",
    "\n",
    "# cross ViT class\n",
    "class CrossViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        image_size=224,\n",
    "        num_classes=1,\n",
    "        dim=192,\n",
    "        patch_size = 16,\n",
    "        enc_depth = [2,1,1],\n",
    "        enc_heads = 8,\n",
    "        enc_mlp_dim = 2048,\n",
    "        enc_dim_head = 64,\n",
    "        cross_attn_depth = [1,1,2],\n",
    "        cross_attn_heads = 12,\n",
    "        cross_attn_dim_head = 64,\n",
    "        dropout = 0.2,\n",
    "        emb_dropout = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_embedder = ImageEmbedder(dim = dim, image_size = image_size, patch_size = patch_size, dropout = emb_dropout)\n",
    "\n",
    "        self.multi_scale_encoder = MultiScaleEncoder(\n",
    "            dim = dim,\n",
    "            enc_depth = enc_depth,\n",
    "            cross_attn_heads = cross_attn_heads,\n",
    "            cross_attn_dim_head = cross_attn_dim_head,\n",
    "            cross_attn_depth = cross_attn_depth,\n",
    "            enc_params = dict(\n",
    "                heads = enc_heads,\n",
    "                mlp_dim = enc_mlp_dim,\n",
    "                dim_head = enc_dim_head\n",
    "            ),\n",
    "            dropout = dropout\n",
    "        )\n",
    "\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, num_classes))\n",
    "\n",
    "    def forward(self, query, reference):\n",
    "        q_tokens = self.image_embedder(query)\n",
    "        ref_tokens = self.image_embedder(reference)\n",
    "\n",
    "        q_tokens, ref_tokens = self.multi_scale_encoder(q_tokens, ref_tokens)\n",
    "\n",
    "        q_cls, ref_cls = map(lambda t: t[:, 0], (q_tokens, ref_tokens))\n",
    "\n",
    "        #cls = torch.cat([q_cls, ref_cls], dim=1)\n",
    "        cls = q_cls + ref_cls\n",
    "        logits = self.mlp_head(cls)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "\n",
    "def crossvit_base_224():\n",
    "    \n",
    "    return CrossViT(\n",
    "        image_size=224,\n",
    "        num_classes=1,\n",
    "        dim=192,\n",
    "        patch_size = 32,\n",
    "        enc_depth = [1,1],\n",
    "        enc_heads = 8,\n",
    "        enc_mlp_dim = 2048,\n",
    "        enc_dim_head = 64,\n",
    "        cross_attn_depth = [1,1],\n",
    "        cross_attn_heads = 12,\n",
    "        cross_attn_dim_head = 64,\n",
    "        dropout = 0.2,\n",
    "        emb_dropout = 0.1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = crossvit_base_224().to(device)\n",
    "\n",
    "#ckpt = torch.load('/kaggle/input/ckptttt/net (3).pt')\n",
    "#model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-5)\n",
    "#optimizer.load_state_dict(ckpt['optimizer_state_dict'], )\n",
    "\n",
    "#opt = torch.optim.SGD(model.parameters(), lr = .05)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossViT(\n",
       "  (image_embedder): ImageEmbedder(\n",
       "    (to_patch_embedding): Sequential(\n",
       "      (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=16, p2=16)\n",
       "      (1): Linear(in_features=768, out_features=192, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (multi_scale_encoder): MultiScaleEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): Transformer(\n",
       "          (layers): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (attend): Softmax(dim=-1)\n",
       "                  (dropout): Dropout(p=0.2, inplace=False)\n",
       "                  (to_q): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=192, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=192, bias=True)\n",
       "                    (1): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=192, out_features=2048, bias=True)\n",
       "                    (1): GELU()\n",
       "                    (2): Dropout(p=0.2, inplace=False)\n",
       "                    (3): Linear(in_features=2048, out_features=192, bias=True)\n",
       "                    (4): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): CrossTransformer(\n",
       "          (layers): ModuleList(\n",
       "            (0): ProjectInOut(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (attend): Softmax(dim=-1)\n",
       "                  (dropout): Dropout(p=0.2, inplace=False)\n",
       "                  (to_q): Linear(in_features=192, out_features=768, bias=False)\n",
       "                  (to_kv): Linear(in_features=192, out_features=1536, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "                    (1): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (project_in): Identity()\n",
       "              (project_out): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): Transformer(\n",
       "          (layers): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (attend): Softmax(dim=-1)\n",
       "                  (dropout): Dropout(p=0.2, inplace=False)\n",
       "                  (to_q): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=192, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=192, bias=True)\n",
       "                    (1): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=192, out_features=2048, bias=True)\n",
       "                    (1): GELU()\n",
       "                    (2): Dropout(p=0.2, inplace=False)\n",
       "                    (3): Linear(in_features=2048, out_features=192, bias=True)\n",
       "                    (4): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): CrossTransformer(\n",
       "          (layers): ModuleList(\n",
       "            (0): ProjectInOut(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (attend): Softmax(dim=-1)\n",
       "                  (dropout): Dropout(p=0.2, inplace=False)\n",
       "                  (to_q): Linear(in_features=192, out_features=768, bias=False)\n",
       "                  (to_kv): Linear(in_features=192, out_features=1536, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "                    (1): Dropout(p=0.2, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (project_in): Identity()\n",
       "              (project_out): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp_head): Sequential(\n",
       "    (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=192, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 21/88 [01:03<03:21,  3.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\repos\\belugas\\notebooks\\bce.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce.ipynb#ch0000011?line=25'>26</a>\u001b[0m scaler\u001b[39m.\u001b[39mstep(optimizer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce.ipynb#ch0000011?line=26'>27</a>\u001b[0m scaler\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce.ipynb#ch0000011?line=28'>29</a>\u001b[0m run[\u001b[39m'\u001b[39;49m\u001b[39mrunning/loss\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mlog(loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce.ipynb#ch0000011?line=30'>31</a>\u001b[0m \u001b[39m# accuracy\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce.ipynb#ch0000011?line=31'>32</a>\u001b[0m preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mSigmoid()(logits)\u001b[39m.\u001b[39mround()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\neptune\\new\\handler.py:66\u001b[0m, in \u001b[0;36mcheck_protected_paths.<locals>.inner_fun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/handler.py?line=61'>62</a>\u001b[0m \u001b[39m@wraps\u001b[39m(fun)\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/handler.py?line=62'>63</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner_fun\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mHandler\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/handler.py?line=63'>64</a>\u001b[0m     \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/handler.py?line=64'>65</a>\u001b[0m     validate_path_not_protected(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path, \u001b[39mself\u001b[39m)\n\u001b[1;32m---> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/handler.py?line=65'>66</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m fun(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\neptune\\new\\handler.py:282\u001b[0m, in \u001b[0;36mHandler.log\u001b[1;34m(self, value, step, timestamp, wait, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/handler.py?line=276'>277</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/handler.py?line=277'>278</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mValue of unsupported type \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(first_value))\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/handler.py?line=278'>279</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/handler.py?line=280'>281</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container\u001b[39m.\u001b[39mset_attribute(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path, attr)\n\u001b[1;32m--> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/handler.py?line=281'>282</a>\u001b[0m attr\u001b[39m.\u001b[39mlog(value, step\u001b[39m=\u001b[39mstep, timestamp\u001b[39m=\u001b[39mtimestamp, wait\u001b[39m=\u001b[39mwait, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\neptune\\new\\attributes\\series\\series.py:88\u001b[0m, in \u001b[0;36mSeries.log\u001b[1;34m(self, value, step, timestamp, wait, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/attributes/series/series.py?line=85'>86</a>\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_to_value(value, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/attributes/series/series.py?line=86'>87</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/attributes/series/series.py?line=87'>88</a>\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_to_value([value], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/attributes/series/series.py?line=89'>90</a>\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/attributes/series/series.py?line=90'>91</a>\u001b[0m     verify_type(\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m, step, (\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m))\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\neptune\\new\\attributes\\series\\float_series.py:75\u001b[0m, in \u001b[0;36mFloatSeries._data_to_value\u001b[1;34m(self, values, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/attributes/series/float_series.py?line=67'>68</a>\u001b[0m \u001b[39mif\u001b[39;00m kwargs:\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/attributes/series/float_series.py?line=68'>69</a>\u001b[0m     click\u001b[39m.\u001b[39mecho(\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/attributes/series/float_series.py?line=69'>70</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWarning: unexpected arguments (\u001b[39m\u001b[39m{kwargs}\u001b[39;00m\u001b[39m) in FloatSeries\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/attributes/series/float_series.py?line=70'>71</a>\u001b[0m             kwargs\u001b[39m=\u001b[39mkwargs\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/attributes/series/float_series.py?line=71'>72</a>\u001b[0m         ),\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/attributes/series/float_series.py?line=72'>73</a>\u001b[0m         err\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/attributes/series/float_series.py?line=73'>74</a>\u001b[0m     )\n\u001b[1;32m---> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/attributes/series/float_series.py?line=74'>75</a>\u001b[0m \u001b[39mreturn\u001b[39;00m FloatSeriesVal(values)\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\neptune\\new\\types\\series\\float_series.py:40\u001b[0m, in \u001b[0;36mFloatSeries.__init__\u001b[1;34m(self, values, min, max, unit)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/types/series/float_series.py?line=37'>38</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_collection(values):\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/types/series/float_series.py?line=38'>39</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`values` is not a collection\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/types/series/float_series.py?line=39'>40</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values \u001b[39m=\u001b[39m [\u001b[39mfloat\u001b[39m(value) \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m values]\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/types/series/float_series.py?line=40'>41</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_min \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/types/series/float_series.py?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\neptune\\new\\types\\series\\float_series.py:40\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/types/series/float_series.py?line=37'>38</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_collection(values):\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/types/series/float_series.py?line=38'>39</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`values` is not a collection\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/types/series/float_series.py?line=39'>40</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values \u001b[39m=\u001b[39m [\u001b[39mfloat\u001b[39;49m(value) \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m values]\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/types/series/float_series.py?line=40'>41</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_min \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/neptune/new/types/series/float_series.py?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    epoch_loss, epoch_acc = 0, 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for anchor, pos, neg in tqdm(train_train_dataloader):\n",
    "\n",
    "        optimizer.zero_grad(True)\n",
    "        \n",
    "        anchor = anchor.to(device, non_blocking=True, dtype=input_dtype)\n",
    "        pos = pos.to(device, non_blocking=True, dtype=input_dtype)\n",
    "        neg = neg.to(device, non_blocking=True, dtype=input_dtype)\n",
    "        \n",
    "        query = torch.cat([anchor, anchor], dim=0)\n",
    "        reference = torch.cat([pos, neg], dim=0)\n",
    "        labels = torch.cat([torch.ones(pos.shape[0],1), torch.zeros(neg.shape[0],1)], dim=0).to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=fp16):\n",
    "            logits = model(query=query, reference=reference)\n",
    "            loss = loss_fn(logits, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        run['running/loss'].log(loss)\n",
    "        \n",
    "        # accuracy\n",
    "        preds = torch.nn.Sigmoid()(logits).round().detach().cpu().numpy()\n",
    "        acc = accuracy_score(labels.detach().cpu().numpy(), preds)\n",
    "        run['running/acc'].log(acc)\n",
    "        \n",
    "        epoch_loss += loss\n",
    "        epoch_acc += acc\n",
    "        \n",
    "    run['epoch/train/loss'].log(epoch_loss / len(train_train_dataloader))\n",
    "    run['epoch/train/acc'].log(epoch_acc / len(train_train_dataloader))\n",
    "    \n",
    "    if epoch_i % 20 == 0:\n",
    "        map = map_score(valid_eval_dataloader, model)\n",
    "        run['epoch5/valid/map'].log(map)\n",
    "        \n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, f'/kaggle/working/artifacts/net.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, f'../artifacts/net_{epoch_i}_83acc.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal batch size for inference\n",
    "\"\"\"\n",
    "5vcpus\n",
    "52 ram\n",
    "12 ram gpu\n",
    "\n",
    "model to eval, optim bactch size calculate with tqdm\n",
    "\"\"\"\n",
    "model.eval()\n",
    "\n",
    "TEST_INFER_BS_INIT = 10\n",
    "TEST_INFER_BS_ML = 10\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "class DummyTest(torch.utils.data.Dataset):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def __len__(self):\n",
    "    return 7000000\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return torch.zeros((3,224,224)), torch.zeros((3,224,224))\n",
    "\n",
    "BS = TEST_INFER_BS_INIT = 5\n",
    "while True:\n",
    "  i = 0\n",
    "  BS = BS + TEST_INFER_BS_ML\n",
    "  dataloader = torch.utils.data.DataLoader(\n",
    "                          DummyTest(), \n",
    "                          batch_size=BS,\n",
    "                          shuffle=False, \n",
    "                          num_workers=0,\n",
    "                          pin_memory=True\n",
    "                      )   \n",
    "\n",
    "  for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "    query = batch[0].to(device, non_blocking=True, dtype=torch.float32)\n",
    "    reference = batch[1].to(device, non_blocking=True, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "      logits, attn, q_cls, r_cls = model(query=query, reference=reference)\n",
    "      i+=1\n",
    "    \n",
    "    if i == 50:\n",
    "      print(BS)\n",
    "      print(torch.cuda.mem_get_info(device=0))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6465060864, 8589737984)\n"
     ]
    }
   ],
   "source": [
    "fp16 = True \n",
    "input_dtype = torch.float16 if fp16 else torch.float32\n",
    "\n",
    "scaler =  torch.cuda.amp.GradScaler(enabled=fp16)\n",
    "\n",
    "model = crossvit_base_224().to(device)\n",
    "input = torch.zeros((2,3,224,224), dtype=input_dtype, device=device)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "labels = torch.ones((2,1), dtype=input_dtype, device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-4)\n",
    "\n",
    "warmup, reps = 30, 10\n",
    "\n",
    "for i in range(0, warmup):\n",
    "    \n",
    "        with torch.cuda.amp.autocast(enabled = fp16):\n",
    "            logits, attn, q_cls, r_cls = model(query=input, reference=input)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    profile_memory=True,\n",
    ") as prof:\n",
    "    print(torch.cuda.mem_get_info(0))\n",
    "    for i in range(0, reps):\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled = fp16):\n",
    "            \n",
    "            logits, attn, q_cls, r_cls = model(query=input, reference=input)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          aten::reshape         5.00%     566.319ms        11.90%        1.346s     153.356us     544.024ms         4.84%        1.311s     149.324us           0 b           0 b     542.46 Mb           0 b          8780  \n",
      "                                            aten::copy_         4.72%     534.034ms         4.72%     534.034ms      78.650us     525.057ms         4.67%     525.057ms      77.328us           0 b           0 b           0 b           0 b          6790  \n",
      "                                         aten::_to_copy         4.56%     516.087ms        10.33%        1.170s     227.134us     503.597ms         4.48%        1.151s     223.463us           0 b           0 b       2.03 Gb           0 b          5150  \n",
      "                                       aten::as_strided         4.20%     475.203ms         4.20%     475.203ms      29.297us     453.851ms         4.04%     453.851ms      27.981us           0 b           0 b           0 b           0 b         16220  \n",
      "                                               aten::mm         4.04%     457.166ms         4.04%     457.166ms     155.499us     510.154ms         4.54%     510.154ms     173.522us           0 b           0 b       1.28 Gb       1.28 Gb          2940  \n",
      "                                          aten::permute         3.78%     427.307ms         5.55%     628.754ms      92.737us     439.648ms         3.91%     627.262ms      92.517us           0 b           0 b           0 b           0 b          6780  \n",
      "                                            aten::empty         3.60%     407.873ms         3.60%     407.873ms      74.159us     403.471ms         3.59%     403.471ms      73.358us           0 b           0 b       1.35 Gb       1.35 Gb          5500  \n",
      "                                        aten::transpose         3.07%     347.453ms         4.57%     517.742ms      87.902us     344.904ms         3.07%     519.313ms      88.169us           0 b           0 b           0 b           0 b          5890  \n",
      "                                    aten::empty_strided         2.93%     331.262ms         2.93%     331.262ms      50.420us     331.970ms         2.96%     331.970ms      50.528us           0 b           0 b       2.72 Gb       2.72 Gb          6570  \n",
      "                                                aten::t         2.80%     316.616ms         6.63%     750.914ms     152.315us     320.957ms         2.86%     755.833ms     153.313us           0 b           0 b           0 b           0 b          4930  \n",
      "                                   aten::_reshape_alias         2.50%     283.267ms         2.50%     283.267ms      37.469us     283.366ms         2.52%     283.366ms      37.482us           0 b           0 b           0 b           0 b          7560  \n",
      "                                               aten::to         2.50%     282.847ms        12.83%        1.453s     280.964us     295.169ms         2.63%        1.446s     279.691us           0 b           0 b       2.03 Gb           0 b          5170  \n",
      "                                             aten::add_         2.13%     240.690ms         2.13%     240.690ms      81.590us     244.176ms         2.17%     244.176ms      82.772us           0 b           0 b           0 b           0 b          2950  \n",
      "                                           aten::linear         2.09%     236.620ms        19.31%        2.186s       1.104ms     238.288ms         2.12%        2.180s       1.101ms           0 b           0 b       1.57 Gb           0 b          1980  \n",
      "                                              aten::bmm         2.00%     226.344ms         2.87%     324.686ms     193.265us     220.517ms         1.96%     317.646ms     189.075us           0 b           0 b     852.90 Mb     642.09 Mb          1680  \n",
      "                                       aten::empty_like         1.98%     223.804ms         4.65%     526.345ms     147.024us     220.125ms         1.96%     521.439ms     145.653us           0 b           0 b       1.79 Gb           0 b          3580  \n",
      "autograd::engine::evaluate_function: PermuteBackward...         1.94%     219.509ms         6.43%     727.753ms     215.312us     220.368ms         1.96%     725.153ms     214.542us           0 b           0 b           0 b           0 b          3380  \n",
      "                                           aten::einsum         1.77%     200.538ms         9.10%        1.030s       2.147ms     224.760ms         2.00%        1.032s       2.150ms           0 b           0 b     652.13 Mb           0 b           480  \n",
      "autograd::engine::evaluate_function: ToCopyBackward0...         1.77%     200.472ms         9.14%        1.034s     438.266us     193.696ms         1.72%        1.032s     437.249us           0 b           0 b     725.12 Mb    -703.96 Mb          2360  \n",
      "                                       PermuteBackward0         1.73%     195.666ms         4.49%     508.244ms     150.368us     188.534ms         1.68%     504.785ms     149.345us           0 b           0 b           0 b           0 b          3380  \n",
      "                 struct torch::autograd::AccumulateGrad         1.70%     192.112ms         3.38%     382.221ms     159.259us     187.491ms         1.67%     381.166ms     158.819us           0 b           0 b    -973.02 Mb    -973.02 Mb          2400  \n",
      "                                             aten::view         1.64%     185.266ms         1.64%     185.266ms      33.624us     184.722ms         1.64%     184.722ms      33.525us           0 b           0 b           0 b           0 b          5510  \n",
      "                                            MmBackward0         1.63%     184.982ms         8.02%     907.421ms     925.940us     187.506ms         1.67%     942.223ms     961.452us           0 b           0 b     746.62 Mb           0 b           980  \n",
      "autograd::engine::evaluate_function: ReshapeAliasBac...         1.36%     154.437ms         5.63%     636.799ms     289.454us     153.435ms         1.37%     635.240ms     288.745us           0 b           0 b           0 b    -161.60 Mb          2200  \n",
      "autograd::engine::evaluate_function: struct torch::a...         1.32%     149.725ms         4.70%     531.946ms     221.644us     140.895ms         1.25%     522.061ms     217.525us           0 b           0 b    -973.02 Mb           0 b          2400  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 11.319s\n",
      "Self CUDA time total: 11.233s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 1, 197])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layers x bs x (q,ref) x cross_attn_depth x n_heads x 1 x tokens(inc cls)\n",
    "attn[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 1, 197])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(attn))\n",
    "print(len(attn[0]))\n",
    "print(len(attn[0][0]))\n",
    "attn[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e460829be586a745d810aec71d83684bd38b76dd3b8d2db700ccf14d30953fce"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
