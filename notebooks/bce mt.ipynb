{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLAB\n",
    "!pip install neptune-client\n",
    "# pip install torch-tensorrt -f https://github.com/NVIDIA/Torch-TensorRT/releases\n",
    "!unzip data.zip\n",
    "!mkdir artifacts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "from torchvision import transforms as T\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import neptune.new as neptune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/victorcallejas/Belluga/e/BEL-172\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"victorcallejas/Belluga\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlNDRlNTJiNC00OTQwLTQxYjgtYWZiNS02OWQ0MDcwZmU5N2YifQ==\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1070 with Max-Q Design _CudaDeviceProperties(name='NVIDIA GeForce GTX 1070 with Max-Q Design', major=6, minor=1, total_memory=8191MB, multi_processor_count=16)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(torch.cuda.get_device_name(0), torch.cuda.get_device_properties(device))\n",
    "\n",
    "input_dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCORING\n",
    "PREDICTION_LIMIT = 20\n",
    "QUERY_ID_COL = \"query_id\"\n",
    "DATABASE_ID_COL = \"database_image_id\"\n",
    "SCORE_COL = \"score\"\n",
    "\n",
    "SCORE_THRESHOLD = 0.5\n",
    "\n",
    "class MeanAveragePrecision:\n",
    "    @classmethod\n",
    "    def score(cls, predicted: pd.DataFrame, actual: pd.DataFrame, prediction_limit: int):\n",
    "        \"\"\"Calculates mean average precision for a ranking task.\n",
    "        :param predicted: The predicted values as a dataframe with specified column names\n",
    "        :param actual: The ground truth values as a dataframe with specified column names\n",
    "        \"\"\"\n",
    "        if not predicted[SCORE_COL].between(0.0, 1.0).all():\n",
    "            raise ValueError(\"Scores must be in range [0, 1].\")\n",
    "        if predicted.index.name != QUERY_ID_COL:\n",
    "            raise ValueError(\n",
    "                f\"First column of submission must be named '{QUERY_ID_COL}', \"\n",
    "                f\"got {predicted.index.name}.\"\n",
    "            )\n",
    "        if predicted.columns.to_list() != [DATABASE_ID_COL, SCORE_COL]:\n",
    "            raise ValueError(\n",
    "                f\"Columns of submission must be named '{[DATABASE_ID_COL, SCORE_COL]}', \"\n",
    "                f\"got {predicted.columns.to_list()}.\"\n",
    "            )\n",
    "\n",
    "        unadjusted_aps, predicted_n_pos, actual_n_pos = cls._score_per_query(\n",
    "            predicted, actual, prediction_limit\n",
    "        )\n",
    "        adjusted_aps = unadjusted_aps.multiply(predicted_n_pos).divide(actual_n_pos)\n",
    "        return adjusted_aps.mean()\n",
    "\n",
    "    @classmethod\n",
    "    def _score_per_query(\n",
    "        cls, predicted: pd.DataFrame, actual: pd.DataFrame, prediction_limit: int\n",
    "    ):\n",
    "        \"\"\"Calculates per-query mean average precision for a ranking task.\"\"\"\n",
    "        merged = predicted.merge(\n",
    "            right=actual.assign(actual=1.0),\n",
    "            how=\"left\",\n",
    "            on=[QUERY_ID_COL, DATABASE_ID_COL],\n",
    "        ).fillna({\"actual\": 0.0})\n",
    "        # Per-query raw average precisions based on predictions\n",
    "        unadjusted_aps = merged.groupby(QUERY_ID_COL).apply(\n",
    "            lambda df: average_precision_score(df[\"actual\"].values, df[SCORE_COL].values)\n",
    "            if df[\"actual\"].sum()\n",
    "            else 0.0\n",
    "        )\n",
    "        # Total ground truth positive counts for rescaling\n",
    "        predicted_n_pos = merged[\"actual\"].groupby(QUERY_ID_COL).sum().astype(\"int64\").rename()\n",
    "        actual_n_pos = actual.groupby(QUERY_ID_COL).size().clip(upper=prediction_limit)\n",
    "        return unadjusted_aps, predicted_n_pos, actual_n_pos\n",
    "    \n",
    "    \n",
    "def map_score(dataloader, model, threshold=SCORE_THRESHOLD):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    sub = []\n",
    "    \n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    with torch.no_grad():        \n",
    "    \n",
    "        for query, reference, query_id, reference_id in tqdm(dataloader):\n",
    "            \n",
    "            query = query.to(device, non_blocking=True, dtype=input_dtype)\n",
    "            reference = reference.to(device, non_blocking=True, dtype=input_dtype)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled = fp16):\n",
    "                logits = sigmoid(model(query=query, reference=reference)).cpu().squeeze().tolist()\n",
    "                \n",
    "            sub.extend(zip(query_id, reference_id, logits))\n",
    "            \n",
    "    sub = pd.DataFrame(sub, columns=['query_id', 'database_image_id', 'score'])\n",
    "    sub = sub[sub.score > threshold]\n",
    "    sub = sub.set_index(['database_image_id']).groupby('query_id')['score'].nlargest(20).reset_index()\n",
    "    sub = sub.set_index('query_id')\n",
    "    \n",
    "    mean_avg_prec = MeanAveragePrecision.score(\n",
    "        predicted=sub, actual=dataloader.dataset.gt, prediction_limit=PREDICTION_LIMIT\n",
    "    )\n",
    "    \n",
    "    print('MaP: ',mean_avg_prec)\n",
    "    return mean_avg_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 105.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# DATA\n",
    "\n",
    "IMG_SIZE = 224\n",
    "ROOT_DIR = '../data/'\n",
    "NORM_TRANSFORMS = torch.nn.Sequential(\n",
    "    T.Resize([IMG_SIZE, IMG_SIZE]),\n",
    "    T.ConvertImageDtype(input_dtype),\n",
    "    T.Normalize(mean = (0.4234, 0.4272, 0.4641),\n",
    "                std  = (0.2037, 0.2027, 0.2142)),\n",
    ")\n",
    "\n",
    "VAL_SPLIT = 0.05\n",
    "\n",
    "METADATA = pd.read_csv('../data/metadata.csv')[:50]\n",
    "\n",
    "TRAIN, VAL = train_test_split(METADATA, test_size=0.05, random_state=42)\n",
    "TRAIN, VAL = TRAIN.reset_index(), VAL.reset_index()\n",
    "#TRAIN, VAL = METADATA, METADATA\n",
    "#TRAIN = METADATA\n",
    "\n",
    "def getImages(metadata):\n",
    "    IMAGES = {}\n",
    "    for image_id, path in tqdm(zip(metadata.image_id, metadata.path), total=metadata.shape[0]):\n",
    "        IMAGES[image_id] = NORM_TRANSFORMS(read_image(ROOT_DIR + path))\n",
    "    return IMAGES\n",
    "\n",
    "IMAGES = getImages(METADATA)\n",
    "\n",
    "class PreTrain_BellugaDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, metadata):\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.metadata.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return IMAGES[self.metadata.image_id[idx]]\n",
    "\n",
    "\n",
    "class Eval_BellugaDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, metadata):\n",
    "        self.metadata = metadata\n",
    "    \n",
    "        # GROUND TRUTH\n",
    "        gt = []\n",
    "        for wid in self.metadata.whale_id: # query\n",
    "            tmp = self.metadata[self.metadata.whale_id == wid].image_id.tolist() # get all images id\n",
    "            gt.extend(list(itertools.permutations(tmp, 2)))\n",
    "        self.gt = pd.DataFrame(gt,columns=['query_id','database_image_id'])\n",
    "        self.gt = self.gt.set_index('query_id')\n",
    "        \n",
    "        # ALL QUERIES\n",
    "        self.query_reference = list(itertools.permutations(self.metadata.image_id, 2))\n",
    "            \n",
    "    def getimage(self, image_id):\n",
    "        return IMAGES[image_id]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.query_reference)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        query_id = self.query_reference[idx][0]\n",
    "        reference_id = self.query_reference[idx][1]\n",
    "        \n",
    "        query = self.getimage(query_id)\n",
    "        reference = self.getimage(reference_id)\n",
    "        \n",
    "        return query, reference, query_id, reference_id\n",
    "    \n",
    "    \n",
    "class Train_BellugaDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, metadata):\n",
    "        self.metadata = metadata\n",
    "        self.aug = T.RandomErasing(p=0.4, scale=(0.12, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n",
    "            \n",
    "    def getimage(self, image_id):\n",
    "        return IMAGES[image_id]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.metadata.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        anchor = self.aug(self.getimage(self.metadata.image_id[idx]))\n",
    "        label = self.metadata.whale_id[idx]\n",
    "        \n",
    "        pos = self.aug(self.getimage(self.metadata[self.metadata.whale_id == label].sample()['image_id'].values[0]))\n",
    "        neg = self.aug(self.getimage(self.metadata[self.metadata.whale_id != label].sample()['image_id'].values[0]))\n",
    "\n",
    "        return anchor, pos, neg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATALOADERS \n",
    "\n",
    "PRETRAIN_BS = 6\n",
    "TRAIN_BS = 4\n",
    "INFER_BS = TRAIN_BS\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "\n",
    "#pretrain_dataset = PreTrain_BellugaDataset(METADATA)\n",
    "train_train_dataset = Train_BellugaDataset(TRAIN)\n",
    "#train_eval_dataset = Eval_BellugaDataset(TRAIN)\n",
    "valid_eval_dataset = Eval_BellugaDataset(VAL)\n",
    "\n",
    "'''\n",
    "pretrain_dataloader = torch.utils.data.DataLoader(\n",
    "                        pretrain_dataset, \n",
    "                        batch_size=PRETRAIN_BS,\n",
    "                        shuffle=True, \n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        pin_memory=True\n",
    "                    )\n",
    "'''\n",
    "train_train_dataloader = torch.utils.data.DataLoader(\n",
    "                        train_train_dataset, \n",
    "                        batch_size=TRAIN_BS,\n",
    "                        shuffle=True, \n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        pin_memory=True\n",
    "                    )\n",
    "'''\n",
    "train_eval_dataloader = torch.utils.data.DataLoader(\n",
    "                        train_eval_dataset, \n",
    "                        batch_size=INFER_BS,\n",
    "                        shuffle=True, \n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        pin_memory=True\n",
    "                    )\n",
    "'''\n",
    "valid_eval_dataloader = torch.utils.data.DataLoader(\n",
    "                        valid_eval_dataset, \n",
    "                        batch_size=INFER_BS,\n",
    "                        shuffle=False, \n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        pin_memory=True\n",
    "                    )   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "        pretrained_window_size (tuple[int]): The height and width of the window in pre-training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.,\n",
    "                 pretrained_window_size=[0, 0]):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.pretrained_window_size = pretrained_window_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)\n",
    "\n",
    "        # mlp to generate continuous relative position bias\n",
    "        self.cpb_mlp = nn.Sequential(nn.Linear(2, 512, bias=True),\n",
    "                                     nn.ReLU(inplace=True),\n",
    "                                     nn.Linear(512, num_heads, bias=False))\n",
    "\n",
    "        # get relative_coords_table\n",
    "        relative_coords_h = torch.arange(-(self.window_size[0] - 1), self.window_size[0], dtype=torch.float32)\n",
    "        relative_coords_w = torch.arange(-(self.window_size[1] - 1), self.window_size[1], dtype=torch.float32)\n",
    "        relative_coords_table = torch.stack(\n",
    "            torch.meshgrid([relative_coords_h,\n",
    "                            relative_coords_w])).permute(1, 2, 0).contiguous().unsqueeze(0)  # 1, 2*Wh-1, 2*Ww-1, 2\n",
    "        if pretrained_window_size[0] > 0:\n",
    "            relative_coords_table[:, :, :, 0] /= (pretrained_window_size[0] - 1)\n",
    "            relative_coords_table[:, :, :, 1] /= (pretrained_window_size[1] - 1)\n",
    "        else:\n",
    "            relative_coords_table[:, :, :, 0] /= (self.window_size[0] - 1)\n",
    "            relative_coords_table[:, :, :, 1] /= (self.window_size[1] - 1)\n",
    "        relative_coords_table *= 8  # normalize to -8, 8\n",
    "        relative_coords_table = torch.sign(relative_coords_table) * torch.log2(\n",
    "            torch.abs(relative_coords_table) + 1.0) / np.log2(8)\n",
    "\n",
    "        self.register_buffer(\"relative_coords_table\", relative_coords_table)\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        if qkv_bias:\n",
    "            self.q_bias = nn.Parameter(torch.zeros(dim))\n",
    "            self.v_bias = nn.Parameter(torch.zeros(dim))\n",
    "        else:\n",
    "            self.q_bias = None\n",
    "            self.v_bias = None\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv_bias = None\n",
    "        if self.q_bias is not None:\n",
    "            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n",
    "        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n",
    "        qkv = qkv.reshape(B_, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        # cosine attention\n",
    "        attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1))\n",
    "        logit_scale = torch.clamp(self.logit_scale, max=torch.log(torch.tensor(1. / 0.01))).exp()\n",
    "        attn = attn * logit_scale\n",
    "\n",
    "        relative_position_bias_table = self.cpb_mlp(self.relative_coords_table).view(-1, self.num_heads)\n",
    "        relative_position_bias = relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        relative_position_bias = 16 * torch.sigmoid(relative_position_bias)\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, ' \\\n",
    "               f'pretrained_window_size={self.pretrained_window_size}, num_heads={self.num_heads}'\n",
    "\n",
    "    def flops(self, N):\n",
    "        # calculate flops for 1 window with token length of N\n",
    "        flops = 0\n",
    "        # qkv = self.qkv(x)\n",
    "        flops += N * self.dim * 3 * self.dim\n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
    "        #  x = (attn @ v)\n",
    "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
    "        # x = self.proj(x)\n",
    "        flops += N * self.dim * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\" Swin Transformer Block.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "        pretrained_window_size (int): Window size in pre-training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, pretrained_window_size=0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop,\n",
    "            pretrained_window_size=to_2tuple(pretrained_window_size))\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "        x = shortcut + self.drop_path(self.norm1(x))\n",
    "\n",
    "        # FFN\n",
    "        x = x + self.drop_path(self.norm2(self.mlp(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.input_resolution\n",
    "        # norm1\n",
    "        flops += self.dim * H * W\n",
    "        # W-MSA/SW-MSA\n",
    "        nW = H * W / self.window_size / self.window_size\n",
    "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
    "        # mlp\n",
    "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
    "        # norm2\n",
    "        flops += self.dim * H * W\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(2 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.reduction(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
    "        flops += H * W * self.dim // 2\n",
    "        return flops\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "        pretrained_window_size (int): Local window size in pre-training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n",
    "                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n",
    "                 pretrained_window_size=0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads, window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias,\n",
    "                                 drop=drop, attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                 norm_layer=norm_layer,\n",
    "                                 pretrained_window_size=pretrained_window_size)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        for blk in self.blocks:\n",
    "            flops += blk.flops()\n",
    "        if self.downsample is not None:\n",
    "            flops += self.downsample.flops()\n",
    "        return flops\n",
    "\n",
    "    def _init_respostnorm(self):\n",
    "        for blk in self.blocks:\n",
    "            nn.init.constant_(blk.norm1.bias, 0)\n",
    "            nn.init.constant_(blk.norm1.weight, 0)\n",
    "            nn.init.constant_(blk.norm2.bias, 0)\n",
    "            nn.init.constant_(blk.norm2.weight, 0)\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        Ho, Wo = self.patches_resolution\n",
    "        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n",
    "        if self.norm is not None:\n",
    "            flops += Ho * Wo * self.embed_dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class SwinTransformerV2(nn.Module):\n",
    "    r\"\"\" Swin Transformer\n",
    "        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 224\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "        pretrained_window_sizes (tuple(int)): Pretrained window sizes of each layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n",
    "                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n",
    "                 window_size=7, mlp_ratio=4., qkv_bias=True,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n",
    "                 use_checkpoint=False, pretrained_window_sizes=[0, 0, 0, 0], **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),\n",
    "                               input_resolution=(patches_resolution[0] // (2 ** i_layer),\n",
    "                                                 patches_resolution[1] // (2 ** i_layer)),\n",
    "                               depth=depths[i_layer],\n",
    "                               num_heads=num_heads[i_layer],\n",
    "                               window_size=window_size,\n",
    "                               mlp_ratio=self.mlp_ratio,\n",
    "                               qkv_bias=qkv_bias,\n",
    "                               drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                               norm_layer=norm_layer,\n",
    "                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                               use_checkpoint=use_checkpoint,\n",
    "                               pretrained_window_size=pretrained_window_sizes[i_layer])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for bly in self.layers:\n",
    "            bly._init_respostnorm()\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'absolute_pos_embed'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {\"cpb_mlp\", \"logit_scale\", 'relative_position_bias_table'}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)  # B L C\n",
    "        x = self.avgpool(x.transpose(1, 2))  # B C 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.patch_embed.flops()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n",
    "        flops += self.num_features * self.num_classes\n",
    "        return flops\n",
    "    \n",
    "    \n",
    "def create_cross_swin_base():\n",
    "    return SwinTransformerV2(img_size=224, patch_size=4, in_chans=3, num_classes=1,\n",
    "                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n",
    "                 window_size=7, mlp_ratio=4., qkv_bias=True,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm, ape=True, patch_norm=True,\n",
    "                 use_checkpoint=False, pretrained_window_sizes=[0, 0, 0, 0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Matchformer(\n",
       "  (backbone): Matchformer_SEA_lite(\n",
       "    (AttentionBlock1): AttentionBlock(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 128, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "        (pos): Positional(\n",
       "          (pa_conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (block): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(128, 128, kernel_size=(8, 8), stride=(8, 8))\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(128, 128, kernel_size=(8, 8), stride=(8, 8))\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (kv): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(128, 128, kernel_size=(8, 8), stride=(8, 8))\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (AttentionBlock2): AttentionBlock(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(128, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (pos): Positional(\n",
       "          (pa_conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (block): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (kv): Linear(in_features=192, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(192, 192, kernel_size=(4, 4), stride=(4, 4))\n",
       "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (kv): Linear(in_features=192, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(192, 192, kernel_size=(4, 4), stride=(4, 4))\n",
       "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (kv): Linear(in_features=192, out_features=384, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(192, 192, kernel_size=(4, 4), stride=(4, 4))\n",
       "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (AttentionBlock3): AttentionBlock(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(192, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (pos): Positional(\n",
       "          (pa_conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (block): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (kv): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (sr): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (AttentionBlock4): AttentionBlock(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (pos): Positional(\n",
       "          (pa_conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (block): ModuleList(\n",
       "        (0): Block(\n",
       "          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (kv): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "            )\n",
       "            (act): GELU()\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "    (layer4_outconv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (layer3_outconv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (layer3_outconv2): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (layer2_outconv): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (layer2_outconv2): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Conv2d(256, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (layer1_outconv): Conv2d(128, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (layer1_outconv2): Sequential(\n",
       "      (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "      (3): Conv2d(192, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (mlp_head): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_cross_swin_base().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ckpt = torch.load('/kaggle/input/ckptttt/net (3).pt')\n",
    "#model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-5)\n",
    "#optimizer.load_state_dict(ckpt['optimizer_state_dict'], )\n",
    "\n",
    "#opt = torch.optim.SGD(model.parameters(), lr = .05)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 128, 56, 56]) torch.Size([16, 192, 28, 28])\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3397, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\vcall\\AppData\\Local\\Temp\\ipykernel_22024\\1060586997.py\", line 21, in <cell line: 3>\n",
      "    logits = model(query, reference)\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"c:\\repos\\belugas\\notebooks\\..\\src\\model\\MT\\matchformer.py\", line 320, in forward\n",
      "    x = torch.cat([feats_q, feats_r], dim=1)\n",
      "RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 28 but got size 56 for tensor number 1 in the list.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1992, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    epoch_loss, epoch_acc = 0, 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for anchor, pos, neg in tqdm(train_train_dataloader):\n",
    "\n",
    "        optimizer.zero_grad(True)\n",
    "        \n",
    "        anchor = anchor.to(device, non_blocking=True, dtype=input_dtype)\n",
    "        pos = pos.to(device, non_blocking=True, dtype=input_dtype)\n",
    "        neg = neg.to(device, non_blocking=True, dtype=input_dtype)\n",
    "        \n",
    "        query = torch.cat([anchor, anchor], dim=0)\n",
    "        reference = torch.cat([pos, neg], dim=0)\n",
    "        labels = torch.cat([torch.ones(pos.shape[0],1), torch.zeros(neg.shape[0],1)], dim=0).to(device)\n",
    "\n",
    "        logits = model(query, reference)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "        run['running/loss'].log(loss)\n",
    "        \n",
    "        # accuracy\n",
    "        preds = torch.nn.Sigmoid()(logits).round().detach().cpu().numpy()\n",
    "        acc = accuracy_score(labels.detach().cpu().numpy(), preds)\n",
    "        run['running/acc'].log(acc)\n",
    "        \n",
    "        epoch_loss += loss\n",
    "        epoch_acc += acc\n",
    "        \n",
    "    run['epoch/train/loss'].log(epoch_loss / len(train_train_dataloader))\n",
    "    run['epoch/train/acc'].log(epoch_acc / len(train_train_dataloader))\n",
    "    \n",
    "    if epoch_i % 20 == 0:\n",
    "        map = map_score(valid_eval_dataloader, model)\n",
    "        run['epoch5/valid/map'].log(map)\n",
    "        \n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, f'/kaggle/working/artifacts/net.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, f'../artifacts/net_{epoch_i}_83acc.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal batch size for inference\n",
    "\"\"\"\n",
    "5vcpus\n",
    "52 ram\n",
    "12 ram gpu\n",
    "\n",
    "model to eval, optim bactch size calculate with tqdm\n",
    "\"\"\"\n",
    "model.eval()\n",
    "\n",
    "TEST_INFER_BS_INIT = 10\n",
    "TEST_INFER_BS_ML = 10\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "class DummyTest(torch.utils.data.Dataset):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def __len__(self):\n",
    "    return 7000000\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return torch.zeros((3,224,224)), torch.zeros((3,224,224))\n",
    "\n",
    "BS = TEST_INFER_BS_INIT = 5\n",
    "while True:\n",
    "  i = 0\n",
    "  BS = BS + TEST_INFER_BS_ML\n",
    "  dataloader = torch.utils.data.DataLoader(\n",
    "                          DummyTest(), \n",
    "                          batch_size=BS,\n",
    "                          shuffle=False, \n",
    "                          num_workers=0,\n",
    "                          pin_memory=True\n",
    "                      )   \n",
    "\n",
    "  for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "    query = batch[0].to(device, non_blocking=True, dtype=torch.float32)\n",
    "    reference = batch[1].to(device, non_blocking=True, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "      logits, attn, q_cls, r_cls = model(query=query, reference=reference)\n",
    "      i+=1\n",
    "    \n",
    "    if i == 50:\n",
    "      print(BS)\n",
    "      print(torch.cuda.mem_get_info(device=0))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6465060864, 8589737984)\n"
     ]
    }
   ],
   "source": [
    "fp16 = True \n",
    "input_dtype = torch.float16 if fp16 else torch.float32\n",
    "\n",
    "scaler =  torch.cuda.amp.GradScaler(enabled=fp16)\n",
    "\n",
    "model = crossvit_base_224().to(device)\n",
    "input = torch.zeros((2,3,224,224), dtype=input_dtype, device=device)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "labels = torch.ones((2,1), dtype=input_dtype, device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-4)\n",
    "\n",
    "warmup, reps = 30, 10\n",
    "\n",
    "for i in range(0, warmup):\n",
    "    \n",
    "        with torch.cuda.amp.autocast(enabled = fp16):\n",
    "            logits, attn, q_cls, r_cls = model(query=input, reference=input)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    profile_memory=True,\n",
    ") as prof:\n",
    "    print(torch.cuda.mem_get_info(0))\n",
    "    for i in range(0, reps):\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled = fp16):\n",
    "            \n",
    "            logits, attn, q_cls, r_cls = model(query=input, reference=input)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          aten::reshape         5.00%     566.319ms        11.90%        1.346s     153.356us     544.024ms         4.84%        1.311s     149.324us           0 b           0 b     542.46 Mb           0 b          8780  \n",
      "                                            aten::copy_         4.72%     534.034ms         4.72%     534.034ms      78.650us     525.057ms         4.67%     525.057ms      77.328us           0 b           0 b           0 b           0 b          6790  \n",
      "                                         aten::_to_copy         4.56%     516.087ms        10.33%        1.170s     227.134us     503.597ms         4.48%        1.151s     223.463us           0 b           0 b       2.03 Gb           0 b          5150  \n",
      "                                       aten::as_strided         4.20%     475.203ms         4.20%     475.203ms      29.297us     453.851ms         4.04%     453.851ms      27.981us           0 b           0 b           0 b           0 b         16220  \n",
      "                                               aten::mm         4.04%     457.166ms         4.04%     457.166ms     155.499us     510.154ms         4.54%     510.154ms     173.522us           0 b           0 b       1.28 Gb       1.28 Gb          2940  \n",
      "                                          aten::permute         3.78%     427.307ms         5.55%     628.754ms      92.737us     439.648ms         3.91%     627.262ms      92.517us           0 b           0 b           0 b           0 b          6780  \n",
      "                                            aten::empty         3.60%     407.873ms         3.60%     407.873ms      74.159us     403.471ms         3.59%     403.471ms      73.358us           0 b           0 b       1.35 Gb       1.35 Gb          5500  \n",
      "                                        aten::transpose         3.07%     347.453ms         4.57%     517.742ms      87.902us     344.904ms         3.07%     519.313ms      88.169us           0 b           0 b           0 b           0 b          5890  \n",
      "                                    aten::empty_strided         2.93%     331.262ms         2.93%     331.262ms      50.420us     331.970ms         2.96%     331.970ms      50.528us           0 b           0 b       2.72 Gb       2.72 Gb          6570  \n",
      "                                                aten::t         2.80%     316.616ms         6.63%     750.914ms     152.315us     320.957ms         2.86%     755.833ms     153.313us           0 b           0 b           0 b           0 b          4930  \n",
      "                                   aten::_reshape_alias         2.50%     283.267ms         2.50%     283.267ms      37.469us     283.366ms         2.52%     283.366ms      37.482us           0 b           0 b           0 b           0 b          7560  \n",
      "                                               aten::to         2.50%     282.847ms        12.83%        1.453s     280.964us     295.169ms         2.63%        1.446s     279.691us           0 b           0 b       2.03 Gb           0 b          5170  \n",
      "                                             aten::add_         2.13%     240.690ms         2.13%     240.690ms      81.590us     244.176ms         2.17%     244.176ms      82.772us           0 b           0 b           0 b           0 b          2950  \n",
      "                                           aten::linear         2.09%     236.620ms        19.31%        2.186s       1.104ms     238.288ms         2.12%        2.180s       1.101ms           0 b           0 b       1.57 Gb           0 b          1980  \n",
      "                                              aten::bmm         2.00%     226.344ms         2.87%     324.686ms     193.265us     220.517ms         1.96%     317.646ms     189.075us           0 b           0 b     852.90 Mb     642.09 Mb          1680  \n",
      "                                       aten::empty_like         1.98%     223.804ms         4.65%     526.345ms     147.024us     220.125ms         1.96%     521.439ms     145.653us           0 b           0 b       1.79 Gb           0 b          3580  \n",
      "autograd::engine::evaluate_function: PermuteBackward...         1.94%     219.509ms         6.43%     727.753ms     215.312us     220.368ms         1.96%     725.153ms     214.542us           0 b           0 b           0 b           0 b          3380  \n",
      "                                           aten::einsum         1.77%     200.538ms         9.10%        1.030s       2.147ms     224.760ms         2.00%        1.032s       2.150ms           0 b           0 b     652.13 Mb           0 b           480  \n",
      "autograd::engine::evaluate_function: ToCopyBackward0...         1.77%     200.472ms         9.14%        1.034s     438.266us     193.696ms         1.72%        1.032s     437.249us           0 b           0 b     725.12 Mb    -703.96 Mb          2360  \n",
      "                                       PermuteBackward0         1.73%     195.666ms         4.49%     508.244ms     150.368us     188.534ms         1.68%     504.785ms     149.345us           0 b           0 b           0 b           0 b          3380  \n",
      "                 struct torch::autograd::AccumulateGrad         1.70%     192.112ms         3.38%     382.221ms     159.259us     187.491ms         1.67%     381.166ms     158.819us           0 b           0 b    -973.02 Mb    -973.02 Mb          2400  \n",
      "                                             aten::view         1.64%     185.266ms         1.64%     185.266ms      33.624us     184.722ms         1.64%     184.722ms      33.525us           0 b           0 b           0 b           0 b          5510  \n",
      "                                            MmBackward0         1.63%     184.982ms         8.02%     907.421ms     925.940us     187.506ms         1.67%     942.223ms     961.452us           0 b           0 b     746.62 Mb           0 b           980  \n",
      "autograd::engine::evaluate_function: ReshapeAliasBac...         1.36%     154.437ms         5.63%     636.799ms     289.454us     153.435ms         1.37%     635.240ms     288.745us           0 b           0 b           0 b    -161.60 Mb          2200  \n",
      "autograd::engine::evaluate_function: struct torch::a...         1.32%     149.725ms         4.70%     531.946ms     221.644us     140.895ms         1.25%     522.061ms     217.525us           0 b           0 b    -973.02 Mb           0 b          2400  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 11.319s\n",
      "Self CUDA time total: 11.233s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 1, 197])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layers x bs x (q,ref) x cross_attn_depth x n_heads x 1 x tokens(inc cls)\n",
    "attn[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 1, 197])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(attn))\n",
    "print(len(attn[0]))\n",
    "print(len(attn[0][0]))\n",
    "attn[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e460829be586a745d810aec71d83684bd38b76dd3b8d2db700ccf14d30953fce"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
