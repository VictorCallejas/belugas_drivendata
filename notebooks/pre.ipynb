{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/victorcallejas/Belluga/e/BEL-92\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 596/5902 [00:22<02:50, 31.17it/s]"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import torch\n",
    "\n",
    "import neptune.new as neptune\n",
    "run = neptune.init(\n",
    "    project=\"victorcallejas/Belluga\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlNDRlNTJiNC00OTQwLTQxYjgtYWZiNS02OWQ0MDcwZmU5N2YifQ==\"\n",
    ")\n",
    "\n",
    "from src.model.CrossVit import VisionTransformer, crossvit_base_448, crossvit_base_244\n",
    "\n",
    "from src.data.dataloaders import pretrain_dataloader\n",
    "\n",
    "from vit_pytorch import Dino\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = crossvit_base_448().to(device)\n",
    "\n",
    "fp16 = False\n",
    "input_dtype = torch.float16 if fp16 else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 41/984 [01:01<23:26,  1.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\repos\\belugas\\notebooks\\bce.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce.ipynb#ch0000015?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m tqdm(pretrain_dataloader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce.ipynb#ch0000015?line=22'>23</a>\u001b[0m     image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mto(device, non_blocking\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, dtype\u001b[39m=\u001b[39minput_dtype)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce.ipynb#ch0000015?line=23'>24</a>\u001b[0m     loss \u001b[39m=\u001b[39m learner(image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce.ipynb#ch0000015?line=24'>25</a>\u001b[0m     opt\u001b[39m.\u001b[39mzero_grad(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce.ipynb#ch0000015?line=25'>26</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\vit_pytorch\\dino.py:289\u001b[0m, in \u001b[0;36mDino.forward\u001b[1;34m(self, x, return_embedding, return_projection, student_temp, teacher_temp)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/vit_pytorch/dino.py?line=286'>287</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/vit_pytorch/dino.py?line=287'>288</a>\u001b[0m     teacher_encoder \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_teacher_encoder()\n\u001b[1;32m--> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/vit_pytorch/dino.py?line=288'>289</a>\u001b[0m     teacher_proj_one, _ \u001b[39m=\u001b[39m teacher_encoder(global_image_one)\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/vit_pytorch/dino.py?line=289'>290</a>\u001b[0m     teacher_proj_two, _ \u001b[39m=\u001b[39m teacher_encoder(global_image_two)\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/vit_pytorch/dino.py?line=291'>292</a>\u001b[0m loss_fn_ \u001b[39m=\u001b[39m partial(\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/vit_pytorch/dino.py?line=292'>293</a>\u001b[0m     loss_fn,\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/vit_pytorch/dino.py?line=293'>294</a>\u001b[0m     student_temp \u001b[39m=\u001b[39m default(student_temp, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstudent_temp),\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/vit_pytorch/dino.py?line=294'>295</a>\u001b[0m     teacher_temp \u001b[39m=\u001b[39m default(teacher_temp, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mteacher_temp),\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/vit_pytorch/dino.py?line=295'>296</a>\u001b[0m     centers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mteacher_centers\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/vit_pytorch/dino.py?line=296'>297</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\vit_pytorch\\dino.py:175\u001b[0m, in \u001b[0;36mNetWrapper.forward\u001b[1;34m(self, x, return_projection)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/vit_pytorch/dino.py?line=173'>174</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, return_projection \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/vit_pytorch/dino.py?line=174'>175</a>\u001b[0m     embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_embedding(x)\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/vit_pytorch/dino.py?line=175'>176</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_projection:\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/vit_pytorch/dino.py?line=176'>177</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m embed\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\vit_pytorch\\dino.py:167\u001b[0m, in \u001b[0;36mNetWrapper.get_embedding\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/vit_pytorch/dino.py?line=163'>164</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_register_hook()\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/vit_pytorch/dino.py?line=165'>166</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden\u001b[39m.\u001b[39mclear()\n\u001b[1;32m--> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/vit_pytorch/dino.py?line=166'>167</a>\u001b[0m _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(x)\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/vit_pytorch/dino.py?line=167'>168</a>\u001b[0m hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden[x\u001b[39m.\u001b[39mdevice]\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/vit_pytorch/dino.py?line=168'>169</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden\u001b[39m.\u001b[39mclear()\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\repos\\belugas\\notebooks\\..\\src\\model\\CrossVit.py:305\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[1;34m(self, query, reference)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=301'>302</a>\u001b[0m \u001b[39mif\u001b[39;00m reference \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=302'>303</a>\u001b[0m     reference \u001b[39m=\u001b[39m query\n\u001b[1;32m--> <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=304'>305</a>\u001b[0m q_cls, r_cls \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_features(query, reference)\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=306'>307</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([q_cls, r_cls], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=308'>309</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdino_emb(x)\n",
      "File \u001b[1;32mc:\\repos\\belugas\\notebooks\\..\\src\\model\\CrossVit.py:291\u001b[0m, in \u001b[0;36mVisionTransformer.forward_features\u001b[1;34m(self, query, reference)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=287'>288</a>\u001b[0m     xs\u001b[39m.\u001b[39mappend(tmp)\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=289'>290</a>\u001b[0m \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[1;32m--> <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=290'>291</a>\u001b[0m     xs \u001b[39m=\u001b[39m blk(xs)\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=292'>293</a>\u001b[0m \u001b[39m# NOTE: was before branch token section, move to here to assure all branch token are before layer norm\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=293'>294</a>\u001b[0m xs \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm[i](x) \u001b[39mfor\u001b[39;00m i, x \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(xs)]\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\repos\\belugas\\notebooks\\..\\src\\model\\CrossVit.py:182\u001b[0m, in \u001b[0;36mMultiScaleBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=179'>180</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_branches):\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=180'>181</a>\u001b[0m     tmp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((proj_cls_token[i], outs_b[(i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_branches][:, \u001b[39m1\u001b[39m:, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=181'>182</a>\u001b[0m     tmp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfusion[i](tmp)\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=182'>183</a>\u001b[0m     reverted_proj_cls_token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrevert_projs[i](tmp[:, \u001b[39m0\u001b[39m:\u001b[39m1\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m])\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=183'>184</a>\u001b[0m     tmp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((reverted_proj_cls_token, outs_b[i][:, \u001b[39m1\u001b[39m:, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\repos\\belugas\\notebooks\\..\\src\\model\\CrossVit.py:116\u001b[0m, in \u001b[0;36mCrossAttentionBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=114'>115</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=115'>116</a>\u001b[0m     x \u001b[39m=\u001b[39m x[:, \u001b[39m0\u001b[39m:\u001b[39m1\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_path(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(x)))\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=116'>117</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_mlp:\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=117'>118</a>\u001b[0m         x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x)))\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\repos\\belugas\\notebooks\\..\\src\\model\\CrossVit.py:85\u001b[0m, in \u001b[0;36mCrossAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=81'>82</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=83'>84</a>\u001b[0m     B, N, C \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[1;32m---> <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=84'>85</a>\u001b[0m     q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwq(x[:, \u001b[39m0\u001b[39;49m:\u001b[39m1\u001b[39;49m, \u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m])\u001b[39m.\u001b[39mreshape(B, \u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, C \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)  \u001b[39m# B1C -> B1H(C/H) -> BH1(C/H)\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=85'>86</a>\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwk(x)\u001b[39m.\u001b[39mreshape(B, N, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, C \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)  \u001b[39m# BNC -> BNH(C/H) -> BHN(C/H)\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/notebooks/../src/model/CrossVit.py?line=86'>87</a>\u001b[0m     v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwv(x)\u001b[39m.\u001b[39mreshape(B, N, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, C \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)  \u001b[39m# BNC -> BNH(C/H) -> BHN(C/H)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DINO\n",
    "\n",
    "learner = Dino(\n",
    "    model,\n",
    "    image_size = 256,\n",
    "    hidden_layer = 'dino_emb',         # hidden layer name or index, from which to extract the embedding\n",
    "    projection_hidden_size = 256,      # projector network hidden dimension\n",
    "    projection_layers = 2,             # number of layers in projection network\n",
    "    num_classes_K = 65336,             # output logits dimensions (referenced as K in paper)\n",
    "    student_temp = 0.9,                # student temperature\n",
    "    teacher_temp = 0.04,               # teacher temperature, needs to be annealed from 0.04 to 0.07 over 30 epochs\n",
    "    local_upper_crop_scale = 0.4,      # upper bound for local crop - 0.4 was recommended in the paper \n",
    "    global_lower_crop_scale = 0.5,     # lower bound for global crop - 0.5 was recommended in the paper\n",
    "    moving_average_decay = 0.9,        # moving average of encoder - paper showed anywhere from 0.9 to 0.999 was ok\n",
    "    center_moving_average_decay = 0.9, # moving average of teacher centers - paper showed anywhere from 0.9 to 0.999 was ok\n",
    ").to(device)\n",
    "\n",
    "opt = torch.optim.Adam(learner.parameters(), lr = 3e-4)\n",
    "opt.zero_grad(True)\n",
    "\n",
    "while True:\n",
    "    for image in tqdm(pretrain_dataloader):\n",
    "        image = image.to(device, non_blocking=True, dtype=input_dtype)\n",
    "        loss = learner(image)\n",
    "        opt.zero_grad(True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        learner.update_moving_average() # update moving average of teacher encoder and teacher centers\n",
    "        run['dino/loss'].log(loss)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'pre.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e460829be586a745d810aec71d83684bd38b76dd3b8d2db700ccf14d30953fce"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
