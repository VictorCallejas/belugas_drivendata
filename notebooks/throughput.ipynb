{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLAB\n",
    "!pip install neptune-client\n",
    "# pip install torch-tensorrt -f https://github.com/NVIDIA/Torch-TensorRT/releases\n",
    "!unzip data.zip\n",
    "!mkdir artifacts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "import torch\n",
    "from torch import einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from torchvision import transforms as T\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import neptune.new as neptune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1070 with Max-Q Design _CudaDeviceProperties(name='NVIDIA GeForce GTX 1070 with Max-Q Design', major=6, minor=1, total_memory=8191MB, multi_processor_count=16)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(torch.cuda.get_device_name(0), torch.cuda.get_device_properties(device))\n",
    "\n",
    "fp16 = True\n",
    "input_dtype = torch.float16 if fp16 else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "# pre-layernorm\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "# feedforward\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# attention\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context = None, kv_include_self = False):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        context = default(context, x)\n",
    "\n",
    "        if kv_include_self:\n",
    "            context = torch.cat((x, context), dim = 1) # cross attention requires CLS token includes itself as key / value\n",
    "\n",
    "        qkv = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out), attn\n",
    "\n",
    "# transformer encoder, for qall and large patches\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            _x, _attn = attn(x)\n",
    "            x = _x + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)\n",
    "\n",
    "# projecting CLS tokens, in the case that qall and large patch tokens have different dimensions\n",
    "\n",
    "class ProjectInOut(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "        need_projection = dim_in != dim_out\n",
    "        self.project_in = nn.Linear(dim_in, dim_out) if need_projection else nn.Identity()\n",
    "        self.project_out = nn.Linear(dim_out, dim_in) if need_projection else nn.Identity()\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self.project_in(x)\n",
    "        x = self.fn(x, *args, **kwargs)\n",
    "        x = self.project_out(x)\n",
    "        return x\n",
    "\n",
    "# cross attention transformer\n",
    "\n",
    "class CrossTransformer(nn.Module):\n",
    "    def __init__(self, q_dim, ref_dim, depth, heads, dim_head, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                ProjectInOut(q_dim, ref_dim, PreNorm(ref_dim, Attention(ref_dim, heads = heads, dim_head = dim_head, dropout = dropout))),\n",
    "                ProjectInOut(ref_dim, q_dim, PreNorm(q_dim, Attention(q_dim, heads = heads, dim_head = dim_head, dropout = dropout)))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, q_tokens, ref_tokens):\n",
    "        (q_cls, q_patch_tokens), (ref_cls, ref_patch_tokens) = map(lambda t: (t[:, :1], t[:, 1:]), (q_tokens, ref_tokens))\n",
    "        _cross_attn_mat = []\n",
    "        for q_attend_ref, ref_attend_q in self.layers:\n",
    "            q_a_r, q_a_r_attn = q_attend_ref(q_cls, context = ref_patch_tokens, kv_include_self = True)\n",
    "            r_a_q, r_a_q_attn= ref_attend_q(ref_cls, context = q_patch_tokens, kv_include_self = True)\n",
    "            q_cls = q_a_r + q_cls\n",
    "            ref_cls = r_a_q + ref_cls\n",
    "            _cross_attn_mat.append((q_a_r_attn, r_a_q_attn))\n",
    "            \n",
    "\n",
    "        q_tokens = torch.cat((q_cls, q_patch_tokens), dim = 1)\n",
    "        ref_tokens = torch.cat((ref_cls, ref_patch_tokens), dim = 1)\n",
    "        \n",
    "        return q_tokens, ref_tokens, _cross_attn_mat\n",
    "\n",
    "# multi-scale encoder\n",
    "\n",
    "class MultiScaleEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        depth,\n",
    "        q_dim,\n",
    "        ref_dim,\n",
    "        q_enc_params,\n",
    "        ref_enc_params,\n",
    "        cross_attn_heads,\n",
    "        cross_attn_depth,\n",
    "        cross_attn_dim_head = 64,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Transformer(dim = q_dim, dropout = dropout, **q_enc_params),\n",
    "                Transformer(dim = ref_dim, dropout = dropout, **ref_enc_params),\n",
    "                CrossTransformer(q_dim = q_dim, ref_dim = ref_dim, depth = cross_attn_depth, heads = cross_attn_heads, dim_head = cross_attn_dim_head, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, q_tokens, ref_tokens):\n",
    "        cross_attn_mat = []\n",
    "        for q_enc, ref_enc, cross_attend in self.layers:\n",
    "            q_tokens, ref_tokens = q_enc(q_tokens), ref_enc(ref_tokens)\n",
    "            q_tokens, ref_tokens, _cross_attn_mat = cross_attend(q_tokens, ref_tokens)\n",
    "            cross_attn_mat.append(_cross_attn_mat)\n",
    "\n",
    "        return q_tokens, ref_tokens, cross_attn_mat\n",
    "\n",
    "# patch-based image to token embedder\n",
    "\n",
    "class ImageEmbedder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        image_size,\n",
    "        patch_size,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = 3 * patch_size ** 2\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "\n",
    "        return self.dropout(x)\n",
    "\n",
    "# cross ViT class\n",
    "\n",
    "class CrossViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        image_size,\n",
    "        num_classes,\n",
    "        q_dim,\n",
    "        ref_dim,\n",
    "        q_patch_size = 12,\n",
    "        q_enc_depth = 1,\n",
    "        q_enc_heads = 8,\n",
    "        q_enc_mlp_dim = 2048,\n",
    "        q_enc_dim_head = 64,\n",
    "        ref_patch_size = 16,\n",
    "        ref_enc_depth = 4,\n",
    "        ref_enc_heads = 8,\n",
    "        ref_enc_mlp_dim = 2048,\n",
    "        ref_enc_dim_head = 64,\n",
    "        cross_attn_depth = 2,\n",
    "        cross_attn_heads = 8,\n",
    "        cross_attn_dim_head = 64,\n",
    "        depth = 3,\n",
    "        dropout = 0.1,\n",
    "        emb_dropout = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.q_image_embedder = ImageEmbedder(dim = q_dim, image_size = image_size, patch_size = q_patch_size, dropout = emb_dropout)\n",
    "        self.ref_image_embedder = ImageEmbedder(dim = ref_dim, image_size = image_size, patch_size = ref_patch_size, dropout = emb_dropout)\n",
    "\n",
    "        self.multi_scale_encoder = MultiScaleEncoder(\n",
    "            depth = depth,\n",
    "            q_dim = q_dim,\n",
    "            ref_dim = ref_dim,\n",
    "            cross_attn_heads = cross_attn_heads,\n",
    "            cross_attn_dim_head = cross_attn_dim_head,\n",
    "            cross_attn_depth = cross_attn_depth,\n",
    "            q_enc_params = dict(\n",
    "                depth = q_enc_depth,\n",
    "                heads = q_enc_heads,\n",
    "                mlp_dim = q_enc_mlp_dim,\n",
    "                dim_head = q_enc_dim_head\n",
    "            ),\n",
    "            ref_enc_params = dict(\n",
    "                depth = ref_enc_depth,\n",
    "                heads = ref_enc_heads,\n",
    "                mlp_dim = ref_enc_mlp_dim,\n",
    "                dim_head = ref_enc_dim_head\n",
    "            ),\n",
    "            dropout = dropout\n",
    "        )\n",
    "\n",
    "        self.mlp_head = nn.Sequential(nn.LayerNorm(q_dim + ref_dim), nn.Linear(q_dim + ref_dim, num_classes))\n",
    "\n",
    "    def forward(self, query, reference):\n",
    "        q_tokens = self.q_image_embedder(query)\n",
    "        ref_tokens = self.ref_image_embedder(reference)\n",
    "\n",
    "        q_tokens, ref_tokens, cross_attn_mat = self.multi_scale_encoder(q_tokens, ref_tokens)\n",
    "\n",
    "        q_cls, ref_cls = map(lambda t: t[:, 0], (q_tokens, ref_tokens))\n",
    "\n",
    "        cls = torch.cat([q_cls, ref_cls], dim=1)\n",
    "        logits = self.mlp_head(cls)\n",
    "\n",
    "        return logits, cross_attn_mat, q_cls, ref_cls\n",
    "    \n",
    "\n",
    "def crossvit_base_224():\n",
    "    \n",
    "    return CrossViT(\n",
    "        image_size=224,\n",
    "        num_classes=1,\n",
    "        q_dim=192,\n",
    "        ref_dim=192,\n",
    "        q_patch_size = 16,\n",
    "        q_enc_depth = 2,\n",
    "        q_enc_heads = 8,\n",
    "        q_enc_mlp_dim = 2048,\n",
    "        q_enc_dim_head = 64,\n",
    "        ref_patch_size = 16,\n",
    "        ref_enc_depth = 2,\n",
    "        ref_enc_heads = 8,\n",
    "        ref_enc_mlp_dim = 2048,\n",
    "        ref_enc_dim_head = 64,\n",
    "        cross_attn_depth = 2,\n",
    "        cross_attn_heads = 12,\n",
    "        cross_attn_dim_head = 64,\n",
    "        depth = 3,\n",
    "        dropout = 0.1,\n",
    "        emb_dropout = 0.1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossViT(\n",
       "  (q_image_embedder): ImageEmbedder(\n",
       "    (to_patch_embedding): Sequential(\n",
       "      (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=16, p2=16)\n",
       "      (1): Linear(in_features=768, out_features=192, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ref_image_embedder): ImageEmbedder(\n",
       "    (to_patch_embedding): Sequential(\n",
       "      (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=16, p2=16)\n",
       "      (1): Linear(in_features=768, out_features=192, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (multi_scale_encoder): MultiScaleEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): Transformer(\n",
       "          (layers): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (attend): Softmax(dim=-1)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (to_q): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=192, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=192, bias=True)\n",
       "                    (1): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=192, out_features=2048, bias=True)\n",
       "                    (1): GELU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=2048, out_features=192, bias=True)\n",
       "                    (4): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (attend): Softmax(dim=-1)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (to_q): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=192, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=192, bias=True)\n",
       "                    (1): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=192, out_features=2048, bias=True)\n",
       "                    (1): GELU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=2048, out_features=192, bias=True)\n",
       "                    (4): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): Transformer(\n",
       "          (layers): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (attend): Softmax(dim=-1)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (to_q): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=192, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=192, bias=True)\n",
       "                    (1): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=192, out_features=2048, bias=True)\n",
       "                    (1): GELU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=2048, out_features=192, bias=True)\n",
       "                    (4): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (attend): Softmax(dim=-1)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (to_q): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=192, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=192, bias=True)\n",
       "                    (1): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=192, out_features=2048, bias=True)\n",
       "                    (1): GELU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=2048, out_features=192, bias=True)\n",
       "                    (4): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): CrossTransformer(\n",
       "          (layers): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): ProjectInOut(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Attention(\n",
       "                    (attend): Softmax(dim=-1)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (to_q): Linear(in_features=192, out_features=768, bias=False)\n",
       "                    (to_kv): Linear(in_features=192, out_features=1536, bias=False)\n",
       "                    (to_out): Sequential(\n",
       "                      (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "                      (1): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (project_in): Identity()\n",
       "                (project_out): Identity()\n",
       "              )\n",
       "              (1): ProjectInOut(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Attention(\n",
       "                    (attend): Softmax(dim=-1)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (to_q): Linear(in_features=192, out_features=768, bias=False)\n",
       "                    (to_kv): Linear(in_features=192, out_features=1536, bias=False)\n",
       "                    (to_out): Sequential(\n",
       "                      (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "                      (1): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (project_in): Identity()\n",
       "                (project_out): Identity()\n",
       "              )\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): ProjectInOut(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Attention(\n",
       "                    (attend): Softmax(dim=-1)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (to_q): Linear(in_features=192, out_features=768, bias=False)\n",
       "                    (to_kv): Linear(in_features=192, out_features=1536, bias=False)\n",
       "                    (to_out): Sequential(\n",
       "                      (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "                      (1): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (project_in): Identity()\n",
       "                (project_out): Identity()\n",
       "              )\n",
       "              (1): ProjectInOut(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Attention(\n",
       "                    (attend): Softmax(dim=-1)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (to_q): Linear(in_features=192, out_features=768, bias=False)\n",
       "                    (to_kv): Linear(in_features=192, out_features=1536, bias=False)\n",
       "                    (to_out): Sequential(\n",
       "                      (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "                      (1): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (project_in): Identity()\n",
       "                (project_out): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): Transformer(\n",
       "          (layers): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (attend): Softmax(dim=-1)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (to_q): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=192, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=192, bias=True)\n",
       "                    (1): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=192, out_features=2048, bias=True)\n",
       "                    (1): GELU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=2048, out_features=192, bias=True)\n",
       "                    (4): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (attend): Softmax(dim=-1)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (to_q): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=192, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=192, bias=True)\n",
       "                    (1): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=192, out_features=2048, bias=True)\n",
       "                    (1): GELU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=2048, out_features=192, bias=True)\n",
       "                    (4): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): Transformer(\n",
       "          (layers): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (attend): Softmax(dim=-1)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (to_q): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=192, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=192, bias=True)\n",
       "                    (1): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=192, out_features=2048, bias=True)\n",
       "                    (1): GELU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=2048, out_features=192, bias=True)\n",
       "                    (4): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (attend): Softmax(dim=-1)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (to_q): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=192, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=192, bias=True)\n",
       "                    (1): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=192, out_features=2048, bias=True)\n",
       "                    (1): GELU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=2048, out_features=192, bias=True)\n",
       "                    (4): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): CrossTransformer(\n",
       "          (layers): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): ProjectInOut(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Attention(\n",
       "                    (attend): Softmax(dim=-1)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (to_q): Linear(in_features=192, out_features=768, bias=False)\n",
       "                    (to_kv): Linear(in_features=192, out_features=1536, bias=False)\n",
       "                    (to_out): Sequential(\n",
       "                      (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "                      (1): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (project_in): Identity()\n",
       "                (project_out): Identity()\n",
       "              )\n",
       "              (1): ProjectInOut(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Attention(\n",
       "                    (attend): Softmax(dim=-1)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (to_q): Linear(in_features=192, out_features=768, bias=False)\n",
       "                    (to_kv): Linear(in_features=192, out_features=1536, bias=False)\n",
       "                    (to_out): Sequential(\n",
       "                      (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "                      (1): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (project_in): Identity()\n",
       "                (project_out): Identity()\n",
       "              )\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): ProjectInOut(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Attention(\n",
       "                    (attend): Softmax(dim=-1)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (to_q): Linear(in_features=192, out_features=768, bias=False)\n",
       "                    (to_kv): Linear(in_features=192, out_features=1536, bias=False)\n",
       "                    (to_out): Sequential(\n",
       "                      (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "                      (1): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (project_in): Identity()\n",
       "                (project_out): Identity()\n",
       "              )\n",
       "              (1): ProjectInOut(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Attention(\n",
       "                    (attend): Softmax(dim=-1)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (to_q): Linear(in_features=192, out_features=768, bias=False)\n",
       "                    (to_kv): Linear(in_features=192, out_features=1536, bias=False)\n",
       "                    (to_out): Sequential(\n",
       "                      (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "                      (1): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (project_in): Identity()\n",
       "                (project_out): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): Transformer(\n",
       "          (layers): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (attend): Softmax(dim=-1)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (to_q): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=192, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=192, bias=True)\n",
       "                    (1): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=192, out_features=2048, bias=True)\n",
       "                    (1): GELU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=2048, out_features=192, bias=True)\n",
       "                    (4): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (attend): Softmax(dim=-1)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (to_q): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=192, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=192, bias=True)\n",
       "                    (1): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=192, out_features=2048, bias=True)\n",
       "                    (1): GELU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=2048, out_features=192, bias=True)\n",
       "                    (4): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): Transformer(\n",
       "          (layers): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (attend): Softmax(dim=-1)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (to_q): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=192, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=192, bias=True)\n",
       "                    (1): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=192, out_features=2048, bias=True)\n",
       "                    (1): GELU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=2048, out_features=192, bias=True)\n",
       "                    (4): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): Attention(\n",
       "                  (attend): Softmax(dim=-1)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (to_q): Linear(in_features=192, out_features=512, bias=False)\n",
       "                  (to_kv): Linear(in_features=192, out_features=1024, bias=False)\n",
       "                  (to_out): Sequential(\n",
       "                    (0): Linear(in_features=512, out_features=192, bias=True)\n",
       "                    (1): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): PreNorm(\n",
       "                (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=192, out_features=2048, bias=True)\n",
       "                    (1): GELU()\n",
       "                    (2): Dropout(p=0.1, inplace=False)\n",
       "                    (3): Linear(in_features=2048, out_features=192, bias=True)\n",
       "                    (4): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): CrossTransformer(\n",
       "          (layers): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): ProjectInOut(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Attention(\n",
       "                    (attend): Softmax(dim=-1)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (to_q): Linear(in_features=192, out_features=768, bias=False)\n",
       "                    (to_kv): Linear(in_features=192, out_features=1536, bias=False)\n",
       "                    (to_out): Sequential(\n",
       "                      (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "                      (1): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (project_in): Identity()\n",
       "                (project_out): Identity()\n",
       "              )\n",
       "              (1): ProjectInOut(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Attention(\n",
       "                    (attend): Softmax(dim=-1)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (to_q): Linear(in_features=192, out_features=768, bias=False)\n",
       "                    (to_kv): Linear(in_features=192, out_features=1536, bias=False)\n",
       "                    (to_out): Sequential(\n",
       "                      (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "                      (1): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (project_in): Identity()\n",
       "                (project_out): Identity()\n",
       "              )\n",
       "            )\n",
       "            (1): ModuleList(\n",
       "              (0): ProjectInOut(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Attention(\n",
       "                    (attend): Softmax(dim=-1)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (to_q): Linear(in_features=192, out_features=768, bias=False)\n",
       "                    (to_kv): Linear(in_features=192, out_features=1536, bias=False)\n",
       "                    (to_out): Sequential(\n",
       "                      (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "                      (1): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (project_in): Identity()\n",
       "                (project_out): Identity()\n",
       "              )\n",
       "              (1): ProjectInOut(\n",
       "                (fn): PreNorm(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (fn): Attention(\n",
       "                    (attend): Softmax(dim=-1)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    (to_q): Linear(in_features=192, out_features=768, bias=False)\n",
       "                    (to_kv): Linear(in_features=192, out_features=1536, bias=False)\n",
       "                    (to_out): Sequential(\n",
       "                      (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "                      (1): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "                (project_in): Identity()\n",
       "                (project_out): Identity()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp_head): Sequential(\n",
       "    (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=384, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = crossvit_base_224().to(device)\n",
    "model = model.half() if fp16 else model\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min. Req. Throughput: 648.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput with batch_size 20: 198.24\n",
      "Min. Req. Throughput: 648.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:23<00:00,  4.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput with batch_size 40: 171.94\n",
      "Min. Req. Throughput: 648.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:42<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput with batch_size 60: 142.93\n",
      "Min. Req. Throughput: 648.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [00:14<00:43,  1.71it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\repos\\belugas\\notebooks\\throughput.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/throughput.ipynb#ch0000015?line=12'>13</a>\u001b[0m _ \u001b[39m=\u001b[39m model(dummy_input, dummy_input)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/throughput.ipynb#ch0000015?line=13'>14</a>\u001b[0m ender\u001b[39m.\u001b[39mrecord()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/throughput.ipynb#ch0000015?line=14'>15</a>\u001b[0m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49msynchronize()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/throughput.ipynb#ch0000015?line=15'>16</a>\u001b[0m curr_time \u001b[39m=\u001b[39m starter\u001b[39m.\u001b[39melapsed_time(ender)\u001b[39m/\u001b[39m\u001b[39m1000\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/throughput.ipynb#ch0000015?line=16'>17</a>\u001b[0m total_time \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m curr_time\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\torch\\cuda\\__init__.py:494\u001b[0m, in \u001b[0;36msynchronize\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/cuda/__init__.py?line=485'>486</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Waits for all kernels in all streams on a CUDA device to complete.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/cuda/__init__.py?line=486'>487</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/cuda/__init__.py?line=487'>488</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/cuda/__init__.py?line=490'>491</a>\u001b[0m \u001b[39m        if :attr:`device` is ``None`` (default).\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/cuda/__init__.py?line=491'>492</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/cuda/__init__.py?line=492'>493</a>\u001b[0m _lazy_init()\n\u001b[1;32m--> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/cuda/__init__.py?line=493'>494</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/cuda/__init__.py?line=494'>495</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_cuda_synchronize()\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\torch\\cuda\\__init__.py:280\u001b[0m, in \u001b[0;36mdevice.__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/cuda/__init__.py?line=276'>277</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting():\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/cuda/__init__.py?line=277'>278</a>\u001b[0m         _lazy_init()\n\u001b[1;32m--> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/cuda/__init__.py?line=279'>280</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mtype\u001b[39m: Any, value: Any, traceback: Any):\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/cuda/__init__.py?line=280'>281</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprev_idx \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39midx:\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/cuda/__init__.py?line=281'>282</a>\u001b[0m         torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mset_device(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprev_idx)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 0\n",
    "inc_bs = 10\n",
    "print(f'Min. Req. Throughput: {(7_000_000/(3*60*60)):.2f}')\n",
    "while True:\n",
    "    batch_size += inc_bs\n",
    "    dummy_input = torch.randn(batch_size, 3,224,224, dtype=input_dtype).to(device)\n",
    "    repetitions = 100\n",
    "    total_time = 0\n",
    "    with torch.no_grad():\n",
    "        for rep in tqdm(range(0,repetitions)):\n",
    "            starter, ender = torch.cuda.Event(enable_timing=True),   torch.cuda.Event(enable_timing=True)\n",
    "            starter.record()\n",
    "            _ = model(dummy_input, dummy_input)\n",
    "            ender.record()\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)/1000\n",
    "            total_time += curr_time\n",
    "    Throughput = (repetitions*batch_size)/total_time\n",
    "    print(f'Throughput with batch_size {batch_size}: {Throughput:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e460829be586a745d810aec71d83684bd38b76dd3b8d2db700ccf14d30953fce"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
