{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting timm\n",
      "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
      "\u001b[K     |████████████████████████████████| 431 kB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from timm) (0.13.0a0)\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.8/site-packages (from timm) (1.12.0a0+bd13bc6)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.4->timm) (4.1.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchvision->timm) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.8/site-packages (from torchvision->timm) (9.0.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision->timm) (1.22.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->timm) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->timm) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->timm) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->torchvision->timm) (2.0.12)\n",
      "Installing collected packages: timm\n",
      "Successfully installed timm-0.5.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch_tensorrt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch_tensorrt/__init__.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython 2 has reached end-of-life and is not supported by Torch-TensorRT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mctypes\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_tensorrt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_tensorrt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch_tensorrt\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.vision_transformer import Mlp, Block\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from torchvision import transforms as T\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Obtaining torch from git+https://github.com/pytorch/pytorch#egg=torch\n",
      "  Updating ./src/torch clone\n",
      "  Running command git fetch -q --tags\n",
      "  Running command git reset --hard -q 8c88a55d447ea77ef4d3137dbc83bf315a5971ba\n",
      "  Running command git submodule update --init --recursive -q\n",
      "Installing collected packages: torch\n",
      "  Running setup.py develop for torch\n",
      "\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: /opt/conda/bin/python3.8 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/wd/notebooks/src/torch/setup.py'\"'\"'; __file__='\"'\"'/wd/notebooks/src/torch/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' develop --no-deps\n",
      "         cwd: /wd/notebooks/src/torch/\n",
      "    Complete output (1885 lines):\n",
      "    -- The CXX compiler identification is GNU 9.4.0\n",
      "    -- The C compiler identification is GNU 9.4.0\n",
      "    -- Detecting CXX compiler ABI info\n",
      "    -- Detecting CXX compiler ABI info - done\n",
      "    -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "    -- Detecting CXX compile features\n",
      "    -- Detecting CXX compile features - done\n",
      "    -- Detecting C compiler ABI info\n",
      "    -- Detecting C compiler ABI info - done\n",
      "    -- Check for working C compiler: /usr/bin/cc - skipped\n",
      "    -- Detecting C compile features\n",
      "    -- Detecting C compile features - done\n",
      "    -- Not forcing any particular BLAS to be found\n",
      "    CMake Warning (dev) at /opt/conda/share/cmake-3.22/Modules/CMakeDependentOption.cmake:84 (message):\n",
      "      Policy CMP0127 is not set: cmake_dependent_option() supports full Condition\n",
      "      Syntax.  Run \"cmake --help-policy CMP0127\" for policy details.  Use the\n",
      "      cmake_policy command to set the policy and suppress this warning.\n",
      "    Call Stack (most recent call first):\n",
      "      CMakeLists.txt:255 (cmake_dependent_option)\n",
      "    This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "    \n",
      "    CMake Warning (dev) at /opt/conda/share/cmake-3.22/Modules/CMakeDependentOption.cmake:84 (message):\n",
      "      Policy CMP0127 is not set: cmake_dependent_option() supports full Condition\n",
      "      Syntax.  Run \"cmake --help-policy CMP0127\" for policy details.  Use the\n",
      "      cmake_policy command to set the policy and suppress this warning.\n",
      "    Call Stack (most recent call first):\n",
      "      CMakeLists.txt:286 (cmake_dependent_option)\n",
      "    This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "    \n",
      "    -- Could not find ccache. Consider installing ccache to speed up compilation.\n",
      "    -- Performing Test COMPILER_WORKS\n",
      "    -- Performing Test COMPILER_WORKS - Success\n",
      "    -- Performing Test SUPPORT_GLIBCXX_USE_C99\n",
      "    -- Performing Test SUPPORT_GLIBCXX_USE_C99 - Success\n",
      "    -- Performing Test CAFFE2_EXCEPTION_PTR_SUPPORTED\n",
      "    -- Performing Test CAFFE2_EXCEPTION_PTR_SUPPORTED - Success\n",
      "    -- std::exception_ptr is supported.\n",
      "    -- Performing Test CAFFE2_NEED_TO_TURN_OFF_DEPRECATION_WARNING\n",
      "    -- Performing Test CAFFE2_NEED_TO_TURN_OFF_DEPRECATION_WARNING - Success\n",
      "    -- Performing Test C_HAS_AVX_1\n",
      "    -- Performing Test C_HAS_AVX_1 - Failed\n",
      "    -- Performing Test C_HAS_AVX_2\n",
      "    -- Performing Test C_HAS_AVX_2 - Success\n",
      "    -- Performing Test C_HAS_AVX2_1\n",
      "    -- Performing Test C_HAS_AVX2_1 - Failed\n",
      "    -- Performing Test C_HAS_AVX2_2\n",
      "    -- Performing Test C_HAS_AVX2_2 - Success\n",
      "    -- Performing Test C_HAS_AVX512_1\n",
      "    -- Performing Test C_HAS_AVX512_1 - Failed\n",
      "    -- Performing Test C_HAS_AVX512_2\n",
      "    -- Performing Test C_HAS_AVX512_2 - Success\n",
      "    -- Performing Test CXX_HAS_AVX_1\n",
      "    -- Performing Test CXX_HAS_AVX_1 - Failed\n",
      "    -- Performing Test CXX_HAS_AVX_2\n",
      "    -- Performing Test CXX_HAS_AVX_2 - Success\n",
      "    -- Performing Test CXX_HAS_AVX2_1\n",
      "    -- Performing Test CXX_HAS_AVX2_1 - Failed\n",
      "    -- Performing Test CXX_HAS_AVX2_2\n",
      "    -- Performing Test CXX_HAS_AVX2_2 - Success\n",
      "    -- Performing Test CXX_HAS_AVX512_1\n",
      "    -- Performing Test CXX_HAS_AVX512_1 - Failed\n",
      "    -- Performing Test CXX_HAS_AVX512_2\n",
      "    -- Performing Test CXX_HAS_AVX512_2 - Success\n",
      "    -- Current compiler supports avx2 extension. Will build perfkernels.\n",
      "    -- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX512_EXTENSIONS\n",
      "    -- Performing Test CAFFE2_COMPILER_SUPPORTS_AVX512_EXTENSIONS - Success\n",
      "    -- Current compiler supports avx512f extension. Will build fbgemm.\n",
      "    -- Performing Test COMPILER_SUPPORTS_HIDDEN_VISIBILITY\n",
      "    -- Performing Test COMPILER_SUPPORTS_HIDDEN_VISIBILITY - Success\n",
      "    -- Performing Test COMPILER_SUPPORTS_HIDDEN_INLINE_VISIBILITY\n",
      "    -- Performing Test COMPILER_SUPPORTS_HIDDEN_INLINE_VISIBILITY - Success\n",
      "    -- Performing Test COMPILER_SUPPORTS_RDYNAMIC\n",
      "    -- Performing Test COMPILER_SUPPORTS_RDYNAMIC - Success\n",
      "    -- Found CUDA: /usr/local/cuda (found version \"11.6\")\n",
      "    -- The CUDA compiler identification is NVIDIA 11.6.124\n",
      "    -- Detecting CUDA compiler ABI info\n",
      "    -- Detecting CUDA compiler ABI info - done\n",
      "    -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
      "    -- Detecting CUDA compile features\n",
      "    -- Detecting CUDA compile features - done\n",
      "    -- Caffe2: CUDA detected: 11.6\n",
      "    -- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc\n",
      "    -- Caffe2: CUDA toolkit directory: /usr/local/cuda\n",
      "    -- Caffe2: Header version is: 11.6\n",
      "    -- Found CUDNN: /usr/lib/x86_64-linux-gnu/libcudnn.so\n",
      "    -- Found cuDNN: v8.4.0  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libcudnn.so)\n",
      "    -- /usr/local/cuda/lib64/libnvrtc.so shorthash is 4dd39364\n",
      "    CMake Warning at cmake/public/utils.cmake:385 (message):\n",
      "      In the future we will require one to explicitly pass TORCH_CUDA_ARCH_LIST\n",
      "      to cmake instead of implicitly setting it as an env variable.  This will\n",
      "      become a FATAL_ERROR in future version of pytorch.\n",
      "    Call Stack (most recent call first):\n",
      "      cmake/public/cuda.cmake:440 (torch_cuda_get_nvcc_gencode_flag)\n",
      "      cmake/Dependencies.cmake:43 (include)\n",
      "      CMakeLists.txt:692 (include)\n",
      "    \n",
      "    \n",
      "    -- Added CUDA NVCC flags for: -gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_86,code=compute_86\n",
      "    -- Building using own protobuf under third_party per request.\n",
      "    -- Use custom protobuf build.\n",
      "    --\n",
      "    -- 3.13.0.0\n",
      "    -- Looking for pthread.h\n",
      "    -- Looking for pthread.h - found\n",
      "    -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "    -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n",
      "    -- Check if compiler accepts -pthread\n",
      "    -- Check if compiler accepts -pthread - yes\n",
      "    -- Found Threads: TRUE\n",
      "    -- Performing Test protobuf_HAVE_BUILTIN_ATOMICS\n",
      "    -- Performing Test protobuf_HAVE_BUILTIN_ATOMICS - Success\n",
      "    -- Caffe2 protobuf include directory: $<BUILD_INTERFACE:/wd/notebooks/src/torch/third_party/protobuf/src>$<INSTALL_INTERFACE:include>\n",
      "    -- Trying to find preferred BLAS backend of choice: MKL\n",
      "    -- MKL_THREADING = OMP\n",
      "    -- Looking for sys/types.h\n",
      "    -- Looking for sys/types.h - found\n",
      "    -- Looking for stdint.h\n",
      "    -- Looking for stdint.h - found\n",
      "    -- Looking for stddef.h\n",
      "    -- Looking for stddef.h - found\n",
      "    -- Check size of void*\n",
      "    -- Check size of void* - done\n",
      "    CMake Warning (dev) at /opt/conda/share/cmake-3.22/Modules/FindPackageHandleStandardArgs.cmake:438 (message):\n",
      "      The package name passed to `find_package_handle_standard_args` (OpenMP_C)\n",
      "      does not match the name of the calling package (OpenMP).  This can lead to\n",
      "      problems in calling code that expects `find_package` result variables\n",
      "      (e.g., `_FOUND`) to follow a certain pattern.\n",
      "    Call Stack (most recent call first):\n",
      "      cmake/Modules/FindOpenMP.cmake:576 (find_package_handle_standard_args)\n",
      "      cmake/Modules/FindMKL.cmake:233 (FIND_PACKAGE)\n",
      "      cmake/Modules/FindMKL.cmake:328 (CHECK_ALL_LIBRARIES)\n",
      "      cmake/Dependencies.cmake:206 (find_package)\n",
      "      CMakeLists.txt:692 (include)\n",
      "    This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "    \n",
      "    CMake Warning (dev) at /opt/conda/share/cmake-3.22/Modules/FindPackageHandleStandardArgs.cmake:438 (message):\n",
      "      The package name passed to `find_package_handle_standard_args` (OpenMP_CXX)\n",
      "      does not match the name of the calling package (OpenMP).  This can lead to\n",
      "      problems in calling code that expects `find_package` result variables\n",
      "      (e.g., `_FOUND`) to follow a certain pattern.\n",
      "    Call Stack (most recent call first):\n",
      "      cmake/Modules/FindOpenMP.cmake:576 (find_package_handle_standard_args)\n",
      "      cmake/Modules/FindMKL.cmake:233 (FIND_PACKAGE)\n",
      "      cmake/Modules/FindMKL.cmake:328 (CHECK_ALL_LIBRARIES)\n",
      "      cmake/Dependencies.cmake:206 (find_package)\n",
      "      CMakeLists.txt:692 (include)\n",
      "    This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "    \n",
      "    -- Looking for cblas_sgemm\n",
      "    -- Looking for cblas_sgemm - found\n",
      "    -- MKL libraries: /opt/conda/lib/libmkl_intel_lp64.so;/opt/conda/lib/libmkl_gnu_thread.so;/opt/conda/lib/libmkl_core.so;-fopenmp;/usr/lib/x86_64-linux-gnu/libpthread.so;/usr/lib/x86_64-linux-gnu/libm.so;/usr/lib/x86_64-linux-gnu/libdl.so\n",
      "    -- MKL include directory: /opt/conda/include\n",
      "    -- MKL OpenMP type: GNU\n",
      "    -- MKL OpenMP library: -fopenmp\n",
      "    -- The ASM compiler identification is GNU\n",
      "    -- Found assembler: /usr/bin/cc\n",
      "    -- Brace yourself, we are building NNPACK\n",
      "    -- Performing Test NNPACK_ARCH_IS_X86_32\n",
      "    -- Performing Test NNPACK_ARCH_IS_X86_32 - Failed\n",
      "    -- Found PythonInterp: /opt/conda/bin/python3.8 (found version \"3.8.13\")\n",
      "    -- NNPACK backend is x86-64\n",
      "    -- Found Python: /opt/conda/bin/python3.8 (found version \"3.8.13\") found components: Interpreter\n",
      "    -- Failed to find LLVM FileCheck\n",
      "    -- Found Git: /usr/bin/git (found version \"2.25.1\")\n",
      "    -- git version: v1.5.5 normalized to 1.5.5\n",
      "    -- Version: 1.5.5\n",
      "    -- Performing Test HAVE_CXX_FLAG_STD_CXX11\n",
      "    -- Performing Test HAVE_CXX_FLAG_STD_CXX11 - Success\n",
      "    -- Performing Test HAVE_CXX_FLAG_WALL\n",
      "    -- Performing Test HAVE_CXX_FLAG_WALL - Success\n",
      "    -- Performing Test HAVE_CXX_FLAG_WEXTRA\n",
      "    -- Performing Test HAVE_CXX_FLAG_WEXTRA - Success\n",
      "    -- Performing Test HAVE_CXX_FLAG_WSHADOW\n",
      "    -- Performing Test HAVE_CXX_FLAG_WSHADOW - Success\n",
      "    -- Performing Test HAVE_CXX_FLAG_WERROR\n",
      "    -- Performing Test HAVE_CXX_FLAG_WERROR - Success\n",
      "    -- Performing Test HAVE_CXX_FLAG_WSUGGEST_OVERRIDE\n",
      "    -- Performing Test HAVE_CXX_FLAG_WSUGGEST_OVERRIDE - Success\n",
      "    -- Performing Test HAVE_CXX_FLAG_PEDANTIC\n",
      "    -- Performing Test HAVE_CXX_FLAG_PEDANTIC - Success\n",
      "    -- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS\n",
      "    -- Performing Test HAVE_CXX_FLAG_PEDANTIC_ERRORS - Success\n",
      "    -- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32\n",
      "    -- Performing Test HAVE_CXX_FLAG_WSHORTEN_64_TO_32 - Failed\n",
      "    -- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING\n",
      "    -- Performing Test HAVE_CXX_FLAG_FSTRICT_ALIASING - Success\n",
      "    -- Performing Test HAVE_CXX_FLAG_WNO_DEPRECATED_DECLARATIONS\n",
      "    -- Performing Test HAVE_CXX_FLAG_WNO_DEPRECATED_DECLARATIONS - Success\n",
      "    -- Performing Test HAVE_CXX_FLAG_WNO_DEPRECATED\n",
      "    -- Performing Test HAVE_CXX_FLAG_WNO_DEPRECATED - Success\n",
      "    -- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING\n",
      "    -- Performing Test HAVE_CXX_FLAG_WSTRICT_ALIASING - Success\n",
      "    -- Performing Test HAVE_CXX_FLAG_WD654\n",
      "    -- Performing Test HAVE_CXX_FLAG_WD654 - Failed\n",
      "    -- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY\n",
      "    -- Performing Test HAVE_CXX_FLAG_WTHREAD_SAFETY - Failed\n",
      "    -- Performing Test HAVE_CXX_FLAG_COVERAGE\n",
      "    -- Performing Test HAVE_CXX_FLAG_COVERAGE - Success\n",
      "    -- Performing Test HAVE_STD_REGEX\n",
      "    -- Performing Test HAVE_STD_REGEX\n",
      "    -- Performing Test HAVE_STD_REGEX -- success\n",
      "    -- Performing Test HAVE_GNU_POSIX_REGEX\n",
      "    -- Performing Test HAVE_GNU_POSIX_REGEX\n",
      "    -- Performing Test HAVE_GNU_POSIX_REGEX -- failed to compile\n",
      "    -- Performing Test HAVE_POSIX_REGEX\n",
      "    -- Performing Test HAVE_POSIX_REGEX\n",
      "    -- Performing Test HAVE_POSIX_REGEX -- success\n",
      "    -- Performing Test HAVE_STEADY_CLOCK\n",
      "    -- Performing Test HAVE_STEADY_CLOCK\n",
      "    -- Performing Test HAVE_STEADY_CLOCK -- success\n",
      "    -- Performing Test COMPILER_SUPPORTS_AVX512\n",
      "    -- Performing Test COMPILER_SUPPORTS_AVX512 - Success\n",
      "    CMake Warning (dev) at /opt/conda/share/cmake-3.22/Modules/FindPackageHandleStandardArgs.cmake:438 (message):\n",
      "      The package name passed to `find_package_handle_standard_args` (OpenMP_C)\n",
      "      does not match the name of the calling package (OpenMP).  This can lead to\n",
      "      problems in calling code that expects `find_package` result variables\n",
      "      (e.g., `_FOUND`) to follow a certain pattern.\n",
      "    Call Stack (most recent call first):\n",
      "      cmake/Modules/FindOpenMP.cmake:576 (find_package_handle_standard_args)\n",
      "      third_party/fbgemm/CMakeLists.txt:61 (find_package)\n",
      "    This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "    \n",
      "    -- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
      "    CMake Warning (dev) at /opt/conda/share/cmake-3.22/Modules/FindPackageHandleStandardArgs.cmake:438 (message):\n",
      "      The package name passed to `find_package_handle_standard_args` (OpenMP_CXX)\n",
      "      does not match the name of the calling package (OpenMP).  This can lead to\n",
      "      problems in calling code that expects `find_package` result variables\n",
      "      (e.g., `_FOUND`) to follow a certain pattern.\n",
      "    Call Stack (most recent call first):\n",
      "      cmake/Modules/FindOpenMP.cmake:576 (find_package_handle_standard_args)\n",
      "      third_party/fbgemm/CMakeLists.txt:61 (find_package)\n",
      "    This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "    \n",
      "    -- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
      "    -- Found OpenMP: TRUE (found version \"4.5\")\n",
      "    CMake Warning at third_party/fbgemm/CMakeLists.txt:63 (message):\n",
      "      OpenMP found! OpenMP_C_INCLUDE_DIRS =\n",
      "    \n",
      "    \n",
      "    CMake Warning at third_party/fbgemm/CMakeLists.txt:162 (message):\n",
      "      ==========\n",
      "    \n",
      "    \n",
      "    CMake Warning at third_party/fbgemm/CMakeLists.txt:163 (message):\n",
      "      CMAKE_BUILD_TYPE = Release\n",
      "    \n",
      "    \n",
      "    CMake Warning at third_party/fbgemm/CMakeLists.txt:164 (message):\n",
      "      CMAKE_CXX_FLAGS_DEBUG is -g\n",
      "    \n",
      "    \n",
      "    CMake Warning at third_party/fbgemm/CMakeLists.txt:165 (message):\n",
      "      CMAKE_CXX_FLAGS_RELEASE is -O3 -DNDEBUG\n",
      "    \n",
      "    \n",
      "    CMake Warning at third_party/fbgemm/CMakeLists.txt:166 (message):\n",
      "      ==========\n",
      "    \n",
      "    \n",
      "    -- Performing Test __CxxFlag__fno_threadsafe_statics\n",
      "    -- Performing Test __CxxFlag__fno_threadsafe_statics - Success\n",
      "    -- Performing Test __CxxFlag__fno_semantic_interposition\n",
      "    -- Performing Test __CxxFlag__fno_semantic_interposition - Success\n",
      "    -- Performing Test __CxxFlag__fmerge_all_constants\n",
      "    -- Performing Test __CxxFlag__fmerge_all_constants - Success\n",
      "    -- Performing Test __CxxFlag__fno_enforce_eh_specs\n",
      "    -- Performing Test __CxxFlag__fno_enforce_eh_specs - Success\n",
      "    ** AsmJit Summary **\n",
      "       ASMJIT_DIR=/wd/notebooks/src/torch/third_party/fbgemm/third_party/asmjit\n",
      "       ASMJIT_TEST=FALSE\n",
      "       ASMJIT_TARGET_TYPE=STATIC\n",
      "       ASMJIT_DEPS=pthread;rt\n",
      "       ASMJIT_LIBS=asmjit;pthread;rt\n",
      "       ASMJIT_CFLAGS=-DASMJIT_STATIC\n",
      "       ASMJIT_PRIVATE_CFLAGS=-Wall;-Wextra;-Wconversion;-fno-math-errno;-fno-threadsafe-statics;-fno-semantic-interposition;-DASMJIT_STATIC\n",
      "       ASMJIT_PRIVATE_CFLAGS_DBG=\n",
      "       ASMJIT_PRIVATE_CFLAGS_REL=-O2;-fmerge-all-constants;-fno-enforce-eh-specs\n",
      "    -- Found Numa: /usr/include\n",
      "    -- Found Numa  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libnuma.so)\n",
      "    -- Using third party subdirectory Eigen.\n",
      "    -- Found PythonInterp: /opt/conda/bin/python3.8 (found suitable version \"3.8.13\", minimum required is \"3.0\")\n",
      "    -- Found PythonLibs: /opt/conda/lib/libpython3.8.a (found suitable version \"3.8.13\", minimum required is \"3.0\")\n",
      "    -- Using third_party/pybind11.\n",
      "    -- pybind11 include dirs: /wd/notebooks/src/torch/cmake/../third_party/pybind11/include\n",
      "    -- Found MPI_C: /opt/hpcx/ompi/lib/libmpi.so (found version \"3.1\")\n",
      "    -- Found MPI_CXX: /opt/hpcx/ompi/lib/libmpi.so (found version \"3.1\")\n",
      "    -- Found MPI: TRUE (found version \"3.1\")\n",
      "    -- MPI support found\n",
      "    -- MPI compile flags: -pthread\n",
      "    -- MPI include path: /opt/hpcx/ompi/include/opt/hpcx/ompi/include/openmpi/opt/hpcx/ompi/include/openmpi/opal/mca/hwloc/hwloc201/hwloc/include/opt/hpcx/ompi/include/openmpi/opal/mca/event/libevent2022/libevent/opt/hpcx/ompi/include/openmpi/opal/mca/event/libevent2022/libevent/include\n",
      "    -- MPI LINK flags path: -Wl,-rpath -Wl,/opt/hpcx/ompi/lib -Wl,--enable-new-dtags -pthread\n",
      "    -- MPI libraries: /opt/hpcx/ompi/lib/libmpi.so\n",
      "    -- Found OpenMPI with CUDA support built.\n",
      "    CMake Warning (dev) at /opt/conda/share/cmake-3.22/Modules/FindPackageHandleStandardArgs.cmake:438 (message):\n",
      "      The package name passed to `find_package_handle_standard_args` (OpenMP_C)\n",
      "      does not match the name of the calling package (OpenMP).  This can lead to\n",
      "      problems in calling code that expects `find_package` result variables\n",
      "      (e.g., `_FOUND`) to follow a certain pattern.\n",
      "    Call Stack (most recent call first):\n",
      "      cmake/Modules/FindOpenMP.cmake:576 (find_package_handle_standard_args)\n",
      "      cmake/Dependencies.cmake:1199 (find_package)\n",
      "      CMakeLists.txt:692 (include)\n",
      "    This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "    \n",
      "    CMake Warning (dev) at /opt/conda/share/cmake-3.22/Modules/FindPackageHandleStandardArgs.cmake:438 (message):\n",
      "      The package name passed to `find_package_handle_standard_args` (OpenMP_CXX)\n",
      "      does not match the name of the calling package (OpenMP).  This can lead to\n",
      "      problems in calling code that expects `find_package` result variables\n",
      "      (e.g., `_FOUND`) to follow a certain pattern.\n",
      "    Call Stack (most recent call first):\n",
      "      cmake/Modules/FindOpenMP.cmake:576 (find_package_handle_standard_args)\n",
      "      cmake/Dependencies.cmake:1199 (find_package)\n",
      "      CMakeLists.txt:692 (include)\n",
      "    This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "    \n",
      "    -- Adding OpenMP CXX_FLAGS: -fopenmp\n",
      "    -- Will link against OpenMP libraries: /usr/lib/gcc/x86_64-linux-gnu/9/libgomp.so;/usr/lib/x86_64-linux-gnu/libpthread.so\n",
      "    CMake Warning at cmake/public/utils.cmake:385 (message):\n",
      "      In the future we will require one to explicitly pass TORCH_CUDA_ARCH_LIST\n",
      "      to cmake instead of implicitly setting it as an env variable.  This will\n",
      "      become a FATAL_ERROR in future version of pytorch.\n",
      "    Call Stack (most recent call first):\n",
      "      cmake/External/nccl.cmake:13 (torch_cuda_get_nvcc_gencode_flag)\n",
      "      cmake/Dependencies.cmake:1347 (include)\n",
      "      CMakeLists.txt:692 (include)\n",
      "    \n",
      "    \n",
      "    CMake Warning at cmake/External/nccl.cmake:66 (message):\n",
      "      Objcopy version is too old to support NCCL library slimming\n",
      "    Call Stack (most recent call first):\n",
      "      cmake/Dependencies.cmake:1347 (include)\n",
      "      CMakeLists.txt:692 (include)\n",
      "    \n",
      "    \n",
      "    -- Found CUB: /usr/local/cuda/include\n",
      "    -- Converting CMAKE_CUDA_FLAGS to CUDA_NVCC_FLAGS:\n",
      "        CUDA_NVCC_FLAGS                = -Xfatbin;-compress-all;-DONNX_NAMESPACE=onnx_torch;-gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_86,code=compute_86;-Xcudafe;--diag_suppress=cc_clobber_ignored,--diag_suppress=integer_sign_change,--diag_suppress=useless_using_declaration,--diag_suppress=set_but_not_used,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=implicit_return_from_non_void_function,--diag_suppress=unsigned_compare_with_zero,--diag_suppress=declared_but_not_referenced,--diag_suppress=bad_friend_decl;--expt-relaxed-constexpr;--expt-extended-lambda\n",
      "        CUDA_NVCC_FLAGS_DEBUG          = -g;-lineinfo;--source-in-ptx\n",
      "        CUDA_NVCC_FLAGS_RELEASE        = -O3;-DNDEBUG\n",
      "        CUDA_NVCC_FLAGS_RELWITHDEBINFO = -g;-lineinfo;--source-in-ptx\n",
      "        CUDA_NVCC_FLAGS_MINSIZEREL     = -O1;-DNDEBUG\n",
      "    -- Performing Test UV_LINT_W4\n",
      "    -- Performing Test UV_LINT_W4 - Failed\n",
      "    -- Performing Test UV_LINT_NO_UNUSED_PARAMETER_MSVC\n",
      "    -- Performing Test UV_LINT_NO_UNUSED_PARAMETER_MSVC - Failed\n",
      "    -- Performing Test UV_LINT_NO_CONDITIONAL_CONSTANT_MSVC\n",
      "    -- Performing Test UV_LINT_NO_CONDITIONAL_CONSTANT_MSVC - Failed\n",
      "    -- Performing Test UV_LINT_NO_NONSTANDARD_MSVC\n",
      "    -- Performing Test UV_LINT_NO_NONSTANDARD_MSVC - Failed\n",
      "    -- Performing Test UV_LINT_NO_NONSTANDARD_EMPTY_TU_MSVC\n",
      "    -- Performing Test UV_LINT_NO_NONSTANDARD_EMPTY_TU_MSVC - Failed\n",
      "    -- Performing Test UV_LINT_NO_NONSTANDARD_FILE_SCOPE_MSVC\n",
      "    -- Performing Test UV_LINT_NO_NONSTANDARD_FILE_SCOPE_MSVC - Failed\n",
      "    -- Performing Test UV_LINT_NO_NONSTANDARD_NONSTATIC_DLIMPORT_MSVC\n",
      "    -- Performing Test UV_LINT_NO_NONSTANDARD_NONSTATIC_DLIMPORT_MSVC - Failed\n",
      "    -- Performing Test UV_LINT_NO_HIDES_LOCAL\n",
      "    -- Performing Test UV_LINT_NO_HIDES_LOCAL - Failed\n",
      "    -- Performing Test UV_LINT_NO_HIDES_PARAM\n",
      "    -- Performing Test UV_LINT_NO_HIDES_PARAM - Failed\n",
      "    -- Performing Test UV_LINT_NO_HIDES_GLOBAL\n",
      "    -- Performing Test UV_LINT_NO_HIDES_GLOBAL - Failed\n",
      "    -- Performing Test UV_LINT_NO_CONDITIONAL_ASSIGNMENT_MSVC\n",
      "    -- Performing Test UV_LINT_NO_CONDITIONAL_ASSIGNMENT_MSVC - Failed\n",
      "    -- Performing Test UV_LINT_NO_UNSAFE_MSVC\n",
      "    -- Performing Test UV_LINT_NO_UNSAFE_MSVC - Failed\n",
      "    -- Performing Test UV_LINT_WALL\n",
      "    -- Performing Test UV_LINT_WALL - Success\n",
      "    -- Performing Test UV_LINT_NO_UNUSED_PARAMETER\n",
      "    -- Performing Test UV_LINT_NO_UNUSED_PARAMETER - Success\n",
      "    -- Performing Test UV_LINT_STRICT_PROTOTYPES\n",
      "    -- Performing Test UV_LINT_STRICT_PROTOTYPES - Success\n",
      "    -- Performing Test UV_LINT_EXTRA\n",
      "    -- Performing Test UV_LINT_EXTRA - Success\n",
      "    -- Performing Test UV_LINT_UTF8_MSVC\n",
      "    -- Performing Test UV_LINT_UTF8_MSVC - Failed\n",
      "    -- Performing Test UV_F_STRICT_ALIASING\n",
      "    -- Performing Test UV_F_STRICT_ALIASING - Success\n",
      "    -- summary of build options:\n",
      "        Install prefix:  /wd/notebooks/src/torch/torch\n",
      "        Target system:   Linux\n",
      "        Compiler:\n",
      "          C compiler:    /usr/bin/cc\n",
      "          CFLAGS:          -fopenmp\n",
      "    \n",
      "    -- Found uv: 1.38.1 (found version \"1.38.1\")\n",
      "    -- Converting CMAKE_CUDA_FLAGS to CUDA_NVCC_FLAGS:\n",
      "        CUDA_NVCC_FLAGS                = -Xfatbin;-compress-all;-DONNX_NAMESPACE=onnx_torch;-gencode;arch=compute_52,code=sm_52;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_86,code=compute_86;-Xcudafe;--diag_suppress=cc_clobber_ignored,--diag_suppress=integer_sign_change,--diag_suppress=useless_using_declaration,--diag_suppress=set_but_not_used,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=implicit_return_from_non_void_function,--diag_suppress=unsigned_compare_with_zero,--diag_suppress=declared_but_not_referenced,--diag_suppress=bad_friend_decl;--expt-relaxed-constexpr;--expt-extended-lambda\n",
      "        CUDA_NVCC_FLAGS_DEBUG          = -g;-lineinfo;--source-in-ptx\n",
      "        CUDA_NVCC_FLAGS_RELEASE        = -O3;-DNDEBUG\n",
      "        CUDA_NVCC_FLAGS_RELWITHDEBINFO = -g;-lineinfo;--source-in-ptx\n",
      "        CUDA_NVCC_FLAGS_MINSIZEREL     = -O1;-DNDEBUG\n",
      "    CMake Warning (dev) at third_party/gloo/CMakeLists.txt:21 (option):\n",
      "      Policy CMP0077 is not set: option() honors normal variables.  Run \"cmake\n",
      "      --help-policy CMP0077\" for policy details.  Use the cmake_policy command to\n",
      "      set the policy and suppress this warning.\n",
      "    \n",
      "      For compatibility with older versions of CMake, option is clearing the\n",
      "      normal variable 'BUILD_BENCHMARK'.\n",
      "    This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "    \n",
      "    -- Gloo build as SHARED library\n",
      "    -- MPI include path: /opt/hpcx/ompi/include/opt/hpcx/ompi/include/openmpi/opt/hpcx/ompi/include/openmpi/opal/mca/hwloc/hwloc201/hwloc/include/opt/hpcx/ompi/include/openmpi/opal/mca/event/libevent2022/libevent/opt/hpcx/ompi/include/openmpi/opal/mca/event/libevent2022/libevent/include\n",
      "    -- MPI libraries: /opt/hpcx/ompi/lib/libmpi.so\n",
      "    -- Found CUDA: /usr/local/cuda (found suitable version \"11.6\", minimum required is \"7.0\")\n",
      "    -- CUDA detected: 11.6\n",
      "    CMake Warning (dev) at /opt/conda/share/cmake-3.22/Modules/FindPackageHandleStandardArgs.cmake:438 (message):\n",
      "      The package name passed to `find_package_handle_standard_args` (NCCL) does\n",
      "      not match the name of the calling package (nccl).  This can lead to\n",
      "      problems in calling code that expects `find_package` result variables\n",
      "      (e.g., `_FOUND`) to follow a certain pattern.\n",
      "    Call Stack (most recent call first):\n",
      "      cmake/Modules/Findnccl.cmake:50 (find_package_handle_standard_args)\n",
      "      third_party/gloo/cmake/Dependencies.cmake:128 (find_package)\n",
      "      third_party/gloo/CMakeLists.txt:109 (include)\n",
      "    This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "    \n",
      "    -- Found NCCL: /wd/notebooks/src/torch/build/nccl/include\n",
      "    -- Determining NCCL version from /wd/notebooks/src/torch/build/nccl/include/nccl.h...\n",
      "    -- Looking for NCCL_VERSION_CODE\n",
      "    -- Looking for NCCL_VERSION_CODE - not found\n",
      "    -- NCCL version < 2.3.5-5\n",
      "    -- Found NCCL (include: /wd/notebooks/src/torch/build/nccl/include, library: /wd/notebooks/src/torch/build/nccl/lib/libnccl_static.a)\n",
      "    CMake Warning at cmake/Dependencies.cmake:1475 (message):\n",
      "      Metal is only used in ios builds.\n",
      "    Call Stack (most recent call first):\n",
      "      CMakeLists.txt:692 (include)\n",
      "    \n",
      "    \n",
      "    -- Found PythonInterp: /opt/conda/bin/python3.8 (found version \"3.8.13\")\n",
      "    -- Found PythonLibs: /opt/conda/lib/libpython3.8.a (found version \"3.8.13\")\n",
      "    Generated: /wd/notebooks/src/torch/build/third_party/onnx/onnx/onnx_onnx_torch-ml.proto\n",
      "    Generated: /wd/notebooks/src/torch/build/third_party/onnx/onnx/onnx-operators_onnx_torch-ml.proto\n",
      "    Generated: /wd/notebooks/src/torch/build/third_party/onnx/onnx/onnx-data_onnx_torch.proto\n",
      "    --\n",
      "    -- ******** Summary ********\n",
      "    --   CMake version             : 3.22.3\n",
      "    --   CMake command             : /opt/conda/bin/cmake\n",
      "    --   System                    : Linux\n",
      "    --   C++ compiler              : /usr/bin/c++\n",
      "    --   C++ compiler version      : 9.4.0\n",
      "    --   CXX flags                 :  -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -Wnon-virtual-dtor\n",
      "    --   Build type                : Release\n",
      "    --   Compile definitions       : ONNX_ML=1;ONNXIFI_ENABLE_EXT=1;__STDC_FORMAT_MACROS\n",
      "    --   CMAKE_PREFIX_PATH         : /opt/conda/lib/python3.8/site-packages;/usr/local/cuda\n",
      "    --   CMAKE_INSTALL_PREFIX      : /wd/notebooks/src/torch/torch\n",
      "    --   CMAKE_MODULE_PATH         : /wd/notebooks/src/torch/cmake/Modules;/wd/notebooks/src/torch/cmake/public/../Modules_CUDA_fix\n",
      "    --\n",
      "    --   ONNX version              : 1.11.0\n",
      "    --   ONNX NAMESPACE            : onnx_torch\n",
      "    --   ONNX_USE_LITE_PROTO       : OFF\n",
      "    --   USE_PROTOBUF_SHARED_LIBS  : OFF\n",
      "    --   Protobuf_USE_STATIC_LIBS  : ON\n",
      "    --   ONNX_DISABLE_EXCEPTIONS   : OFF\n",
      "    --   ONNX_WERROR               : OFF\n",
      "    --   ONNX_BUILD_TESTS          : OFF\n",
      "    --   ONNX_BUILD_BENCHMARKS     : OFF\n",
      "    --   ONNXIFI_DUMMY_BACKEND     : OFF\n",
      "    --   ONNXIFI_ENABLE_EXT        : OFF\n",
      "    --\n",
      "    --   Protobuf compiler         :\n",
      "    --   Protobuf includes         :\n",
      "    --   Protobuf libraries        :\n",
      "    --   BUILD_ONNX_PYTHON         : OFF\n",
      "    --\n",
      "    -- ******** Summary ********\n",
      "    --   CMake version         : 3.22.3\n",
      "    --   CMake command         : /opt/conda/bin/cmake\n",
      "    --   System                : Linux\n",
      "    --   C++ compiler          : /usr/bin/c++\n",
      "    --   C++ compiler version  : 9.4.0\n",
      "    --   CXX flags             :  -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -Wnon-virtual-dtor\n",
      "    --   Build type            : Release\n",
      "    --   Compile definitions   : ONNX_ML=1;ONNXIFI_ENABLE_EXT=1\n",
      "    --   CMAKE_PREFIX_PATH     : /opt/conda/lib/python3.8/site-packages;/usr/local/cuda\n",
      "    --   CMAKE_INSTALL_PREFIX  : /wd/notebooks/src/torch/torch\n",
      "    --   CMAKE_MODULE_PATH     : /wd/notebooks/src/torch/cmake/Modules;/wd/notebooks/src/torch/cmake/public/../Modules_CUDA_fix\n",
      "    --\n",
      "    --   ONNX version          : 1.4.1\n",
      "    --   ONNX NAMESPACE        : onnx_torch\n",
      "    --   ONNX_BUILD_TESTS      : OFF\n",
      "    --   ONNX_BUILD_BENCHMARKS : OFF\n",
      "    --   ONNX_USE_LITE_PROTO   : OFF\n",
      "    --   ONNXIFI_DUMMY_BACKEND : OFF\n",
      "    --\n",
      "    --   Protobuf compiler     :\n",
      "    --   Protobuf includes     :\n",
      "    --   Protobuf libraries    :\n",
      "    --   BUILD_ONNX_PYTHON     : OFF\n",
      "    -- Found CUDA with FP16 support, compiling with torch.cuda.HalfTensor\n",
      "    -- Adding -DNDEBUG to compile flags\n",
      "    -- Checking prototype magma_get_sgeqrf_nb for MAGMA_V2\n",
      "    -- Checking prototype magma_get_sgeqrf_nb for MAGMA_V2 - False\n",
      "    -- Compiling with MAGMA support\n",
      "    -- MAGMA INCLUDE DIRECTORIES: /opt/conda/include\n",
      "    -- MAGMA LIBRARIES: /opt/conda/lib/libmagma.a\n",
      "    -- MAGMA V2 check: 0\n",
      "    -- Could not find hardware support for NEON on this machine.\n",
      "    -- No OMAP3 processor on this machine.\n",
      "    -- No OMAP4 processor on this machine.\n",
      "    -- Found a library with LAPACK API (mkl).\n",
      "    disabling ROCM because NOT USE_ROCM is set\n",
      "    -- MIOpen not found. Compiling without MIOpen support\n",
      "    -- Will build oneDNN Graph\n",
      "    -- MKLDNN_CPU_RUNTIME = OMP\n",
      "    -- cmake version: 3.22.3\n",
      "    CMake Deprecation Warning at third_party/ideep/mkl-dnn/CMakeLists.txt:36 (cmake_policy):\n",
      "      The OLD behavior for policy CMP0025 will be removed from a future version\n",
      "      of CMake.\n",
      "    \n",
      "      The cmake-policies(7) manual explains that the OLD behaviors of all\n",
      "      policies are deprecated and that a policy should be set to OLD only under\n",
      "      specific short-term circumstances.  Projects should be ported to the NEW\n",
      "      behavior and not rely on setting a policy to OLD.\n",
      "    \n",
      "    \n",
      "    -- DNNL_TARGET_ARCH: X64\n",
      "    -- DNNL_LIBRARY_NAME: dnnl\n",
      "    CMake Warning (dev) at /opt/conda/share/cmake-3.22/Modules/FindPackageHandleStandardArgs.cmake:438 (message):\n",
      "      The package name passed to `find_package_handle_standard_args` (OpenMP_C)\n",
      "      does not match the name of the calling package (OpenMP).  This can lead to\n",
      "      problems in calling code that expects `find_package` result variables\n",
      "      (e.g., `_FOUND`) to follow a certain pattern.\n",
      "    Call Stack (most recent call first):\n",
      "      cmake/Modules/FindOpenMP.cmake:576 (find_package_handle_standard_args)\n",
      "      third_party/ideep/mkl-dnn/third_party/oneDNN/cmake/OpenMP.cmake:69 (find_package)\n",
      "      third_party/ideep/mkl-dnn/third_party/oneDNN/CMakeLists.txt:117 (include)\n",
      "    This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "    \n",
      "    -- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
      "    CMake Warning (dev) at /opt/conda/share/cmake-3.22/Modules/FindPackageHandleStandardArgs.cmake:438 (message):\n",
      "      The package name passed to `find_package_handle_standard_args` (OpenMP_CXX)\n",
      "      does not match the name of the calling package (OpenMP).  This can lead to\n",
      "      problems in calling code that expects `find_package` result variables\n",
      "      (e.g., `_FOUND`) to follow a certain pattern.\n",
      "    Call Stack (most recent call first):\n",
      "      cmake/Modules/FindOpenMP.cmake:576 (find_package_handle_standard_args)\n",
      "      third_party/ideep/mkl-dnn/third_party/oneDNN/cmake/OpenMP.cmake:69 (find_package)\n",
      "      third_party/ideep/mkl-dnn/third_party/oneDNN/CMakeLists.txt:117 (include)\n",
      "    This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "    \n",
      "    -- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
      "    -- Could NOT find Doxyrest (missing: DOXYREST_EXECUTABLE)\n",
      "    -- Found PythonInterp: /opt/conda/bin/python3.8 (found suitable version \"3.8.13\", minimum required is \"2.7\")\n",
      "    -- Found Sphinx: /opt/conda/bin/sphinx-build (found version \"sphinx-build 4.5.0\")\n",
      "    -- Enabled workload: TRAINING\n",
      "    -- Enabled primitives: ALL\n",
      "    -- Enabled primitive CPU ISA: ALL\n",
      "    -- Enabled primitive GPU ISA: ALL\n",
      "    -- Primitive cache is enabled\n",
      "    -- Looking for /wd/notebooks/src/torch/third_party/ideep/mkl-dnn/include/oneapi/dnnl/dnnl_graph.h\n",
      "    -- Looking for /wd/notebooks/src/torch/third_party/ideep/mkl-dnn/include/oneapi/dnnl/dnnl_graph.h - found\n",
      "    -- Looking for /wd/notebooks/src/torch/third_party/ideep/mkl-dnn/include/oneapi/dnnl/dnnl_graph_types.h\n",
      "    -- Looking for /wd/notebooks/src/torch/third_party/ideep/mkl-dnn/include/oneapi/dnnl/dnnl_graph_types.h - found\n",
      "    -- Looking for C++ include /wd/notebooks/src/torch/third_party/ideep/mkl-dnn/include/oneapi/dnnl/dnnl_graph.hpp\n",
      "    -- Looking for C++ include /wd/notebooks/src/torch/third_party/ideep/mkl-dnn/include/oneapi/dnnl/dnnl_graph.hpp - found\n",
      "    -- Could NOT find Doxygen (missing: DOXYGEN_EXECUTABLE)\n",
      "    -- Cannot find Doxygen package\n",
      "    CMake Warning (dev) at /opt/conda/share/cmake-3.22/Modules/FindPackageHandleStandardArgs.cmake:438 (message):\n",
      "      The package name passed to `find_package_handle_standard_args` (OpenMP_C)\n",
      "      does not match the name of the calling package (OpenMP).  This can lead to\n",
      "      problems in calling code that expects `find_package` result variables\n",
      "      (e.g., `_FOUND`) to follow a certain pattern.\n",
      "    Call Stack (most recent call first):\n",
      "      cmake/Modules/FindOpenMP.cmake:576 (find_package_handle_standard_args)\n",
      "      third_party/ideep/mkl-dnn/cmake/OpenMP.cmake:62 (find_package)\n",
      "      third_party/ideep/mkl-dnn/CMakeLists.txt:179 (include)\n",
      "    This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "    \n",
      "    CMake Warning (dev) at /opt/conda/share/cmake-3.22/Modules/FindPackageHandleStandardArgs.cmake:438 (message):\n",
      "      The package name passed to `find_package_handle_standard_args` (OpenMP_CXX)\n",
      "      does not match the name of the calling package (OpenMP).  This can lead to\n",
      "      problems in calling code that expects `find_package` result variables\n",
      "      (e.g., `_FOUND`) to follow a certain pattern.\n",
      "    Call Stack (most recent call first):\n",
      "      cmake/Modules/FindOpenMP.cmake:576 (find_package_handle_standard_args)\n",
      "      third_party/ideep/mkl-dnn/cmake/OpenMP.cmake:62 (find_package)\n",
      "      third_party/ideep/mkl-dnn/CMakeLists.txt:179 (include)\n",
      "    This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "    \n",
      "    -- DNNL_GRAPH_BUILD_FOR_CI is set to be OFF\n",
      "    -- Compiling oneDNN Graph with CPU runtime OMP support\n",
      "    -- Graph compiler backend is disabled.\n",
      "    -- Set version definitions to /wd/notebooks/src/torch/third_party/ideep/mkl-dnn/src/utils/verbose.cpp\n",
      "    -- Compiled partition cache is enabled\n",
      "    -- Found MKL-DNN: TRUE\n",
      "    -- Looking for clock_gettime in rt\n",
      "    -- Looking for clock_gettime in rt - found\n",
      "    -- Looking for mmap\n",
      "    -- Looking for mmap - found\n",
      "    -- Looking for shm_open\n",
      "    -- Looking for shm_open - found\n",
      "    -- Looking for shm_unlink\n",
      "    -- Looking for shm_unlink - found\n",
      "    -- Looking for malloc_usable_size\n",
      "    -- Looking for malloc_usable_size - found\n",
      "    -- Performing Test C_HAS_THREAD\n",
      "    -- Performing Test C_HAS_THREAD - Success\n",
      "    -- Version: 7.0.3\n",
      "    -- Build type: Release\n",
      "    -- CXX_STANDARD: 14\n",
      "    -- Performing Test has_std_14_flag\n",
      "    -- Performing Test has_std_14_flag - Success\n",
      "    -- Performing Test has_std_1y_flag\n",
      "    -- Performing Test has_std_1y_flag - Success\n",
      "    -- Performing Test SUPPORTS_USER_DEFINED_LITERALS\n",
      "    -- Performing Test SUPPORTS_USER_DEFINED_LITERALS - Success\n",
      "    -- Performing Test FMT_HAS_VARIANT\n",
      "    -- Performing Test FMT_HAS_VARIANT - Success\n",
      "    -- Required features: cxx_variadic_templates\n",
      "    -- Looking for strtod_l\n",
      "    -- Looking for strtod_l - not found\n",
      "    -- Using Kineto with CUPTI support\n",
      "    -- Configuring Kineto dependency:\n",
      "    --   KINETO_SOURCE_DIR = /wd/notebooks/src/torch/third_party/kineto/libkineto\n",
      "    --   KINETO_BUILD_TESTS = OFF\n",
      "    --   KINETO_LIBRARY_TYPE = static\n",
      "    --   CUDA_SOURCE_DIR = /usr/local/cuda\n",
      "    --   CUDA_INCLUDE_DIRS = /usr/local/cuda/include\n",
      "    --   CUPTI_INCLUDE_DIR = /usr/local/cuda/include\n",
      "    --   CUDA_cupti_LIBRARY = /usr/local/cuda/lib64/libcupti.so\n",
      "    -- Found CUPTI\n",
      "    -- Found PythonInterp: /opt/conda/bin/python3.8 (found version \"3.8.13\")\n",
      "    INFO ROCM_SOURCE_DIR =\n",
      "    -- Kineto: FMT_SOURCE_DIR = /wd/notebooks/src/torch/third_party/fmt\n",
      "    -- Kineto: FMT_INCLUDE_DIR = /wd/notebooks/src/torch/third_party/fmt/include\n",
      "    INFO CUPTI_INCLUDE_DIR = /usr/local/cuda/include\n",
      "    INFO ROCTRACER_INCLUDE_DIR = /roctracer/include\n",
      "    -- Configured Kineto\n",
      "    -- GCC 9.4.0: Adding gcc and gcc_s libs to link line\n",
      "    -- Performing Test HAS_WERROR_FORMAT\n",
      "    -- Performing Test HAS_WERROR_FORMAT - Success\n",
      "    -- Performing Test HAS_WERROR_CAST_FUNCTION_TYPE\n",
      "    -- Performing Test HAS_WERROR_CAST_FUNCTION_TYPE - Success\n",
      "    -- Performing Test HAS_WERROR_SIGN_COMPARE\n",
      "    -- Performing Test HAS_WERROR_SIGN_COMPARE - Success\n",
      "    -- Looking for backtrace\n",
      "    -- Looking for backtrace - found\n",
      "    -- backtrace facility detected in default set of libraries\n",
      "    -- Found Backtrace: /usr/include\n",
      "    -- NUMA paths:\n",
      "    -- /usr/include\n",
      "    -- /usr/lib/x86_64-linux-gnu/libnuma.so\n",
      "    -- headers outputs:\n",
      "    -- sources outputs:\n",
      "    -- declarations_yaml outputs:\n",
      "    -- Performing Test COMPILER_SUPPORTS_NO_AVX256_SPLIT\n",
      "    -- Performing Test COMPILER_SUPPORTS_NO_AVX256_SPLIT - Success\n",
      "    -- Using ATen parallel backend: OMP\n",
      "    CMake Warning (dev) at aten/src/ATen/CMakeLists.txt:124:\n",
      "      Syntax Warning in cmake code at column 79\n",
      "    \n",
      "      Argument not separated from preceding token by whitespace.\n",
      "    This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "    \n",
      "    CMake Deprecation Warning at third_party/sleef/CMakeLists.txt:91 (cmake_policy):\n",
      "      The OLD behavior for policy CMP0066 will be removed from a future version\n",
      "      of CMake.\n",
      "    \n",
      "      The cmake-policies(7) manual explains that the OLD behaviors of all\n",
      "      policies are deprecated and that a policy should be set to OLD only under\n",
      "      specific short-term circumstances.  Projects should be ported to the NEW\n",
      "      behavior and not rely on setting a policy to OLD.\n",
      "    \n",
      "    \n",
      "    -- Found OpenSSL: /opt/conda/lib/libcrypto.so (found version \"1.1.1n\")\n",
      "    -- Check size of long double\n",
      "    -- Check size of long double - done\n",
      "    -- Performing Test COMPILER_SUPPORTS_LONG_DOUBLE\n",
      "    -- Performing Test COMPILER_SUPPORTS_LONG_DOUBLE - Success\n",
      "    -- Performing Test COMPILER_SUPPORTS_FLOAT128\n",
      "    -- Performing Test COMPILER_SUPPORTS_FLOAT128 - Success\n",
      "    -- Performing Test COMPILER_SUPPORTS_SSE2\n",
      "    -- Performing Test COMPILER_SUPPORTS_SSE2 - Success\n",
      "    -- Performing Test COMPILER_SUPPORTS_SSE4\n",
      "    -- Performing Test COMPILER_SUPPORTS_SSE4 - Success\n",
      "    -- Performing Test COMPILER_SUPPORTS_AVX\n",
      "    -- Performing Test COMPILER_SUPPORTS_AVX - Success\n",
      "    -- Performing Test COMPILER_SUPPORTS_FMA4\n",
      "    -- Performing Test COMPILER_SUPPORTS_FMA4 - Success\n",
      "    -- Performing Test COMPILER_SUPPORTS_AVX2\n",
      "    -- Performing Test COMPILER_SUPPORTS_AVX2 - Success\n",
      "    -- Performing Test COMPILER_SUPPORTS_AVX512F\n",
      "    -- Performing Test COMPILER_SUPPORTS_AVX512F - Success\n",
      "    -- Performing Test COMPILER_SUPPORTS_OPENMP\n",
      "    -- Performing Test COMPILER_SUPPORTS_OPENMP - Success\n",
      "    -- Performing Test COMPILER_SUPPORTS_WEAK_ALIASES\n",
      "    -- Performing Test COMPILER_SUPPORTS_WEAK_ALIASES - Success\n",
      "    -- Performing Test COMPILER_SUPPORTS_BUILTIN_MATH\n",
      "    -- Performing Test COMPILER_SUPPORTS_BUILTIN_MATH - Success\n",
      "    -- Performing Test COMPILER_SUPPORTS_SYS_GETRANDOM\n",
      "    -- Performing Test COMPILER_SUPPORTS_SYS_GETRANDOM - Success\n",
      "    -- Configuring build for SLEEF-v3.6.0\n",
      "       Target system: Linux-5.10.102.1-microsoft-standard-WSL2\n",
      "       Target processor: x86_64\n",
      "       Host system: Linux-5.10.102.1-microsoft-standard-WSL2\n",
      "       Host processor: x86_64\n",
      "       Detected C compiler: GNU @ /usr/bin/cc\n",
      "       CMake: 3.22.3\n",
      "       Make program: /opt/conda/bin/ninja\n",
      "    -- Using option `-Wall -Wno-unused -Wno-attributes -Wno-unused-result -Wno-psabi -ffp-contract=off -fno-math-errno -fno-trapping-math` to compile libsleef\n",
      "    -- Building shared libs : OFF\n",
      "    -- Building static test bins: OFF\n",
      "    -- MPFR : LIB_MPFR-NOTFOUND\n",
      "    -- GMP : LIBGMP-NOTFOUND\n",
      "    -- RT : /usr/lib/x86_64-linux-gnu/librt.so\n",
      "    -- FFTW3 : LIBFFTW3-NOTFOUND\n",
      "    -- OPENSSL : 1.1.1n\n",
      "    -- SDE : SDE_COMMAND-NOTFOUND\n",
      "    -- RUNNING_ON_TRAVIS :\n",
      "    -- COMPILER_SUPPORTS_OPENMP : 1\n",
      "    AT_INSTALL_INCLUDE_DIR include/ATen/core\n",
      "    core header install: /wd/notebooks/src/torch/build/aten/src/ATen/core/TensorBody.h\n",
      "    core header install: /wd/notebooks/src/torch/build/aten/src/ATen/core/aten_interned_strings.h\n",
      "    -- /usr/bin/c++ /wd/notebooks/src/torch/torch/abi-check.cpp -o /wd/notebooks/src/torch/build/abi-check\n",
      "    -- Determined _GLIBCXX_USE_CXX11_ABI=1\n",
      "    CMake Warning (dev) at torch/CMakeLists.txt:447:\n",
      "      Syntax Warning in cmake code at column 107\n",
      "    \n",
      "      Argument not separated from preceding token by whitespace.\n",
      "    This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "    \n",
      "    CMake Warning (dev) at torch/CMakeLists.txt:447:\n",
      "      Syntax Warning in cmake code at column 115\n",
      "    \n",
      "      Argument not separated from preceding token by whitespace.\n",
      "    This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "    \n",
      "    CMake Warning at cmake/public/utils.cmake:385 (message):\n",
      "      In the future we will require one to explicitly pass TORCH_CUDA_ARCH_LIST\n",
      "      to cmake instead of implicitly setting it as an env variable.  This will\n",
      "      become a FATAL_ERROR in future version of pytorch.\n",
      "    Call Stack (most recent call first):\n",
      "      torch/CMakeLists.txt:404 (torch_cuda_get_nvcc_gencode_flag)\n",
      "    \n",
      "    \n",
      "    CMake Warning (dev) at /opt/conda/share/cmake-3.22/Modules/FindPackageHandleStandardArgs.cmake:438 (message):\n",
      "      The package name passed to `find_package_handle_standard_args` (OpenMP_C)\n",
      "      does not match the name of the calling package (OpenMP).  This can lead to\n",
      "      problems in calling code that expects `find_package` result variables\n",
      "      (e.g., `_FOUND`) to follow a certain pattern.\n",
      "    Call Stack (most recent call first):\n",
      "      cmake/Modules/FindOpenMP.cmake:576 (find_package_handle_standard_args)\n",
      "      caffe2/CMakeLists.txt:1252 (find_package)\n",
      "    This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "    \n",
      "    CMake Warning (dev) at /opt/conda/share/cmake-3.22/Modules/FindPackageHandleStandardArgs.cmake:438 (message):\n",
      "      The package name passed to `find_package_handle_standard_args` (OpenMP_CXX)\n",
      "      does not match the name of the calling package (OpenMP).  This can lead to\n",
      "      problems in calling code that expects `find_package` result variables\n",
      "      (e.g., `_FOUND`) to follow a certain pattern.\n",
      "    Call Stack (most recent call first):\n",
      "      cmake/Modules/FindOpenMP.cmake:576 (find_package_handle_standard_args)\n",
      "      caffe2/CMakeLists.txt:1252 (find_package)\n",
      "    This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "    \n",
      "    -- pytorch is compiling with OpenMP.\n",
      "    OpenMP CXX_FLAGS: -fopenmp.\n",
      "    OpenMP libraries: /usr/lib/gcc/x86_64-linux-gnu/9/libgomp.so;/usr/lib/x86_64-linux-gnu/libpthread.so.\n",
      "    -- Caffe2 is compiling with OpenMP.\n",
      "    OpenMP CXX_FLAGS: -fopenmp.\n",
      "    OpenMP libraries: /usr/lib/gcc/x86_64-linux-gnu/9/libgomp.so;/usr/lib/x86_64-linux-gnu/libpthread.so.\n",
      "    -- Using lib/python3.8/site-packages as python relative installation path\n",
      "    CMake Warning at CMakeLists.txt:1051 (message):\n",
      "      Generated cmake files are only fully tested if one builds with system glog,\n",
      "      gflags, and protobuf.  Other settings may generate files that are not well\n",
      "      tested.\n",
      "    \n",
      "    \n",
      "    --\n",
      "    -- ******** Summary ********\n",
      "    -- General:\n",
      "    --   CMake version         : 3.22.3\n",
      "    --   CMake command         : /opt/conda/bin/cmake\n",
      "    --   System                : Linux\n",
      "    --   C++ compiler          : /usr/bin/c++\n",
      "    --   C++ compiler id       : GNU\n",
      "    --   C++ compiler version  : 9.4.0\n",
      "    --   Using ccache if found : ON\n",
      "    --   Found ccache          : CCACHE_PROGRAM-NOTFOUND\n",
      "    --   CXX flags             :  -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow\n",
      "    --   Build type            : Release\n",
      "    --   Compile definitions   : ONNX_ML=1;ONNXIFI_ENABLE_EXT=1;ONNX_NAMESPACE=onnx_torch;IDEEP_USE_MKL;HAVE_MMAP=1;_FILE_OFFSET_BITS=64;HAVE_SHM_OPEN=1;HAVE_SHM_UNLINK=1;HAVE_MALLOC_USABLE_SIZE=1;USE_EXTERNAL_MZCRC;MINIZ_DISABLE_ZIP_READER_CRC32_CHECKS\n",
      "    --   CMAKE_PREFIX_PATH     : /opt/conda/lib/python3.8/site-packages;/usr/local/cuda\n",
      "    --   CMAKE_INSTALL_PREFIX  : /wd/notebooks/src/torch/torch\n",
      "    --   USE_GOLD_LINKER       : OFF\n",
      "    --\n",
      "    --   TORCH_VERSION         : 1.12.0\n",
      "    --   CAFFE2_VERSION        : 1.12.0\n",
      "    --   BUILD_CAFFE2          : OFF\n",
      "    --   BUILD_CAFFE2_OPS      : OFF\n",
      "    --   BUILD_CAFFE2_MOBILE   : OFF\n",
      "    --   BUILD_STATIC_RUNTIME_BENCHMARK: OFF\n",
      "    --   BUILD_TENSOREXPR_BENCHMARK: OFF\n",
      "    --   BUILD_NVFUSER_BENCHMARK: ON\n",
      "    --   BUILD_BINARY          : OFF\n",
      "    --   BUILD_CUSTOM_PROTOBUF : ON\n",
      "    --     Link local protobuf : ON\n",
      "    --   BUILD_DOCS            : OFF\n",
      "    --   BUILD_PYTHON          : True\n",
      "    --     Python version      : 3.8.13\n",
      "    --     Python executable   : /opt/conda/bin/python3.8\n",
      "    --     Pythonlibs version  : 3.8.13\n",
      "    --     Python library      : /opt/conda/lib/libpython3.8.a\n",
      "    --     Python includes     : /opt/conda/include/python3.8\n",
      "    --     Python site-packages: lib/python3.8/site-packages\n",
      "    --   BUILD_SHARED_LIBS     : ON\n",
      "    --   CAFFE2_USE_MSVC_STATIC_RUNTIME     : OFF\n",
      "    --   BUILD_TEST            : True\n",
      "    --   BUILD_JNI             : OFF\n",
      "    --   BUILD_MOBILE_AUTOGRAD : OFF\n",
      "    --   BUILD_LITE_INTERPRETER: OFF\n",
      "    --   INTERN_BUILD_MOBILE   :\n",
      "    --   USE_BLAS              : 1\n",
      "    --     BLAS                : mkl\n",
      "    --     BLAS_HAS_SBGEMM     :\n",
      "    --   USE_LAPACK            : 1\n",
      "    --     LAPACK              : mkl\n",
      "    --   USE_ASAN              : OFF\n",
      "    --   USE_CPP_CODE_COVERAGE : OFF\n",
      "    --   USE_CUDA              : ON\n",
      "    --     Split CUDA          : OFF\n",
      "    --     CUDA static link    : OFF\n",
      "    --     USE_CUDNN           : ON\n",
      "    --     USE_EXPERIMENTAL_CUDNN_V8_API: 1\n",
      "    --     CUDA version        : 11.6\n",
      "    --     cuDNN version       : 8.4.0\n",
      "    --     CUDA root directory : /usr/local/cuda\n",
      "    --     CUDA library        : /usr/local/cuda/lib64/stubs/libcuda.so\n",
      "    --     cudart library      : /usr/local/cuda/lib64/libcudart.so\n",
      "    --     cublas library      : /usr/local/cuda/lib64/libcublas.so\n",
      "    --     cufft library       : /usr/local/cuda/lib64/libcufft.so\n",
      "    --     curand library      : /usr/local/cuda/lib64/libcurand.so\n",
      "    --     cuDNN library       : /usr/lib/x86_64-linux-gnu/libcudnn.so\n",
      "    --     nvrtc               : /usr/local/cuda/lib64/libnvrtc.so\n",
      "    --     CUDA include path   : /usr/local/cuda/include\n",
      "    --     NVCC executable     : /usr/local/cuda/bin/nvcc\n",
      "    --     CUDA compiler       : /usr/local/cuda/bin/nvcc\n",
      "    --     CUDA flags          :  -Xfatbin -compress-all -DONNX_NAMESPACE=onnx_torch -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -Xcudafe --diag_suppress=cc_clobber_ignored,--diag_suppress=integer_sign_change,--diag_suppress=useless_using_declaration,--diag_suppress=set_but_not_used,--diag_suppress=field_without_dll_interface,--diag_suppress=base_class_has_different_dll_interface,--diag_suppress=dll_interface_conflict_none_assumed,--diag_suppress=dll_interface_conflict_dllexport_assumed,--diag_suppress=implicit_return_from_non_void_function,--diag_suppress=unsigned_compare_with_zero,--diag_suppress=declared_but_not_referenced,--diag_suppress=bad_friend_decl --expt-relaxed-constexpr --expt-extended-lambda  -Wno-deprecated-gpu-targets --expt-extended-lambda -DCUB_WRAPPED_NAMESPACE=at_cuda_detail -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__\n",
      "    --     CUDA host compiler  :\n",
      "    --     CUDA --device-c     : OFF\n",
      "    --     USE_TENSORRT        : OFF\n",
      "    --   USE_ROCM              : OFF\n",
      "    --   USE_EIGEN_FOR_BLAS    :\n",
      "    --   USE_FBGEMM            : ON\n",
      "    --     USE_FAKELOWP          : OFF\n",
      "    --   USE_KINETO            : ON\n",
      "    --   USE_FFMPEG            : OFF\n",
      "    --   USE_GFLAGS            : OFF\n",
      "    --   USE_GLOG              : OFF\n",
      "    --   USE_LEVELDB           : OFF\n",
      "    --   USE_LITE_PROTO        : OFF\n",
      "    --   USE_LMDB              : OFF\n",
      "    --   USE_METAL             : OFF\n",
      "    --   USE_PYTORCH_METAL     : OFF\n",
      "    --   USE_PYTORCH_METAL_EXPORT     : OFF\n",
      "    --   USE_MPS               : OFF\n",
      "    --   USE_FFTW              : OFF\n",
      "    --   USE_MKL               : ON\n",
      "    --   USE_MKLDNN            : ON\n",
      "    --   USE_NCCL              : ON\n",
      "    --     USE_SYSTEM_NCCL     : OFF\n",
      "    --     USE_NCCL_WITH_UCC   : OFF\n",
      "    --   USE_NNPACK            : ON\n",
      "    --   USE_NUMPY             : ON\n",
      "    --   USE_OBSERVERS         : ON\n",
      "    --   USE_OPENCL            : OFF\n",
      "    --   USE_OPENCV            : OFF\n",
      "    --   USE_OPENMP            : ON\n",
      "    --   USE_TBB               : OFF\n",
      "    --   USE_VULKAN            : OFF\n",
      "    --   USE_PROF              : OFF\n",
      "    --   USE_QNNPACK           : ON\n",
      "    --   USE_PYTORCH_QNNPACK   : ON\n",
      "    --   USE_XNNPACK           : ON\n",
      "    --   USE_REDIS             : OFF\n",
      "    --   USE_ROCKSDB           : OFF\n",
      "    --   USE_ZMQ               : OFF\n",
      "    --   USE_DISTRIBUTED       : ON\n",
      "    --     USE_MPI               : ON\n",
      "    --     USE_GLOO              : ON\n",
      "    --     USE_GLOO_WITH_OPENSSL : OFF\n",
      "    --     USE_TENSORPIPE        : ON\n",
      "    --   USE_DEPLOY           : OFF\n",
      "    --   Public Dependencies  : caffe2::Threads;caffe2::mkl\n",
      "    --   Private Dependencies : pthreadpool;cpuinfo;qnnpack;pytorch_qnnpack;nnpack;XNNPACK;fbgemm;/usr/lib/x86_64-linux-gnu/libnuma.so;fp16;/opt/hpcx/ompi/lib/libmpi.so;tensorpipe;gloo;foxi_loader;rt;fmt::fmt-header-only;kineto;gcc_s;gcc;dl\n",
      "    --   USE_COREML_DELEGATE     : OFF\n",
      "    --   BUILD_LAZY_TS_BACKEND   : ON\n",
      "    -- Configuring done\n",
      "    CMake Warning at caffe2/CMakeLists.txt:822 (add_library):\n",
      "      Cannot generate a safe runtime search path for target torch_cpu because\n",
      "      files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target scalar_test because\n",
      "      files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target apply_utils_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target basic because files\n",
      "      in some directories may conflict with libraries in implicit directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target atest because files\n",
      "      in some directories may conflict with libraries in implicit directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target Dimname_test because\n",
      "      files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target Dict_test because\n",
      "      files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target NamedTensor_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target half_test because\n",
      "      files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target broadcast_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target wrapdim_test because\n",
      "      files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target dlconvertor_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target native_test because\n",
      "      files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target scalar_tensor_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target test_parallel because\n",
      "      files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target undefined_tensor_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target verify_api_visibility\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target thread_init_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target weakref_test because\n",
      "      files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target quantized_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target\n",
      "      extension_backend_test because files in some directories may conflict with\n",
      "      libraries in implicit directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target operators_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target lazy_tensor_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target tensor_iterator_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target math_kernel_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target\n",
      "      memory_overlapping_test because files in some directories may conflict with\n",
      "      libraries in implicit directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target mobile_memory_cleanup\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target cpu_generator_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target\n",
      "      cpu_profiling_allocator_test because files in some directories may conflict\n",
      "      with libraries in implicit directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target pow_test because\n",
      "      files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target variant_test because\n",
      "      files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target reduce_ops_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target\n",
      "      reportMemoryUsage_test because files in some directories may conflict with\n",
      "      libraries in implicit directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target memory_format_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target cpu_rng_test because\n",
      "      files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target ivalue_test because\n",
      "      files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target vmap_test because\n",
      "      files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target type_test because\n",
      "      files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target dispatch_key_set_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target\n",
      "      stride_properties_test because files in some directories may conflict with\n",
      "      libraries in implicit directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target IListRef_test because\n",
      "      files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target List_test because\n",
      "      files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target KernelFunction_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target\n",
      "      kernel_function_legacy_test because files in some directories may conflict\n",
      "      with libraries in implicit directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target kernel_function_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target\n",
      "      kernel_lambda_legacy_test because files in some directories may conflict\n",
      "      with libraries in implicit directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target kernel_lambda_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target\n",
      "      kernel_stackbased_test because files in some directories may conflict with\n",
      "      libraries in implicit directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target\n",
      "      make_boxed_from_unboxed_functor_test because files in some directories may\n",
      "      conflict with libraries in implicit directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target CppSignature_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target backend_fallback_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target op_allowlist_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target op_registration_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    CMake Warning at caffe2/CMakeLists.txt:1809 (add_executable):\n",
      "      Cannot generate a safe runtime search path for target inline_container_test\n",
      "      because files in some directories may conflict with libraries in implicit\n",
      "      directories:\n",
      "    \n",
      "        runtime library [libgomp.so.1] in /usr/lib/gcc/x86_64-linux-gnu/9 may be hidden by files in:\n",
      "          /opt/conda/lib\n",
      "    \n",
      "      Some of these libraries may not be found correctly.\n",
      "    \n",
      "    \n",
      "    -- Generating done\n",
      "    -- Build files have been written to: /wd/notebooks/src/torch/build\n",
      "    [1/4] Generating ATen sources\n",
      "    [2/4] Generating ATen declarations_yaml\n",
      "    [3/4] Generating ATen headers\n",
      "    [1/6781] Creating directories for 'nccl_external'\n",
      "    [2/6781] No download step for 'nccl_external'\n",
      "    [3/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/io/io_win32.cc.o\n",
      "    [4/6781] No update step for 'nccl_external'\n",
      "    [5/6781] No patch step for 'nccl_external'\n",
      "    [6/6781] No configure step for 'nccl_external'\n",
      "    [7/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/io/strtod.cc.o\n",
      "    [8/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/arena.cc.o\n",
      "    [9/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/io/zero_copy_stream.cc.o\n",
      "    [10/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/io/zero_copy_stream_impl_lite.cc.o\n",
      "    [11/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/io/zero_copy_stream_impl.cc.o\n",
      "    [12/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/any_lite.cc.o\n",
      "    [13/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/generated_enum_util.cc.o\n",
      "    [14/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/implicit_weak_message.cc.o\n",
      "    [15/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/io/coded_stream.cc.o\n",
      "    [16/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/generated_message_util.cc.o\n",
      "    [17/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/message_lite.cc.o\n",
      "    In file included from /usr/include/string.h:495,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/port.h:39,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/macros.h:34,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/common.h:46,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/message_lite.h:45,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/message_lite.cc:36:\n",
      "    In function ‘void* memcpy(void*, const void*, size_t)’,\n",
      "        inlined from ‘google::protobuf::uint8* google::protobuf::io::EpsCopyOutputStream::WriteRaw(const void*, int, google::protobuf::uint8*)’ at /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/io/coded_stream.h:699:16,\n",
      "        inlined from ‘bool google::protobuf::MessageLite::SerializePartialToZeroCopyStream(google::protobuf::io::ZeroCopyOutputStream*) const’ at /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/implicit_weak_message.h:85:28:\n",
      "    /usr/include/x86_64-linux-gnu/bits/string_fortified.h:34:33: warning: ‘void* __builtin___memcpy_chk(void*, const void*, long unsigned int, long unsigned int)’ specified size between 18446744071562067968 and 18446744073709551615 exceeds maximum object size 9223372036854775807 [-Wstringop-overflow=]\n",
      "       34 |   return __builtin___memcpy_chk (__dest, __src, __len, __bos0 (__dest));\n",
      "          |          ~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    [18/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/bytestream.cc.o\n",
      "    [19/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/extension_set.cc.o\n",
      "    [20/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/generated_message_table_driven_lite.cc.o\n",
      "    [21/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/int128.cc.o\n",
      "    [22/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/status.cc.o\n",
      "    [23/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/parse_context.cc.o\n",
      "    [24/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/common.cc.o\n",
      "    [25/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/statusor.cc.o\n",
      "    [26/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/stringpiece.cc.o\n",
      "    [27/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/extension_set.cc.o\n",
      "    FAILED: third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/extension_set.cc.o\n",
      "    /usr/bin/c++ -DGOOGLE_PROTOBUF_CMAKE_BUILD -DHAVE_PTHREAD -I/wd/notebooks/src/torch/build/third_party/protobuf/cmake -I/wd/notebooks/src/torch/third_party/protobuf/src -fvisibility-inlines-hidden -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG -fPIC -std=c++11 -MD -MT third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/extension_set.cc.o -MF third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/extension_set.cc.o.d -o third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/extension_set.cc.o -c /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/extension_set.cc\n",
      "    In file included from /usr/include/c++/9/string:55,\n",
      "                     from /usr/include/c++/9/stdexcept:39,\n",
      "                     from /usr/include/c++/9/array:39,\n",
      "                     from /usr/include/c++/9/tuple:39,\n",
      "                     from /usr/include/c++/9/bits/stl_map.h:63,\n",
      "                     from /usr/include/c++/9/map:61,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/extension_set.h:43,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/extension_set.cc:35:\n",
      "    /usr/include/c++/9/bits/basic_string.h:39:10: fatal error: /wd/notebooks/src/torch/build/third_party/protobuf/cmake/ext/atomicity.h: Cannot allocate memory\n",
      "       39 | #include <ext/atomicity.h>\n",
      "          |          ^~~~~~~~~~~~~~~~~\n",
      "    compilation terminated.\n",
      "    [28/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/structurally_valid.cc.o\n",
      "    FAILED: third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/structurally_valid.cc.o\n",
      "    /usr/bin/c++ -DGOOGLE_PROTOBUF_CMAKE_BUILD -DHAVE_PTHREAD -I/wd/notebooks/src/torch/build/third_party/protobuf/cmake -I/wd/notebooks/src/torch/third_party/protobuf/src -fvisibility-inlines-hidden -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG -fPIC -std=c++11 -MD -MT third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/structurally_valid.cc.o -MF third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/structurally_valid.cc.o.d -o third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/structurally_valid.cc.o -c /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/structurally_valid.cc\n",
      "    In file included from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/structurally_valid.cc:33:\n",
      "    /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/common.h:200:10: fatal error: /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/port_undef.inc: Input/output error\n",
      "    compilation terminated.\n",
      "    [29/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_util.cc.o\n",
      "    FAILED: third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_util.cc.o\n",
      "    /usr/bin/c++ -DGOOGLE_PROTOBUF_CMAKE_BUILD -DHAVE_PTHREAD -I/wd/notebooks/src/torch/build/third_party/protobuf/cmake -I/wd/notebooks/src/torch/third_party/protobuf/src -fvisibility-inlines-hidden -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG -fPIC -std=c++11 -MD -MT third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_util.cc.o -MF third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_util.cc.o.d -o third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_util.cc.o -c /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/generated_message_util.cc\n",
      "    In file included from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/generated_message_util.cc:35:\n",
      "    /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/generated_message_util.h:45:10: fatal error: /wd/notebooks/src/torch/third_party/protobuf/src/string: Cannot allocate memory\n",
      "       45 | #include <string>\n",
      "          |          ^~~~~~~~\n",
      "    compilation terminated.\n",
      "    [30/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/strutil.cc.o\n",
      "    FAILED: third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/strutil.cc.o\n",
      "    /usr/bin/c++ -DGOOGLE_PROTOBUF_CMAKE_BUILD -DHAVE_PTHREAD -I/wd/notebooks/src/torch/build/third_party/protobuf/cmake -I/wd/notebooks/src/torch/third_party/protobuf/src -fvisibility-inlines-hidden -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG -fPIC -std=c++11 -MD -MT third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/strutil.cc.o -MF third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/strutil.cc.o.d -o third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/strutil.cc.o -c /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/strutil.cc\n",
      "    In file included from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/macros.h:34,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/common.h:46,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/strutil.h:36,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/strutil.cc:33:\n",
      "    /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/port.h:41:10: fatal error: /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/platform_macros.h: Input/output error\n",
      "       41 | #include <google/protobuf/stubs/platform_macros.h>\n",
      "          |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    compilation terminated.\n",
      "    [31/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/time.cc.o\n",
      "    FAILED: third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/time.cc.o\n",
      "    /usr/bin/c++ -DGOOGLE_PROTOBUF_CMAKE_BUILD -DHAVE_PTHREAD -I/wd/notebooks/src/torch/build/third_party/protobuf/cmake -I/wd/notebooks/src/torch/third_party/protobuf/src -fvisibility-inlines-hidden -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG -fPIC -std=c++11 -MD -MT third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/time.cc.o -MF third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/time.cc.o.d -o third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/time.cc.o -c /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/time.cc\n",
      "    In file included from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/time.h:33,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/time.cc:1:\n",
      "    /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/common.h:46:10: fatal error: /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/macros.h: Input/output error\n",
      "       46 | #include <google/protobuf/stubs/macros.h>\n",
      "          |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    compilation terminated.\n",
      "    [32/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arena.cc.o\n",
      "    FAILED: third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arena.cc.o\n",
      "    /usr/bin/c++ -DGOOGLE_PROTOBUF_CMAKE_BUILD -DHAVE_PTHREAD -I/wd/notebooks/src/torch/build/third_party/protobuf/cmake -I/wd/notebooks/src/torch/third_party/protobuf/src -fvisibility-inlines-hidden -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG -fPIC -std=c++11 -MD -MT third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arena.cc.o -MF third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arena.cc.o.d -o third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/arena.cc.o -c /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/arena.cc\n",
      "    In file included from /usr/include/c++/9/bits/ios_base.h:46,\n",
      "                     from /usr/include/c++/9/ios:42,\n",
      "                     from /usr/include/c++/9/ostream:38,\n",
      "                     from /usr/include/c++/9/iostream:39,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/common.h:39,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/arena_impl.h:39,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/arena.h:55,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/arena.cc:31:\n",
      "    /usr/include/c++/9/system_error:39:10: fatal error: /wd/notebooks/src/torch/third_party/protobuf/src/bits/error_constants.h: Input/output error\n",
      "       39 | #include <bits/error_constants.h>\n",
      "          |          ^~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    compilation terminated.\n",
      "    [33/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_enum_util.cc.o\n",
      "    FAILED: third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_enum_util.cc.o\n",
      "    /usr/bin/c++ -DGOOGLE_PROTOBUF_CMAKE_BUILD -DHAVE_PTHREAD -I/wd/notebooks/src/torch/build/third_party/protobuf/cmake -I/wd/notebooks/src/torch/third_party/protobuf/src -fvisibility-inlines-hidden -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG -fPIC -std=c++11 -MD -MT third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_enum_util.cc.o -MF third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_enum_util.cc.o.d -o third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_enum_util.cc.o -c /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/generated_enum_util.cc\n",
      "    In file included from /usr/include/c++/9/string:55,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/message_lite.h:43,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/generated_enum_util.h:36,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/generated_enum_util.cc:31:\n",
      "    /usr/include/c++/9/bits/basic_string.h:6496:10: fatal error: /wd/notebooks/src/torch/third_party/protobuf/src/ext/string_conversions.h: Input/output error\n",
      "     6496 | #include <ext/string_conversions.h>\n",
      "          |          ^~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    compilation terminated.\n",
      "    [34/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/any_lite.cc.o\n",
      "    FAILED: third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/any_lite.cc.o\n",
      "    /usr/bin/c++ -DGOOGLE_PROTOBUF_CMAKE_BUILD -DHAVE_PTHREAD -I/wd/notebooks/src/torch/build/third_party/protobuf/cmake -I/wd/notebooks/src/torch/third_party/protobuf/src -fvisibility-inlines-hidden -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG -fPIC -std=c++11 -MD -MT third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/any_lite.cc.o -MF third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/any_lite.cc.o.d -o third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/any_lite.cc.o -c /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/any_lite.cc\n",
      "    In file included from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/any.h:36,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/any_lite.cc:31:\n",
      "    /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/common.h:40:10: fatal error: /wd/notebooks/src/torch/third_party/protobuf/src/map: Input/output error\n",
      "    compilation terminated.\n",
      "    [35/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/wire_format_lite.cc.o\n",
      "    FAILED: third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/wire_format_lite.cc.o\n",
      "    /usr/bin/c++ -DGOOGLE_PROTOBUF_CMAKE_BUILD -DHAVE_PTHREAD -I/wd/notebooks/src/torch/build/third_party/protobuf/cmake -I/wd/notebooks/src/torch/third_party/protobuf/src -fvisibility-inlines-hidden -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG -fPIC -std=c++11 -MD -MT third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/wire_format_lite.cc.o -MF third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/wire_format_lite.cc.o.d -o third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/wire_format_lite.cc.o -c /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/wire_format_lite.cc\n",
      "    In file included from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/wire_format_lite.h:45,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/wire_format_lite.cc:35:\n",
      "    /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/common.h:46:10: fatal error: /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/macros.h: Input/output error\n",
      "       46 | #include <google/protobuf/stubs/macros.h>\n",
      "          |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    compilation terminated.\n",
      "    [36/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/stringprintf.cc.o\n",
      "    FAILED: third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/stringprintf.cc.o\n",
      "    /usr/bin/c++ -DGOOGLE_PROTOBUF_CMAKE_BUILD -DHAVE_PTHREAD -I/wd/notebooks/src/torch/build/third_party/protobuf/cmake -I/wd/notebooks/src/torch/third_party/protobuf/src -fvisibility-inlines-hidden -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG -fPIC -std=c++11 -MD -MT third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/stringprintf.cc.o -MF third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/stringprintf.cc.o.d -o third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/stubs/stringprintf.cc.o -c /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/stubs/stringprintf.cc\n",
      "    Input/output error[37/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/implicit_weak_message.cc.o\n",
      "    \n",
      "    FAILED: third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/implicit_weak_message.cc.o\n",
      "    /usr/bin/c++ -DGOOGLE_PROTOBUF_CMAKE_BUILD -DHAVE_PTHREAD -I/wd/notebooks/src/torch/build/third_party/protobuf/cmake -I/wd/notebooks/src/torch/third_party/protobuf/src -fvisibility-inlines-hidden -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG -fPIC -std=c++11 -MD -MT third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/implicit_weak_message.cc.o -MF third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/implicit_weak_message.cc.o.d -o third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/implicit_weak_message.cc.o -c /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/implicit_weak_message.cc\n",
      "    In file included from /usr/include/c++/9/cwchar:44,\n",
      "                     from /usr/include/c++/9/bits/postypes.h:40,\n",
      "                     from /usr/include/c++/9/bits/char_traits.h:40,\n",
      "                     from /usr/include/c++/9/string:40,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/implicit_weak_message.h:34,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/implicit_weak_message.cc:31:\n",
      "    /usr/include/wchar.h:42:10: fatal error: /wd/notebooks/src/torch/third_party/protobuf/src/bits/types/mbstate_t.h: Input/output error\n",
      "       42 | #include <bits/types/mbstate_t.h>\n",
      "          |          ^~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    compilation terminated.\n",
      "    [38/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_table_driven_lite.cc.o\n",
      "    FAILED: third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_table_driven_lite.cc.o\n",
      "    /usr/bin/c++ -DGOOGLE_PROTOBUF_CMAKE_BUILD -DHAVE_PTHREAD -I/wd/notebooks/src/torch/build/third_party/protobuf/cmake -I/wd/notebooks/src/torch/third_party/protobuf/src -fvisibility-inlines-hidden -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG -fPIC -std=c++11 -MD -MT third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_table_driven_lite.cc.o -MF third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_table_driven_lite.cc.o.d -o third_party/protobuf/cmake/CMakeFiles/libprotobuf.dir/__/src/google/protobuf/generated_message_table_driven_lite.cc.o -c /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/generated_message_table_driven_lite.cc\n",
      "    In file included from /usr/include/c++/9/bits/localefwd.h:40,\n",
      "                     from /usr/include/c++/9/string:43,\n",
      "                     from /usr/include/c++/9/stdexcept:39,\n",
      "                     from /usr/include/c++/9/array:39,\n",
      "                     from /usr/include/c++/9/tuple:39,\n",
      "                     from /usr/include/c++/9/bits/unique_ptr.h:37,\n",
      "                     from /usr/include/c++/9/memory:80,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/io/zero_copy_stream_impl_lite.h:49,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/generated_message_table_driven_lite.h:34,\n",
      "                     from /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/generated_message_table_driven_lite.cc:31:\n",
      "    /usr/include/x86_64-linux-gnu/c++/9/bits/c++locale.h:41:10: fatal error: /wd/notebooks/src/torch/third_party/protobuf/src/clocale: Input/output error\n",
      "       41 | #include <clocale>\n",
      "          |          ^~~~~~~~~\n",
      "    compilation terminated.\n",
      "    [39/6781] Building CXX object third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/repeated_field.cc.o\n",
      "    FAILED: third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/repeated_field.cc.o\n",
      "    /usr/bin/c++ -DGOOGLE_PROTOBUF_CMAKE_BUILD -DHAVE_PTHREAD -I/wd/notebooks/src/torch/build/third_party/protobuf/cmake -I/wd/notebooks/src/torch/third_party/protobuf/src -fvisibility-inlines-hidden -fvisibility=hidden -fvisibility-inlines-hidden -O3 -DNDEBUG -fPIC -std=c++11 -MD -MT third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/repeated_field.cc.o -MF third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/repeated_field.cc.o.d -o third_party/protobuf/cmake/CMakeFiles/libprotobuf-lite.dir/__/src/google/protobuf/repeated_field.cc.o -c /wd/notebooks/src/torch/third_party/protobuf/src/google/protobuf/repeated_field.cc\n",
      "    Input/output error[40/6781] Performing build step for 'nccl_external'\n",
      "    \n",
      "    FAILED: nccl_external-prefix/src/nccl_external-stamp/nccl_external-build nccl/lib/libnccl_static.a /wd/notebooks/src/torch/build/nccl_external-prefix/src/nccl_external-stamp/nccl_external-build /wd/notebooks/src/torch/build/nccl/lib/libnccl_static.a\n",
      "    cd /wd/notebooks/src/torch/third_party/nccl/nccl && env CCACHE_DISABLE=1 SCCACHE_DISABLE=1 make CXX=/usr/bin/c++ CUDA_HOME=/usr/local/cuda NVCC=/usr/local/cuda/bin/nvcc \"NVCC_GENCODE=-gencode=arch=compute_52,code=sm_52 -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_61,code=sm_61 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86\" BUILDDIR=/wd/notebooks/src/torch/build/nccl VERBOSE=0 -j && /opt/conda/bin/cmake -E touch /wd/notebooks/src/torch/build/nccl_external-prefix/src/nccl_external-stamp/nccl_external-build\n",
      "    make -C src build BUILDDIR=/wd/notebooks/src/torch/build/nccl\n",
      "    make[1]: Entering directory '/wd/notebooks/src/torch/third_party/nccl/nccl/src'\n",
      "    Generating nccl.h.in                           > /wd/notebooks/src/torch/build/nccl/include/nccl.h\n",
      "    Grabbing   include/nccl_net.h                  > /wd/notebooks/src/torch/build/nccl/include/nccl_net.h\n",
      "    Generating nccl.pc.in                          > /wd/notebooks/src/torch/build/nccl/lib/pkgconfig/nccl.pc\n",
      "    Compiling  init.cc                             > /wd/notebooks/src/torch/build/nccl/obj/init.o\n",
      "    Compiling  channel.cc                          > /wd/notebooks/src/torch/build/nccl/obj/channel.o\n",
      "    Compiling  bootstrap.cc                        > /wd/notebooks/src/torch/build/nccl/obj/bootstrap.o\n",
      "    Compiling  transport.cc                        > /wd/notebooks/src/torch/build/nccl/obj/transport.o\n",
      "    Compiling  enqueue.cc                          > /wd/notebooks/src/torch/build/nccl/obj/enqueue.o\n",
      "    Compiling  group.cc                            > /wd/notebooks/src/torch/build/nccl/obj/group.o\n",
      "    Compiling  debug.cc                            > /wd/notebooks/src/torch/build/nccl/obj/debug.o\n",
      "    Compiling  proxy.cc                            > /wd/notebooks/src/torch/build/nccl/obj/proxy.o\n",
      "    Compiling  misc/nvmlwrap.cc                    > /wd/notebooks/src/torch/build/nccl/obj/misc/nvmlwrap.o\n",
      "    Compiling  misc/ibvwrap.cc                     > /wd/notebooks/src/torch/build/nccl/obj/misc/ibvwrap.o\n",
      "    Compiling  misc/gdrwrap.cc                     > /wd/notebooks/src/torch/build/nccl/obj/misc/gdrwrap.o\n",
      "    Compiling  misc/utils.cc                       > /wd/notebooks/src/torch/build/nccl/obj/misc/utils.o\n",
      "    Compiling  misc/argcheck.cc                    > /wd/notebooks/src/torch/build/nccl/obj/misc/argcheck.o\n",
      "    Compiling  transport/p2p.cc                    > /wd/notebooks/src/torch/build/nccl/obj/transport/p2p.o\n",
      "    Compiling  transport/shm.cc                    > /wd/notebooks/src/torch/build/nccl/obj/transport/shm.o\n",
      "    Compiling  transport/net.cc                    > /wd/notebooks/src/torch/build/nccl/obj/transport/net.o\n",
      "    Compiling  transport/net_socket.cc             > /wd/notebooks/src/torch/build/nccl/obj/transport/net_socket.o\n",
      "    Compiling  transport/net_ib.cc                 > /wd/notebooks/src/torch/build/nccl/obj/transport/net_ib.o\n",
      "    Compiling  transport/coll_net.cc               > /wd/notebooks/src/torch/build/nccl/obj/transport/coll_net.o\n",
      "    Compiling  collectives/sendrecv.cc             > /wd/notebooks/src/torch/build/nccl/obj/collectives/sendrecv.o\n",
      "    Compiling  collectives/all_reduce.cc           > /wd/notebooks/src/torch/build/nccl/obj/collectives/all_reduce.o\n",
      "    Compiling  collectives/all_gather.cc           > /wd/notebooks/src/torch/build/nccl/obj/collectives/all_gather.o\n",
      "    Compiling  collectives/broadcast.cc            > /wd/notebooks/src/torch/build/nccl/obj/collectives/broadcast.o\n",
      "    Compiling  collectives/reduce.cc               > /wd/notebooks/src/torch/build/nccl/obj/collectives/reduce.o\n",
      "    Compiling  collectives/reduce_scatter.cc       > /wd/notebooks/src/torch/build/nccl/obj/collectives/reduce_scatter.o\n",
      "    Compiling  graph/topo.cc                       > /wd/notebooks/src/torch/build/nccl/obj/graph/topo.o\n",
      "    Compiling  graph/paths.cc                      > /wd/notebooks/src/torch/build/nccl/obj/graph/paths.o\n",
      "    Compiling  graph/search.cc                     > /wd/notebooks/src/torch/build/nccl/obj/graph/search.o\n",
      "    Compiling  graph/connect.cc                    > /wd/notebooks/src/torch/build/nccl/obj/graph/connect.o\n",
      "    Compiling  graph/rings.cc                      > /wd/notebooks/src/torch/build/nccl/obj/graph/rings.o\n",
      "    Compiling  graph/trees.cc                      > /wd/notebooks/src/torch/build/nccl/obj/graph/trees.o\n",
      "    Compiling  graph/tuning.cc                     > /wd/notebooks/src/torch/build/nccl/obj/graph/tuning.o\n",
      "    Compiling  graph/xml.cc                        > /wd/notebooks/src/torch/build/nccl/obj/graph/xml.o\n",
      "    make[2]: Entering directory '/wd/notebooks/src/torch/third_party/nccl/nccl/src/collectives/device'\n",
      "    In file included from /usr/include/c++/9/cstdlib:75,\n",
      "                     from /usr/include/c++/9/stdlib.h:36,\n",
      "                     from include/core.h:12,\n",
      "                     from graph/rings.cc:7:\n",
      "    /usr/include/stdlib.h:394:11: fatal error: ./sys/types.h: Cannot allocate memory\n",
      "      394 | # include <sys/types.h> /* we need int32_t... */\n",
      "          |           ^~~~~~~~~~~~~\n",
      "    compilation terminated.\n",
      "    make[1]: *** [Makefile:113: /wd/notebooks/src/torch/build/nccl/obj/graph/rings.o] Error 1\n",
      "    make[1]: *** Waiting for unfinished jobs....\n",
      "    In file included from include/transport.h:11,\n",
      "                     from include/comm.h:10,\n",
      "                     from include/enqueue.h:10,\n",
      "                     from collectives/reduce_scatter.cc:7:\n",
      "    include/graph.h:16:10: fatal error: ./sched.h: Cannot allocate memory\n",
      "       16 | #include <sched.h>\n",
      "          |          ^~~~~~~~~\n",
      "    compilation terminated.\n",
      "    make[1]: *** [Makefile:113: /wd/notebooks/src/torch/build/nccl/obj/collectives/reduce_scatter.o] Error 1\n",
      "    In file included from include/core.h:57,\n",
      "                     from include/transport.h:13,\n",
      "                     from include/comm.h:10,\n",
      "                     from include/enqueue.h:10,\n",
      "                     from enqueue.cc:7:\n",
      "    enqueue.cc: In function ‘ncclResult_t ncclLaunchCooperativeKernelMultiDevice(cudaLaunchParams*, int*, int, int)’:\n",
      "    enqueue.cc:119:97: warning: ‘cudaError_t cudaLaunchCooperativeKernelMultiDevice(cudaLaunchParams*, unsigned int, unsigned int)’ is deprecated [-Wdeprecated-declarations]\n",
      "      119 |             cudaCooperativeLaunchMultiDeviceNoPreSync|cudaCooperativeLaunchMultiDeviceNoPostSync));\n",
      "          |                                                                                                 ^\n",
      "    include/checks.h:14:23: note: in definition of macro ‘CUDACHECK’\n",
      "       14 |     cudaError_t err = cmd;                                  \\\n",
      "          |                       ^~~\n",
      "    In file included from /usr/local/cuda/include/channel_descriptor.h:61,\n",
      "                     from /usr/local/cuda/include/cuda_runtime.h:95,\n",
      "                     from /wd/notebooks/src/torch/build/nccl/include/nccl.h:10,\n",
      "                     from include/devcomm.h:10,\n",
      "                     from include/transport.h:10,\n",
      "                     from include/comm.h:10,\n",
      "                     from include/enqueue.h:10,\n",
      "                     from enqueue.cc:7:\n",
      "    /usr/local/cuda/include/cuda_runtime_api.h:4202:57: note: declared here\n",
      "     4202 | extern __CUDA_DEPRECATED __host__ cudaError_t CUDARTAPI cudaLaunchCooperativeKernelMultiDevice(struct cudaLaunchParams *launchParamsList, unsigned int numDevices, unsigned int flags  __dv(0));\n",
      "          |                                                         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    In file included from include/core.h:57,\n",
      "                     from include/transport.h:13,\n",
      "                     from include/comm.h:10,\n",
      "                     from include/enqueue.h:10,\n",
      "                     from enqueue.cc:7:\n",
      "    enqueue.cc:119:97: warning: ‘cudaError_t cudaLaunchCooperativeKernelMultiDevice(cudaLaunchParams*, unsigned int, unsigned int)’ is deprecated [-Wdeprecated-declarations]\n",
      "      119 |             cudaCooperativeLaunchMultiDeviceNoPreSync|cudaCooperativeLaunchMultiDeviceNoPostSync));\n",
      "          |                                                                                                 ^\n",
      "    include/checks.h:14:23: note: in definition of macro ‘CUDACHECK’\n",
      "       14 |     cudaError_t err = cmd;                                  \\\n",
      "          |                       ^~~\n",
      "    In file included from /usr/local/cuda/include/channel_descriptor.h:61,\n",
      "                     from /usr/local/cuda/include/cuda_runtime.h:95,\n",
      "                     from /wd/notebooks/src/torch/build/nccl/include/nccl.h:10,\n",
      "                     from include/devcomm.h:10,\n",
      "                     from include/transport.h:10,\n",
      "                     from include/comm.h:10,\n",
      "                     from include/enqueue.h:10,\n",
      "                     from enqueue.cc:7:\n",
      "    /usr/local/cuda/include/cuda_runtime_api.h:4202:57: note: declared here\n",
      "     4202 | extern __CUDA_DEPRECATED __host__ cudaError_t CUDARTAPI cudaLaunchCooperativeKernelMultiDevice(struct cudaLaunchParams *launchParamsList, unsigned int numDevices, unsigned int flags  __dv(0));\n",
      "          |                                                         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "    Generating rules                               > /wd/notebooks/src/torch/build/nccl/obj/collectives/device/Makefile.rules\n",
      "    In file included from include/comm.h:10,\n",
      "                     from proxy.cc:7:\n",
      "    include/transport.h:10:10: fatal error: include/devcomm.h: Input/output error\n",
      "       10 | #include \"devcomm.h\"\n",
      "          |          ^~~~~~~~~~~\n",
      "    compilation terminated.\n",
      "    graph/topo.cc:13:10: fatal error: graph/coll_net.h: Input/output error\n",
      "       13 | #include \"coll_net.h\"\n",
      "          |          ^~~~~~~~~~~~\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/proxy.o] Error 1\n",
      "    compilation terminated.\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/graph/topo.o] Error 1\n",
      "    c++: error: misc/utils.cc: Input/output error\n",
      "    c++: fatal error: no input files\n",
      "    compilation terminated.\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/misc/utils.o] Error 1\n",
      "    graph/search.cc:9:10: fatal error: graph/topo.h: Input/output error\n",
      "    compilation terminated.\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/graph/search.o] Error 1\n",
      "    graph/paths.cc:11:10: fatal error: graph/net.h: Input/output error\n",
      "    compilation terminated.\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/graph/paths.o] Error 1\n",
      "    Assembler messages:\n",
      "    Fatal error: can't create /wd/notebooks/src/torch/build/nccl/obj/transport/shm.o: Input/output error\n",
      "    graph/tuning.cc:8:10: fatal error: include/devcomm.h: Input/output error\n",
      "    compilation terminated.\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/graph/tuning.o] Error 1\n",
      "    graph/connect.cc:9:10: fatal error: graph/trees.h: Input/output error\n",
      "    compilation terminated.\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/graph/connect.o] Error 1\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/transport/shm.o] Error 1\n",
      "    In file included from include/comm.h:10,\n",
      "                     from include/group.h:11,\n",
      "                     from group.cc:7:\n",
      "    include/transport.h:10:10: fatal error: include/devcomm.h: Input/output error\n",
      "    compilation terminated.\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/group.o] Error 1\n",
      "    In file included from include/transport.h:11,\n",
      "                     from include/comm.h:10,\n",
      "                     from transport.cc:7:\n",
      "    include/graph.h:103:10: fatal error: include/info.h: Input/output error\n",
      "    compilation terminated.\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/transport.o] Error 1\n",
      "    In file included from debug.cc:7:\n",
      "    include/core.h:15:10: fatal error: /wd/notebooks/src/torch/build/nccl/include/nccl.h: Input/output error\n",
      "    compilation terminated.\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/debug.o] Error 1\n",
      "    In file included from include/core.h:56,\n",
      "                     from misc/nvmlwrap.cc:11:\n",
      "    include/debug.h:18:10: fatal error: include/nccl_net.h: Input/output error\n",
      "    compilation terminated.\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/misc/nvmlwrap.o] Error 1\n",
      "    In file included from include/core.h:56,\n",
      "                     from bootstrap.cc:8:\n",
      "    include/debug.h:18:10: fatal error: include/nccl_net.h: Input/output error\n",
      "    compilation terminated.\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/bootstrap.o] Error 1\n",
      "    /bin/bash: /wd/notebooks/src/torch/build/nccl/obj/channel.d.tmp: Input/output error\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/channel.o] Error 1\n",
      "    In file included from include/core.h:56,\n",
      "                     from include/ibvwrap.h:15,\n",
      "                     from misc/ibvwrap.cc:7:\n",
      "    include/debug.h:10:10: fatal error: include/core.h: Input/output error\n",
      "    compilation terminated.\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/misc/ibvwrap.o] Error 1\n",
      "    /bin/bash: /wd/notebooks/src/torch/build/nccl/obj/misc/argcheck.d.tmp: Input/output error\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/misc/argcheck.o] Error 1\n",
      "    Assembler messages:\n",
      "    Fatal error: can't create /wd/notebooks/src/torch/build/nccl/obj/misc/gdrwrap.o: Input/output error\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/misc/gdrwrap.o] Error 1\n",
      "    /bin/bash: /wd/notebooks/src/torch/build/nccl/obj/transport/p2p.d.tmp: Input/output error\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/transport/p2p.o] Error 1\n",
      "    Assembler messages:\n",
      "    Fatal error: can't create /wd/notebooks/src/torch/build/nccl/obj/collectives/all_gather.o: Input/output error\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/collectives/all_gather.o] Error 1\n",
      "    Assembler messages:\n",
      "    Fatal error: can't create /wd/notebooks/src/torch/build/nccl/obj/collectives/broadcast.o: Input/output error\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/collectives/broadcast.o] Error 1\n",
      "    Assembler messages:\n",
      "    Fatal error: can't create /wd/notebooks/src/torch/build/nccl/obj/transport/coll_net.o: Input/output error\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/transport/coll_net.o] Error 1\n",
      "    Assembler messages:\n",
      "    Fatal error: can't create /wd/notebooks/src/torch/build/nccl/obj/collectives/sendrecv.o: Input/output error\n",
      "    Assembler messages:\n",
      "    Fatal error: can't create /wd/notebooks/src/torch/build/nccl/obj/collectives/all_reduce.o: Input/output error\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/collectives/sendrecv.o] Error 1\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/collectives/all_reduce.o] Error 1\n",
      "    Assembler messages:\n",
      "    Fatal error: can't create /wd/notebooks/src/torch/build/nccl/obj/enqueue.o: Input/output error\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/enqueue.o] Error 1\n",
      "    make[2]: stat: /wd/notebooks/src/torch/build/nccl/obj/collectives/device/Makefile.rules: Input/output error\n",
      "    make[2]: stat: /wd/notebooks/src/torch/build/nccl/obj/collectives/device/colldevice.a: Input/output error\n",
      "    make[2]: stat: /wd/notebooks/src/torch/build/nccl/obj/collectives/device/functions.o: Input/output error\n",
      "    make[2]: stat: /wd/notebooks/src/torch/build/nccl/obj/collectives/device/functions.dep: Input/output error\n",
      "    make[2]: stat: /wd/notebooks/src/torch/build/nccl/obj/collectives/device/devlink.o: Input/output error\n",
      "    mkdir: cannot create directory ‘/wd/notebooks/src/torch/build/nccl/obj/collectives’: Input/output error\n",
      "    make[2]: *** [Makefile:52: /wd/notebooks/src/torch/build/nccl/obj/collectives/device/functions.dep] Error 1\n",
      "    make[2]: Leaving directory '/wd/notebooks/src/torch/third_party/nccl/nccl/src/collectives/device'\n",
      "    make[1]: *** [Makefile:50: /wd/notebooks/src/torch/build/nccl/obj/collectives/device/colldevice.a] Error 2\n",
      "    Assembler messages:\n",
      "    Fatal error: can't create /wd/notebooks/src/torch/build/nccl/obj/collectives/reduce.o: Input/output error\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/collectives/reduce.o] Error 1\n",
      "    Assembler messages:\n",
      "    Fatal error: can't create /wd/notebooks/src/torch/build/nccl/obj/transport/net_socket.o: Input/output error\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/transport/net_socket.o] Error 1\n",
      "    Assembler messages:\n",
      "    Fatal error: can't create /wd/notebooks/src/torch/build/nccl/obj/transport/net_ib.o: Input/output error\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/transport/net_ib.o] Error 1\n",
      "    Assembler messages:\n",
      "    Fatal error: can't create /wd/notebooks/src/torch/build/nccl/obj/transport/net.o: Input/output error\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/transport/net.o] Error 1\n",
      "    c++: error: init.cc: Input/output error\n",
      "    c++: fatal error: no input files\n",
      "    compilation terminated.\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/init.o] Error 1\n",
      "    c++: error: graph/xml.cc: Input/output error\n",
      "    c++: fatal error: no input files\n",
      "    compilation terminated.\n",
      "    make[1]: *** [Makefile:114: /wd/notebooks/src/torch/build/nccl/obj/graph/xml.o] Error 1\n",
      "    make[1]: Leaving directory '/wd/notebooks/src/torch/third_party/nccl/nccl/src'\n",
      "    make: *** [Makefile:25: src.build] Error 2\n",
      "    ninja: build stopped: subcommand failed.\n",
      "    Building wheel torch-1.12.0a0+bd13bc6\n",
      "    -- Building version 1.12.0a0+bd13bc6\n",
      "    cmake -GNinja -DBUILD_PYTHON=True -DBUILD_TEST=True -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/wd/notebooks/src/torch/torch -DCMAKE_PREFIX_PATH=/opt/conda/lib/python3.8/site-packages -DNUMPY_INCLUDE_DIR=/opt/conda/lib/python3.8/site-packages/numpy/core/include -DPYTHON_EXECUTABLE=/opt/conda/bin/python3.8 -DPYTHON_INCLUDE_DIR=/opt/conda/include/python3.8 -DPYTHON_LIBRARY=/opt/conda/lib/libpython3.8.a -DTORCH_BUILD_VERSION=1.12.0a0+bd13bc6 -DUSE_EXPERIMENTAL_CUDNN_V8_API=1 -DUSE_NUMPY=True /wd/notebooks/src/torch\n",
      "    cmake --build . --target install --config Release\n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: /opt/conda/bin/python3.8 -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/wd/notebooks/src/torch/setup.py'\"'\"'; __file__='\"'\"'/wd/notebooks/src/torch/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' develop --no-deps Check the logs for full command output.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch -y\n",
    "!pip install -e  git+https://github.com/pytorch/pytorch#egg=torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39m__version__\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1070 with Max-Q Design _CudaDeviceProperties(name='NVIDIA GeForce GTX 1070 with Max-Q Design', major=6, minor=1, total_memory=8191MB, multi_processor_count=16)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(torch.cuda.get_device_name(0), torch.cuda.get_device_properties(device))\n",
    "\n",
    "input_dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, multi_conv=False):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        if multi_conv:\n",
    "            if patch_size[0] == 12:\n",
    "                self.proj = nn.Sequential(\n",
    "                    nn.Conv2d(in_chans, embed_dim // 4, kernel_size=7, stride=4, padding=3),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=3, padding=0),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "            elif patch_size[0] == 16:\n",
    "                self.proj = nn.Sequential(\n",
    "                    nn.Conv2d(in_chans, embed_dim // 4, kernel_size=7, stride=4, padding=3),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=3, stride=2, padding=1),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=3, stride=2, padding=1),\n",
    "                )\n",
    "        else:\n",
    "            self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.wq = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.wk = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.wv = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, N, C = x.shape\n",
    "        q = self.wq(x[:, 0:1, ...]).reshape(B, 1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)  # B1C -> B1H(C/H) -> BH1(C/H)\n",
    "        k = self.wk(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)  # BNC -> BNH(C/H) -> BHN(C/H)\n",
    "        v = self.wv(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)  # BNC -> BNH(C/H) -> BHN(C/H)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale  # BH1(C/H) @ BH(C/H)N -> BH1N\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, 1, C)   # (BH1N @ BHN(C/H)) -> BH1(C/H) -> B1H(C/H) -> B1C\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, has_mlp=True):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = CrossAttention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.has_mlp = has_mlp\n",
    "        if has_mlp:\n",
    "            self.norm2 = norm_layer(dim)\n",
    "            mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "            self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:, 0:1, ...] + self.drop_path(self.attn(self.norm1(x)))\n",
    "        if self.has_mlp:\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiScaleBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, patches, depth, num_heads, mlp_ratio, qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "\n",
    "        num_branches = len(dim)\n",
    "        self.num_branches = num_branches\n",
    "        # different branch could have different embedding size, the first one is the base\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for d in range(num_branches):\n",
    "            tmp = []\n",
    "            for i in range(depth[d]):\n",
    "                tmp.append(\n",
    "                    Block(dim=dim[d], num_heads=num_heads[d], mlp_ratio=mlp_ratio[d], qkv_bias=qkv_bias, \n",
    "                          drop=drop, attn_drop=attn_drop, drop_path=drop_path[i], norm_layer=norm_layer))\n",
    "            if len(tmp) != 0:\n",
    "                self.blocks.append(nn.Sequential(*tmp))\n",
    "\n",
    "        if len(self.blocks) == 0:\n",
    "            self.blocks = None\n",
    "\n",
    "        self.projs = nn.ModuleList()\n",
    "        for d in range(num_branches):\n",
    "            if dim[d] == dim[(d+1) % num_branches] and False:\n",
    "                tmp = [nn.Identity()]\n",
    "            else:\n",
    "                tmp = [norm_layer(dim[d]), act_layer(), nn.Linear(dim[d], dim[(d+1) % num_branches])]\n",
    "            self.projs.append(nn.Sequential(*tmp))\n",
    "\n",
    "        self.fusion = nn.ModuleList()\n",
    "        for d in range(num_branches):\n",
    "            d_ = (d+1) % num_branches\n",
    "            nh = num_heads[d_]\n",
    "            if depth[-1] == 0:  # backward capability:\n",
    "                self.fusion.append(CrossAttentionBlock(dim=dim[d_], num_heads=nh, mlp_ratio=mlp_ratio[d], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                                       drop=drop, attn_drop=attn_drop, drop_path=drop_path[-1], norm_layer=norm_layer,\n",
    "                                                       has_mlp=False))\n",
    "            else:\n",
    "                tmp = []\n",
    "                for _ in range(depth[-1]):\n",
    "                    tmp.append(CrossAttentionBlock(dim=dim[d_], num_heads=nh, mlp_ratio=mlp_ratio[d], qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                                                   drop=drop, attn_drop=attn_drop, drop_path=drop_path[-1], norm_layer=norm_layer,\n",
    "                                                   has_mlp=False))\n",
    "                self.fusion.append(nn.Sequential(*tmp))\n",
    "\n",
    "        self.revert_projs = nn.ModuleList()\n",
    "        for d in range(num_branches):\n",
    "            tmp = [norm_layer(dim[(d+1) % num_branches]), act_layer(), nn.Linear(dim[(d+1) % num_branches], dim[d])]\n",
    "            self.revert_projs.append(nn.Sequential(*tmp))\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs_b = [block(x_) for x_, block in zip(x, self.blocks)]\n",
    "        # only take the cls token out\n",
    "        proj_cls_token = [proj(x[:, 0:1]) for x, proj in zip(outs_b, self.projs)]\n",
    "        # cross attention\n",
    "        outs = []\n",
    "        for i in range(self.num_branches):\n",
    "            tmp = torch.cat((proj_cls_token[i], outs_b[(i + 1) % self.num_branches][:, 1:, ...]), dim=1)\n",
    "            tmp = self.fusion[i](tmp)\n",
    "            reverted_proj_cls_token = self.revert_projs[i](tmp[:, 0:1, ...])\n",
    "            tmp = torch.cat((reverted_proj_cls_token, outs_b[i][:, 1:, ...]), dim=1)\n",
    "            outs.append(tmp)\n",
    "        return outs\n",
    "\n",
    "\n",
    "def _compute_num_patches(img_size, patches):\n",
    "    return [i // p * i // p for i, p in zip(img_size,patches)]\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=(224, 224), patch_size=(16, 16), in_chans=3, num_classes=1, embed_dim=(256, 256), depth=([1, 3, 1], [1, 3, 1], [1, 3, 1]),\n",
    "                 num_heads=(8, 8), mlp_ratio=(2., 2., 4.), qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=nn.LayerNorm, multi_conv=False):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        if not isinstance(img_size, list):\n",
    "            img_size = to_2tuple(img_size)\n",
    "        self.img_size = img_size\n",
    "\n",
    "        num_patches = _compute_num_patches(img_size, patch_size)\n",
    "        self.num_branches = 2\n",
    "\n",
    "        self.patch_embed = nn.ModuleList()\n",
    "\n",
    "        self.pos_embed = nn.ParameterList([nn.Parameter(torch.zeros(1, 1 + num_patches[i], embed_dim[i])) for i in range(self.num_branches)])\n",
    "        for im_s, p, d in zip(img_size, patch_size, embed_dim):\n",
    "            self.patch_embed.append(PatchEmbed(img_size=im_s, patch_size=p, in_chans=in_chans, embed_dim=d, multi_conv=multi_conv))\n",
    "\n",
    "\n",
    "        self.cls_token = nn.ParameterList([nn.Parameter(torch.zeros(1, 1, embed_dim[i])) for i in range(self.num_branches)])\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        total_depth = sum([sum(x[-2:]) for x in depth])\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, total_depth)]  # stochastic depth decay rule\n",
    "        dpr_ptr = 0\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for idx, block_cfg in enumerate(depth):\n",
    "            curr_depth = max(block_cfg[:-1]) + block_cfg[-1]\n",
    "            dpr_ = dpr[dpr_ptr:dpr_ptr + curr_depth]\n",
    "            blk = MultiScaleBlock(embed_dim, num_patches, block_cfg, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                                  qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr_,\n",
    "                                  norm_layer=norm_layer)\n",
    "            dpr_ptr += curr_depth\n",
    "            self.blocks.append(blk)\n",
    "\n",
    "        self.norm = nn.ModuleList([norm_layer(embed_dim[i]) for i in range(self.num_branches)])\n",
    "        \n",
    "        self.dino_emb = nn.Identity()\n",
    "        \n",
    "        self.head_input_size = embed_dim[0]*self.num_branches\n",
    "        self.head_hidden_size = embed_dim[0]*self.num_branches*2\n",
    "        \n",
    "        self.head = nn.Sequential(OrderedDict([\n",
    "          ('head_l1', nn.Linear(self.head_input_size, 150, bias=False)),\n",
    "          ('head_b1', nn.BatchNorm1d(150)),\n",
    "          ('head_act1', nn.ReLU()),\n",
    "          ('head_dp1', nn.Dropout(.2)),\n",
    "          ('head_l3', nn.Linear(150, 50, bias=False)),\n",
    "          ('head_b3', nn.BatchNorm1d(50)),\n",
    "          ('head_act3', nn.ReLU()),\n",
    "          ('head_dp3', nn.Dropout(.2)),\n",
    "          ('head_cls', nn.Linear(50, self.num_classes)),\n",
    "        ]))\n",
    "\n",
    "        for i in range(self.num_branches):\n",
    "            if self.pos_embed[i].requires_grad:\n",
    "                trunc_normal_(self.pos_embed[i], std=.02)\n",
    "            trunc_normal_(self.cls_token[i], std=.02)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        out = {'cls_token'}\n",
    "        if self.pos_embed[0].requires_grad:\n",
    "            out.add('pos_embed')\n",
    "        return out\n",
    "\n",
    "    def forward_features(self, query, reference):\n",
    "        B, C, H, W = query.shape\n",
    "        xs = []\n",
    "        for i, x in enumerate([query, reference]):\n",
    "            x_ = torch.nn.functional.interpolate(x, size=(self.img_size[i], self.img_size[i]), mode='bicubic') if H != self.img_size[i] else x\n",
    "            tmp = self.patch_embed[i](x_)\n",
    "            cls_tokens = self.cls_token[i].expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "            tmp = torch.cat((cls_tokens, tmp), dim=1)\n",
    "            tmp = tmp + self.pos_embed[i]\n",
    "            tmp = self.pos_drop(tmp)\n",
    "            xs.append(tmp)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            xs = blk(xs)\n",
    "\n",
    "        # NOTE: was before branch token section, move to here to assure all branch token are before layer norm\n",
    "        xs = [self.norm[i](x) for i, x in enumerate(xs)]\n",
    "        out = [x[:, 0] for x in xs]\n",
    "\n",
    "        return out[0], out[1]\n",
    "\n",
    "    def forward(self, query, reference=None):\n",
    "        \n",
    "        # Pretraining\n",
    "        if reference is None:\n",
    "            reference = query\n",
    "        \n",
    "        q_cls, r_cls = self.forward_features(query, reference)\n",
    "        \n",
    "        x = torch.cat([q_cls, r_cls], dim=1)\n",
    "        \n",
    "        x = self.dino_emb(x)\n",
    "        \n",
    "        ce_logits = self.head(x)\n",
    "        \n",
    "        return ce_logits\n",
    "\n",
    "\n",
    "def crossvit_base_448(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(img_size=[448, 448],\n",
    "                              patch_size=[32, 32], embed_dim=[384, 384], \n",
    "                              depth=[[1, 4, 1], [1, 4, 1], [1, 4, 1], [1, 4, 1], [1, 4, 1]],\n",
    "                              num_heads=[6, 6], mlp_ratio=[3, 3, 1], qkv_bias=True,\n",
    "                              norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    return model\n",
    "\n",
    "def crossvit_base_244(pretrained=False, **kwargs):\n",
    "    model = VisionTransformer(img_size=(224, 224), \n",
    "                             patch_size=(32, 32), \n",
    "                             embed_dim=(192, 192), \n",
    "                             depth=([1, 3, 0], [1, 3, 0], [1, 3, 0]),\n",
    "                             num_heads=(3, 3),\n",
    "                             mlp_ratio=[4, 4, 1], qkv_bias=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): ModuleList(\n",
       "    (0): PatchEmbed(\n",
       "      (proj): Conv2d(3, 192, kernel_size=(32, 32), stride=(32, 32))\n",
       "    )\n",
       "    (1): PatchEmbed(\n",
       "      (proj): Conv2d(3, 192, kernel_size=(32, 32), stride=(32, 32))\n",
       "    )\n",
       "  )\n",
       "  (pos_embed): ParameterList(\n",
       "      (0): Parameter containing: [torch.FloatTensor of size 1x50x192]\n",
       "      (1): Parameter containing: [torch.FloatTensor of size 1x50x192]\n",
       "  )\n",
       "  (cls_token): ParameterList(\n",
       "      (0): Parameter containing: [torch.FloatTensor of size 1x1x192]\n",
       "      (1): Parameter containing: [torch.FloatTensor of size 1x1x192]\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): MultiScaleBlock(\n",
       "      (blocks): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (projs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (fusion): ModuleList(\n",
       "        (0): CrossAttentionBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): CrossAttention(\n",
       "            (wq): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (wk): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (wv): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): CrossAttentionBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): CrossAttention(\n",
       "            (wq): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (wk): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (wv): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (revert_projs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): MultiScaleBlock(\n",
       "      (blocks): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (projs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (fusion): ModuleList(\n",
       "        (0): CrossAttentionBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): CrossAttention(\n",
       "            (wq): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (wk): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (wv): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): CrossAttentionBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): CrossAttention(\n",
       "            (wq): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (wk): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (wv): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (revert_projs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): MultiScaleBlock(\n",
       "      (blocks): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate=none)\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (projs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (fusion): ModuleList(\n",
       "        (0): CrossAttentionBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): CrossAttention(\n",
       "            (wq): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (wk): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (wv): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (1): CrossAttentionBlock(\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): CrossAttention(\n",
       "            (wq): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (wk): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (wv): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "      (revert_projs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): GELU(approximate=none)\n",
       "          (2): Linear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): ModuleList(\n",
       "    (0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (dino_emb): Identity()\n",
       "  (head): Sequential(\n",
       "    (head_l1): Linear(in_features=384, out_features=150, bias=False)\n",
       "    (head_b1): BatchNorm1d(150, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (head_act1): ReLU()\n",
       "    (head_dp1): Dropout(p=0.2, inplace=False)\n",
       "    (head_l3): Linear(in_features=150, out_features=50, bias=False)\n",
       "    (head_b3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (head_act3): ReLU()\n",
       "    (head_dp3): Dropout(p=0.2, inplace=False)\n",
       "    (head_cls): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = crossvit_base_244()\n",
    "\n",
    "ckpt = torch.load('../submission/net.pt')\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = torch.zeros((2,3,224,224), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "hasattr(): attribute name must be string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m traced_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/jit/_trace.py:741\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m--> 741\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    757\u001b[0m ):\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    759\u001b[0m         func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m,\n\u001b[1;32m    760\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m: example_inputs},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    767\u001b[0m         _module_class,\n\u001b[1;32m    768\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/jit/_trace.py:942\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    939\u001b[0m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_trace\u001b[38;5;241m.\u001b[39m_trace_module_map \u001b[38;5;241m=\u001b[39m trace_module_map\n\u001b[1;32m    940\u001b[0m register_submods(mod, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__module\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 942\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mmake_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_compilation_unit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method_name, example_inputs \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    946\u001b[0m         \u001b[38;5;66;03m# \"forward\" is a special case because we need to trace\u001b[39;00m\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;66;03m# `Module.__call__`, which sets up some extra tracing, but uses\u001b[39;00m\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;66;03m# argument names of the real `Module.forward` method.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/jit/_trace.py:568\u001b[0m, in \u001b[0;36mmake_module\u001b[0;34m(mod, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _module_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    567\u001b[0m     _module_class \u001b[38;5;241m=\u001b[39m TopLevelTracedModule\n\u001b[0;32m--> 568\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_module_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_compilation_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_compilation_unit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/jit/_trace.py:1067\u001b[0m, in \u001b[0;36mTracedModule.__init__\u001b[0;34m(self, orig, id_set, _compilation_unit)\u001b[0m\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m submodule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1066\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m-> 1067\u001b[0m     tmp_module\u001b[38;5;241m.\u001b[39m_modules[name] \u001b[38;5;241m=\u001b[39m \u001b[43mmake_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTracedModule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_compilation_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m script_module \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_recursive\u001b[38;5;241m.\u001b[39mcreate_script_module(\n\u001b[1;32m   1072\u001b[0m     tmp_module, \u001b[38;5;28;01mlambda\u001b[39;00m module: (), share_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, is_tracing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1073\u001b[0m )\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(orig)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/jit/_trace.py:556\u001b[0m, in \u001b[0;36mmake_module\u001b[0;34m(mod, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, ScriptModule):\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mod\n\u001b[0;32m--> 556\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jit_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_has_exports\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    558\u001b[0m     infer_methods_stubs_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_recursive\u001b[38;5;241m.\u001b[39mmake_stubs_from_exported_methods\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_recursive\u001b[38;5;241m.\u001b[39mcreate_script_module(\n\u001b[1;32m    560\u001b[0m         mod,\n\u001b[1;32m    561\u001b[0m         infer_methods_stubs_fn,\n\u001b[1;32m    562\u001b[0m         share_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    563\u001b[0m         is_tracing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    564\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/_jit_internal.py:668\u001b[0m, in \u001b[0;36mmodule_has_exports\u001b[0;34m(mod)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodule_has_exports\u001b[39m(mod):\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(mod):\n\u001b[0;32m--> 668\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    669\u001b[0m             item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, name)\n\u001b[1;32m    670\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m callable(item):\n",
      "\u001b[0;31mTypeError\u001b[0m: hasattr(): attribute name must be string"
     ]
    }
   ],
   "source": [
    "traced_model = torch.jit.trace(model, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [torch_tensorrt.Input(\n",
    "            min_shape=[1, 3, 224, 224],\n",
    "            opt_shape=[1, 3, 224, 224],\n",
    "            max_shape=[1, 3, 224, 224],\n",
    "            dtype=torch.float32,\n",
    "        )]\n",
    "        \n",
    "enabled_precisions = {torch.float, torch.half} # Run with fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "getattr(): attribute name must be string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trt_ts_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_tensorrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menabled_precisions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menabled_precisions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch_tensorrt/_compile.py:114\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(module, ir, inputs, enabled_precisions, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_type \u001b[38;5;241m==\u001b[39m _ModuleType\u001b[38;5;241m.\u001b[39mnn:\n\u001b[1;32m    110\u001b[0m         logging\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m    111\u001b[0m             logging\u001b[38;5;241m.\u001b[39mLevel\u001b[38;5;241m.\u001b[39mInfo,\n\u001b[1;32m    112\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule was provided as a torch.nn.Module, trying to script the module with torch.jit.script. In the event of a failure please preconvert your module to TorchScript\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m         )\n\u001b[0;32m--> 114\u001b[0m         ts_mod \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_tensorrt\u001b[38;5;241m.\u001b[39mts\u001b[38;5;241m.\u001b[39mcompile(ts_mod, inputs\u001b[38;5;241m=\u001b[39minputs, enabled_precisions\u001b[38;5;241m=\u001b[39menabled_precisions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m target_ir \u001b[38;5;241m==\u001b[39m _IRType\u001b[38;5;241m.\u001b[39mfx:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/jit/_script.py:1266\u001b[0m, in \u001b[0;36mscript\u001b[0;34m(obj, optimize, _frames_up, _rcb, example_inputs)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m   1265\u001b[0m     obj \u001b[38;5;241m=\u001b[39m call_prepare_scriptable_func(obj)\n\u001b[0;32m-> 1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_script_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recursive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_methods_to_compile\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m create_script_dict(obj)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/jit/_recursive.py:454\u001b[0m, in \u001b[0;36mcreate_script_module\u001b[0;34m(nn_module, stubs_fn, share_types, is_tracing)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing:\n\u001b[1;32m    453\u001b[0m     AttributeTypeIsSupportedChecker()\u001b[38;5;241m.\u001b[39mcheck(nn_module)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/jit/_recursive.py:516\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    513\u001b[0m     script_module\u001b[38;5;241m.\u001b[39m_concrete_type \u001b[38;5;241m=\u001b[39m concrete_type\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# Actually create the ScriptModule, initializing it with the function we just defined\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m script_module \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecursiveScriptModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcpp_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# Compile methods if necessary\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m concrete_type_store\u001b[38;5;241m.\u001b[39mmethods_compiled:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/jit/_script.py:594\u001b[0m, in \u001b[0;36mRecursiveScriptModule._construct\u001b[0;34m(cpp_module, init_fn)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;124;03mConstruct a RecursiveScriptModule that's ready for use. PyTorch\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;124;03mcode should use this to construct a RecursiveScriptModule instead\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;124;03m    init_fn:  Lambda that initializes the RecursiveScriptModule passed to it.\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    593\u001b[0m script_module \u001b[38;5;241m=\u001b[39m RecursiveScriptModule(cpp_module)\n\u001b[0;32m--> 594\u001b[0m \u001b[43minit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;66;03m# Finalize the ScriptModule: replace the nn.Module state with our\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# custom implementations and flip the _initializing bit.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m RecursiveScriptModule\u001b[38;5;241m.\u001b[39m_finalize_scriptmodule(script_module)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/jit/_recursive.py:494\u001b[0m, in \u001b[0;36mcreate_script_module_impl.<locals>.init_fn\u001b[0;34m(script_module)\u001b[0m\n\u001b[1;32m    491\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m orig_value\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;66;03m# always reuse the provided stubs_fn to infer the methods to compile\u001b[39;00m\n\u001b[0;32m--> 494\u001b[0m     scripted \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_script_module_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_concrete_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstubs_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m cpp_module\u001b[38;5;241m.\u001b[39msetattr(name, scripted)\n\u001b[1;32m    497\u001b[0m script_module\u001b[38;5;241m.\u001b[39m_modules[name] \u001b[38;5;241m=\u001b[39m scripted\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/jit/_recursive.py:466\u001b[0m, in \u001b[0;36mcreate_script_module_impl\u001b[0;34m(nn_module, concrete_type, stubs_fn)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03mConvert an nn.Module to a RecursiveScriptModule.\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;124;03m    stubs_fn:  Lambda that takes an nn.Module and generates a list of ScriptMethodStubs to compile.\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m cpp_module \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_create_module_with_type(concrete_type\u001b[38;5;241m.\u001b[39mjit_type)\n\u001b[0;32m--> 466\u001b[0m method_stubs \u001b[38;5;241m=\u001b[39m \u001b[43mstubs_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m property_stubs \u001b[38;5;241m=\u001b[39m get_property_stubs(nn_module)\n\u001b[1;32m    468\u001b[0m hook_stubs, pre_hook_stubs \u001b[38;5;241m=\u001b[39m get_hook_stubs(nn_module)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/jit/_recursive.py:704\u001b[0m, in \u001b[0;36minfer_methods_to_compile\u001b[0;34m(nn_module)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m ignored_properties:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 704\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnn_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _jit_internal\u001b[38;5;241m.\u001b[39mget_torchscript_modifier(item) \u001b[38;5;129;01mis\u001b[39;00m _jit_internal\u001b[38;5;241m.\u001b[39mFunctionModifiers\u001b[38;5;241m.\u001b[39mEXPORT:\n\u001b[1;32m    706\u001b[0m     exported\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "\u001b[0;31mTypeError\u001b[0m: getattr(): attribute name must be string"
     ]
    }
   ],
   "source": [
    "trt_ts_module = torch_tensorrt.compile(model, inputs=inputs, enabled_precisions=enabled_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trt_ts_module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m input_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrt_ts_module\u001b[49m(input_data)\n\u001b[1;32m      3\u001b[0m result\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trt_ts_module' is not defined"
     ]
    }
   ],
   "source": [
    "result = trt_ts_module(input_data)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(trt_ts_module, \"trt_ts_module.ts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment application\n",
    "\n",
    "trt_ts_module = torch.jit.load(\"trt_ts_module.ts\")\n",
    "input_data = input_data.to('cuda').half()\n",
    "result = trt_ts_module(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e460829be586a745d810aec71d83684bd38b76dd3b8d2db700ccf14d30953fce"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
