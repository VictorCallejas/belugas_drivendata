{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLAB\n",
    "!pip install neptune-client\n",
    "# pip install torch-tensorrt -f https://github.com/NVIDIA/Torch-TensorRT/releases\n",
    "!unzip data.zip\n",
    "!mkdir artifacts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "from torchvision import transforms as T\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import itertools\n",
    "\n",
    "import neptune.new as neptune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/victorcallejas/Belluga/e/BEL-192\n",
      "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#.stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init(\n",
    "    project=\"victorcallejas/Belluga\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlNDRlNTJiNC00OTQwLTQxYjgtYWZiNS02OWQ0MDcwZmU5N2YifQ==\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1070 with Max-Q Design _CudaDeviceProperties(name='NVIDIA GeForce GTX 1070 with Max-Q Design', major=6, minor=1, total_memory=8191MB, multi_processor_count=16)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(torch.cuda.get_device_name(0), torch.cuda.get_device_properties(device))\n",
    "\n",
    "fp16 = False\n",
    "input_dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCORING\n",
    "PREDICTION_LIMIT = 20\n",
    "QUERY_ID_COL = \"query_id\"\n",
    "DATABASE_ID_COL = \"database_image_id\"\n",
    "SCORE_COL = \"score\"\n",
    "\n",
    "SCORE_THRESHOLD = 0.5\n",
    "\n",
    "class MeanAveragePrecision:\n",
    "    @classmethod\n",
    "    def score(cls, predicted: pd.DataFrame, actual: pd.DataFrame, prediction_limit: int):\n",
    "        \"\"\"Calculates mean average precision for a ranking task.\n",
    "        :param predicted: The predicted values as a dataframe with specified column names\n",
    "        :param actual: The ground truth values as a dataframe with specified column names\n",
    "        \"\"\"\n",
    "        if not predicted[SCORE_COL].between(0.0, 1.0).all():\n",
    "            raise ValueError(\"Scores must be in range [0, 1].\")\n",
    "        if predicted.index.name != QUERY_ID_COL:\n",
    "            raise ValueError(\n",
    "                f\"First column of submission must be named '{QUERY_ID_COL}', \"\n",
    "                f\"got {predicted.index.name}.\"\n",
    "            )\n",
    "        if predicted.columns.to_list() != [DATABASE_ID_COL, SCORE_COL]:\n",
    "            raise ValueError(\n",
    "                f\"Columns of submission must be named '{[DATABASE_ID_COL, SCORE_COL]}', \"\n",
    "                f\"got {predicted.columns.to_list()}.\"\n",
    "            )\n",
    "\n",
    "        unadjusted_aps, predicted_n_pos, actual_n_pos = cls._score_per_query(\n",
    "            predicted, actual, prediction_limit\n",
    "        )\n",
    "        adjusted_aps = unadjusted_aps.multiply(predicted_n_pos).divide(actual_n_pos)\n",
    "        return adjusted_aps.mean()\n",
    "\n",
    "    @classmethod\n",
    "    def _score_per_query(\n",
    "        cls, predicted: pd.DataFrame, actual: pd.DataFrame, prediction_limit: int\n",
    "    ):\n",
    "        \"\"\"Calculates per-query mean average precision for a ranking task.\"\"\"\n",
    "        merged = predicted.merge(\n",
    "            right=actual.assign(actual=1.0),\n",
    "            how=\"left\",\n",
    "            on=[QUERY_ID_COL, DATABASE_ID_COL],\n",
    "        ).fillna({\"actual\": 0.0})\n",
    "        # Per-query raw average precisions based on predictions\n",
    "        unadjusted_aps = merged.groupby(QUERY_ID_COL).apply(\n",
    "            lambda df: average_precision_score(df[\"actual\"].values, df[SCORE_COL].values)\n",
    "            if df[\"actual\"].sum()\n",
    "            else 0.0\n",
    "        )\n",
    "        # Total ground truth positive counts for rescaling\n",
    "        predicted_n_pos = merged[\"actual\"].groupby(QUERY_ID_COL).sum().astype(\"int64\").rename()\n",
    "        actual_n_pos = actual.groupby(QUERY_ID_COL).size().clip(upper=prediction_limit)\n",
    "        return unadjusted_aps, predicted_n_pos, actual_n_pos\n",
    "    \n",
    "    \n",
    "def map_score(dataloader, model, threshold=SCORE_THRESHOLD):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    sub = []\n",
    "    \n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    with torch.no_grad():        \n",
    "    \n",
    "        for query, reference, query_id, reference_id in tqdm(dataloader):\n",
    "            \n",
    "            query = query.to(device, non_blocking=True, dtype=input_dtype)\n",
    "            reference = reference.to(device, non_blocking=True, dtype=input_dtype)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled = fp16):\n",
    "                logits = sigmoid(model(query=query, reference=reference)).cpu().squeeze().tolist()\n",
    "                \n",
    "            sub.extend(zip(query_id, reference_id, logits))\n",
    "            \n",
    "    sub = pd.DataFrame(sub, columns=['query_id', 'database_image_id', 'score'])\n",
    "    #sub = sub[sub.score > threshold]\n",
    "    sub = sub.set_index(['database_image_id']).groupby('query_id')['score'].nlargest(20).reset_index()\n",
    "    sub = sub.set_index('query_id')\n",
    "    \n",
    "    mean_avg_prec = MeanAveragePrecision.score(\n",
    "        predicted=sub, actual=dataloader.dataset.gt, prediction_limit=PREDICTION_LIMIT\n",
    "    )\n",
    "    \n",
    "    print('MaP: ',mean_avg_prec)\n",
    "    return mean_avg_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 43.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# DATA\n",
    "\n",
    "IMG_SIZE = 256\n",
    "ROOT_DIR = '../data/'\n",
    "NORM_TRANSFORMS = torch.nn.Sequential(\n",
    "    T.Resize([IMG_SIZE, IMG_SIZE]),\n",
    "    T.ConvertImageDtype(input_dtype),\n",
    "    T.Normalize(mean = (0.4234, 0.4272, 0.4641),\n",
    "                std  = (0.2037, 0.2027, 0.2142)),\n",
    ")\n",
    "\n",
    "VAL_SPLIT = 0.05\n",
    "\n",
    "METADATA = pd.read_csv('../data/metadata.csv')[:50]\n",
    "\n",
    "TRAIN, VAL = train_test_split(METADATA, test_size=0.05, random_state=42)\n",
    "TRAIN, VAL = TRAIN.reset_index(), VAL.reset_index()\n",
    "#TRAIN, VAL = METADATA, METADATA\n",
    "#TRAIN = METADATA\n",
    "\n",
    "def getImages(metadata):\n",
    "    IMAGES = {}\n",
    "    for image_id, path in tqdm(zip(metadata.image_id, metadata.path), total=metadata.shape[0]):\n",
    "        IMAGES[image_id] = NORM_TRANSFORMS(read_image(ROOT_DIR + path))\n",
    "    return IMAGES\n",
    "\n",
    "IMAGES = getImages(METADATA)\n",
    "\n",
    "class PreTrain_BellugaDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, metadata):\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.metadata.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return IMAGES[self.metadata.image_id[idx]]\n",
    "\n",
    "class Eval_BellugaDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, metadata):\n",
    "        self.metadata = metadata\n",
    "    \n",
    "        # GROUND TRUTH\n",
    "        gt = []\n",
    "        for wid in self.metadata.whale_id: # query\n",
    "            tmp = self.metadata[self.metadata.whale_id == wid].image_id.tolist() # get all images id\n",
    "            gt.extend(list(itertools.permutations(tmp, 2)))\n",
    "        self.gt = pd.DataFrame(gt,columns=['query_id','database_image_id'])\n",
    "        self.gt = self.gt.set_index('query_id')\n",
    "        \n",
    "        # ALL QUERIES\n",
    "        self.query_reference = list(itertools.permutations(self.metadata.image_id, 2))\n",
    "            \n",
    "    def getimage(self, image_id):\n",
    "        return IMAGES[image_id]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.query_reference)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        query_id = self.query_reference[idx][0]\n",
    "        reference_id = self.query_reference[idx][1]\n",
    "        \n",
    "        query = self.getimage(query_id)\n",
    "        reference = self.getimage(reference_id)\n",
    "        \n",
    "        return query, reference, query_id, reference_id\n",
    "    \n",
    "    \n",
    "class Train_BellugaDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, metadata):\n",
    "        self.metadata = metadata\n",
    "        self.aug = T.RandomErasing(p=0.4, scale=(0.12, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n",
    "            \n",
    "    def getimage(self, image_id):\n",
    "        return IMAGES[image_id]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.metadata.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        anchor = self.aug(self.getimage(self.metadata.image_id[idx]))\n",
    "        label = self.metadata.whale_id[idx]\n",
    "        \n",
    "        pos = self.aug(self.getimage(self.metadata[self.metadata.whale_id == label].sample()['image_id'].values[0]))\n",
    "        neg = self.aug(self.getimage(self.metadata[self.metadata.whale_id != label].sample()['image_id'].values[0]))\n",
    "\n",
    "        return anchor, pos, neg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATALOADERS\n",
    "PRETRAIN_BS = 32\n",
    "TRAIN_BS = 32\n",
    "INFER_BS = TRAIN_BS\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "pretrain_dataset = PreTrain_BellugaDataset(METADATA)\n",
    "train_train_dataset = Train_BellugaDataset(TRAIN)\n",
    "#train_eval_dataset = Eval_BellugaDataset(TRAIN)\n",
    "valid_eval_dataset = Eval_BellugaDataset(VAL)\n",
    "\n",
    "pretrain_dataloader = torch.utils.data.DataLoader(\n",
    "                        pretrain_dataset, \n",
    "                        batch_size=PRETRAIN_BS,\n",
    "                        shuffle=True, \n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        pin_memory=True\n",
    "                    )\n",
    "\n",
    "train_train_dataloader = torch.utils.data.DataLoader(\n",
    "                        train_train_dataset, \n",
    "                        batch_size=TRAIN_BS,\n",
    "                        shuffle=True, \n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        pin_memory=True\n",
    "                    )\n",
    "'''\n",
    "train_eval_dataloader = torch.utils.data.DataLoader(\n",
    "                        train_eval_dataset, \n",
    "                        batch_size=INFER_BS,\n",
    "                        shuffle=True, \n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        pin_memory=True\n",
    "                    )\n",
    "'''\n",
    "valid_eval_dataloader = torch.utils.data.DataLoader(\n",
    "                        valid_eval_dataset, \n",
    "                        batch_size=INFER_BS,\n",
    "                        shuffle=False, \n",
    "                        num_workers=NUM_WORKERS,\n",
    "                        pin_memory=True\n",
    "                    )   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from einops import repeat\n",
    "\n",
    "class SimMIM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        encoder,\n",
    "        masking_ratio = 0.5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert masking_ratio > 0 and masking_ratio < 1, 'masking ratio must be kept between 0 and 1'\n",
    "        self.masking_ratio = masking_ratio\n",
    "\n",
    "        # extract some hyperparameters and functions from encoder (vision transformer to be trained)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        num_patches, encoder_dim = encoder.pos_embedding.shape[-2:]\n",
    "        self.to_patch, self.patch_to_emb = encoder.to_patch_embedding[:2]\n",
    "        pixel_values_per_patch = self.patch_to_emb.weight.shape[-1]\n",
    "\n",
    "        # simple linear head\n",
    "\n",
    "        self.mask_token = nn.Parameter(torch.randn(encoder_dim))\n",
    "        self.to_pixels = nn.Linear(encoder_dim, pixel_values_per_patch)\n",
    "\n",
    "    def forward(self, img):\n",
    "        device = img.device\n",
    "\n",
    "        # get patches\n",
    "\n",
    "        patches = self.to_patch(img)\n",
    "        batch, num_patches, *_ = patches.shape\n",
    "\n",
    "        # for indexing purposes\n",
    "\n",
    "        batch_range = torch.arange(batch, device = device)[:, None]\n",
    "\n",
    "        # get positions\n",
    "\n",
    "        pos_emb = self.encoder.pos_embedding[:,:num_patches]\n",
    "\n",
    "        # patch to encoder tokens and add positions\n",
    "\n",
    "        tokens = self.patch_to_emb(patches)\n",
    "        tokens = tokens + pos_emb\n",
    "\n",
    "        # prepare mask tokens\n",
    "\n",
    "        mask_tokens = repeat(self.mask_token, 'd -> b n d', b = batch, n = num_patches)\n",
    "        mask_tokens = mask_tokens + pos_emb\n",
    "\n",
    "        # calculate of patches needed to be masked, and get positions (indices) to be masked\n",
    "\n",
    "        num_masked = int(self.masking_ratio * num_patches)\n",
    "        masked_indices = torch.rand(batch, num_patches, device = device).topk(k = num_masked, dim = -1).indices\n",
    "        masked_bool_mask = torch.zeros((batch, num_patches), device = device).scatter_(-1, masked_indices, 1).bool()\n",
    "\n",
    "        # mask tokens\n",
    "\n",
    "        tokens = torch.where(masked_bool_mask[..., None], mask_tokens, tokens)\n",
    "\n",
    "        # attend with vision transformer\n",
    "\n",
    "        encoded = self.encoder.patch_transformer(tokens)\n",
    "\n",
    "        # get the masked tokens\n",
    "\n",
    "        encoded_mask_tokens = encoded[batch_range, masked_indices]\n",
    "\n",
    "        # small linear projection for predicted pixel values\n",
    "\n",
    "        pred_pixel_values = self.to_pixels(encoded_mask_tokens)\n",
    "\n",
    "        # get the masked patches for the final reconstruction loss\n",
    "\n",
    "        masked_patches = patches[batch_range, masked_indices]\n",
    "\n",
    "        # calculate reconstruction loss\n",
    "\n",
    "        recon_loss = F.l1_loss(pred_pixel_values, masked_patches) / num_masked\n",
    "        return recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def dropout_layers(layers, dropout):\n",
    "    if dropout == 0:\n",
    "        return layers\n",
    "\n",
    "    num_layers = len(layers)\n",
    "    to_drop = torch.zeros(num_layers).uniform_(0., 1.) < dropout\n",
    "\n",
    "    # make sure at least one layer makes it\n",
    "    if all(to_drop):\n",
    "        rand_index = randrange(num_layers)\n",
    "        to_drop[rand_index] = False\n",
    "\n",
    "    layers = [layer for (layer, drop) in zip(layers, to_drop) if not drop]\n",
    "    return layers\n",
    "\n",
    "# classes\n",
    "\n",
    "class LayerScale(nn.Module):\n",
    "    def __init__(self, dim, fn, depth):\n",
    "        super().__init__()\n",
    "        if depth <= 18:  # epsilon detailed in section 2 of paper\n",
    "            init_eps = 0.1\n",
    "        elif depth > 18 and depth <= 24:\n",
    "            init_eps = 1e-5\n",
    "        else:\n",
    "            init_eps = 1e-6\n",
    "\n",
    "        scale = torch.zeros(1, 1, dim).fill_(init_eps)\n",
    "        self.scale = nn.Parameter(scale)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) * self.scale\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.mix_heads_pre_attn = nn.Parameter(torch.randn(heads, heads))\n",
    "        self.mix_heads_post_attn = nn.Parameter(torch.randn(heads, heads))\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context = None):\n",
    "        \n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "\n",
    "        context = x if not exists(context) else torch.cat((x, context), dim = 1)\n",
    "\n",
    "        qkv = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        dots = einsum('b h i j, h g -> b g i j', dots, self.mix_heads_pre_attn)    # talking heads, pre-softmax\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        attn = einsum('b h i j, h g -> b g i j', attn, self.mix_heads_post_attn)   # talking heads, post-softmax\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0., layer_dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.layer_dropout = layer_dropout\n",
    "\n",
    "        for ind in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                LayerScale(dim, PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)), depth = ind + 1),\n",
    "                LayerScale(dim, PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout)), depth = ind + 1)\n",
    "            ]))\n",
    "    def forward(self, x, context = None):\n",
    "        layers = dropout_layers(self.layers, dropout = self.layer_dropout)\n",
    "\n",
    "        for attn, ff in layers:\n",
    "            x = attn(x, context = context) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class CaiT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        image_size,\n",
    "        patch_size,\n",
    "        num_classes,\n",
    "        dim,\n",
    "        depth,\n",
    "        cross_depth,\n",
    "        cls_depth,\n",
    "        heads,\n",
    "        mlp_dim,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.,\n",
    "        emb_dropout = 0.,\n",
    "        layer_dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = 3 * patch_size ** 2\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim))\n",
    "        self.pos_embedding_q, self.pos_embedding_r = nn.Parameter(torch.randn(1, 1, dim)), nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "\n",
    "        self.patch_transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout, layer_dropout)\n",
    "        #self.cross_transformer = Transformer(dim, cross_depth, heads, dim_head, mlp_dim, dropout, layer_dropout)\n",
    "        self.cls_transformer = Transformer(dim, cls_depth, heads, dim_head, mlp_dim, dropout, layer_dropout)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, query, reference):\n",
    "        \n",
    "        xq, xr = self.to_patch_embedding(query), self.to_patch_embedding(reference)\n",
    "        xq, xr = xq + self.pos_embedding + self.pos_embedding_q, xr + self.pos_embedding + self.pos_embedding_r\n",
    "\n",
    "        xq, xr = self.patch_transformer(xq), self.patch_transformer(xr)\n",
    "        \n",
    "        x = torch.cat([xq, xr], dim = 1)\n",
    "        #x = self.cross_transformer(x)\n",
    "        \n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = self.cls_transformer(cls_tokens, context = x)\n",
    "\n",
    "        return self.mlp_head(x[:, 0])\n",
    "    \n",
    "def vit_concat():\n",
    "    return CaiT(image_size=256, patch_size=32, num_classes=1, dim=512, depth=4, cross_depth=3, cls_depth=4, heads=16, mlp_dim=1024, dim_head = 64, dropout = 0., emb_dropout = 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vit_concat().to(device)\n",
    "\n",
    "#ckpt = torch.load('/kaggle/input/ckptttt/net (3).pt')\n",
    "#model.load_state_dict(ckpt['model_state_dict'], strict=False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-5)\n",
    "#optimizer.load_state_dict(ckpt['optimizer_state_dict'], )\n",
    "#opt = torch.optim.SGD(model.parameters(), lr = .05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CaiT(\n",
       "  (to_patch_embedding): Sequential(\n",
       "    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=32, p2=32)\n",
       "    (1): Linear(in_features=3072, out_features=512, bias=True)\n",
       "  )\n",
       "  (patch_transformer): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): LayerScale(\n",
       "          (fn): PreNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (to_kv): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (attend): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): LayerScale(\n",
       "          (fn): PreNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): LayerScale(\n",
       "          (fn): PreNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (to_kv): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (attend): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): LayerScale(\n",
       "          (fn): PreNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): LayerScale(\n",
       "          (fn): PreNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (to_kv): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (attend): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): LayerScale(\n",
       "          (fn): PreNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): ModuleList(\n",
       "        (0): LayerScale(\n",
       "          (fn): PreNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (to_kv): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (attend): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): LayerScale(\n",
       "          (fn): PreNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls_transformer): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): LayerScale(\n",
       "          (fn): PreNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (to_kv): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (attend): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): LayerScale(\n",
       "          (fn): PreNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): LayerScale(\n",
       "          (fn): PreNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (to_kv): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (attend): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): LayerScale(\n",
       "          (fn): PreNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): LayerScale(\n",
       "          (fn): PreNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (to_kv): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (attend): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): LayerScale(\n",
       "          (fn): PreNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): ModuleList(\n",
       "        (0): LayerScale(\n",
       "          (fn): PreNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): Attention(\n",
       "              (to_q): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (to_kv): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (attend): Softmax(dim=-1)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (to_out): Sequential(\n",
       "                (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): LayerScale(\n",
       "          (fn): PreNorm(\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fn): FeedForward(\n",
       "              (net): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "                (1): GELU()\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "                (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp_head): Sequential(\n",
       "    (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  7.72it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  7.63it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.33it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.31it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.83it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.64it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.68it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  9.12it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.89it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.93it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.97it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.85it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.91it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.89it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.89it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.80it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.61it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  9.05it/s]\n",
      " 50%|█████     | 1/2 [00:00<00:00,  4.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\repos\\belugas\\notebooks\\bce_vit_concat.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000023?line=11'>12</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000023?line=12'>13</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000023?line=13'>14</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000023?line=14'>15</a>\u001b[0m run[\u001b[39m'\u001b[39m\u001b[39mpretrain/running/loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mlog(loss)\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/optimizer.py?line=85'>86</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\torch\\optim\\adamw.py:145\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/adamw.py?line=141'>142</a>\u001b[0m         \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/adamw.py?line=142'>143</a>\u001b[0m         state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/adamw.py?line=144'>145</a>\u001b[0m     F\u001b[39m.\u001b[39;49madamw(params_with_grad,\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/adamw.py?line=145'>146</a>\u001b[0m             grads,\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/adamw.py?line=146'>147</a>\u001b[0m             exp_avgs,\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/adamw.py?line=147'>148</a>\u001b[0m             exp_avg_sqs,\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/adamw.py?line=148'>149</a>\u001b[0m             max_exp_avg_sqs,\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/adamw.py?line=149'>150</a>\u001b[0m             state_steps,\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/adamw.py?line=150'>151</a>\u001b[0m             amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/adamw.py?line=151'>152</a>\u001b[0m             beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/adamw.py?line=152'>153</a>\u001b[0m             beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/adamw.py?line=153'>154</a>\u001b[0m             lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/adamw.py?line=154'>155</a>\u001b[0m             weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/adamw.py?line=155'>156</a>\u001b[0m             eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/adamw.py?line=156'>157</a>\u001b[0m             maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/adamw.py?line=158'>159</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\torch\\optim\\_functional.py:137\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/_functional.py?line=133'>134</a>\u001b[0m step \u001b[39m=\u001b[39m state_steps[i]\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/_functional.py?line=135'>136</a>\u001b[0m \u001b[39m# Perform stepweight decay\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/_functional.py?line=136'>137</a>\u001b[0m param\u001b[39m.\u001b[39;49mmul_(\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m lr \u001b[39m*\u001b[39;49m weight_decay)\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/_functional.py?line=138'>139</a>\u001b[0m bias_correction1 \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m step\n\u001b[0;32m    <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/optim/_functional.py?line=139'>140</a>\u001b[0m bias_correction2 \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m step\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mim = SimMIM(\n",
    "    encoder = model,\n",
    "    masking_ratio = 0.5  # they found 50% to yield the best results\n",
    ").to(device)\n",
    "\n",
    "epochs = 5000\n",
    "for epoch_i in range(0, epochs):\n",
    "    model.train()\n",
    "    for images in tqdm(pretrain_dataloader):\n",
    "        images = images.to(device, non_blocking=True, dtype=input_dtype)\n",
    "        loss = mim(images)\n",
    "        optimizer.zero_grad(True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        run['pretrain/running/loss'].log(loss)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, f'../submission/net.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-5)\n",
    "#optimizer.load_state_dict(ckpt['optimizer_state_dict'], )\n",
    "\n",
    "optimizer = torch.optim.SGD([{'params':model.patch_transformer.parameters(), 'lr':5e-4},\n",
    "                            {'params':model.cls_transformer.parameters(), 'lr':5e-3},\n",
    "                            {'params':model.mlp_head.parameters(), 'lr':.05}]\n",
    "                            , lr = 5e-4, momentum=0.9)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaP:  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\repos\\belugas\\notebooks\\bce_vit_concat.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000010?line=18'>19</a>\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([torch\u001b[39m.\u001b[39mones(pos\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\u001b[39m1\u001b[39m), torch\u001b[39m.\u001b[39mzeros(neg\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\u001b[39m1\u001b[39m)], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000010?line=20'>21</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(enabled\u001b[39m=\u001b[39mfp16):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000010?line=21'>22</a>\u001b[0m     logits \u001b[39m=\u001b[39m model(query\u001b[39m=\u001b[39;49mquery, reference\u001b[39m=\u001b[39;49mreference)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000010?line=22'>23</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(logits, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000010?line=24'>25</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\repos\\belugas\\notebooks\\bce_vit_concat.ipynb Cell 9'\u001b[0m in \u001b[0;36mCaiT.forward\u001b[1;34m(self, query, reference)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000007?line=172'>173</a>\u001b[0m xq, xr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_patch_embedding(query), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_patch_embedding(reference)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000007?line=173'>174</a>\u001b[0m xq, xr \u001b[39m=\u001b[39m xq \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embedding \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embedding_q, xr \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embedding \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embedding_r\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000007?line=175'>176</a>\u001b[0m xq, xr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatch_transformer(xq), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch_transformer(xr)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000007?line=177'>178</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([xq, xr], dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000007?line=178'>179</a>\u001b[0m \u001b[39m#x = self.cross_transformer(x)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\repos\\belugas\\notebooks\\bce_vit_concat.ipynb Cell 9'\u001b[0m in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, context)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000007?line=122'>123</a>\u001b[0m layers \u001b[39m=\u001b[39m dropout_layers(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers, dropout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_dropout)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000007?line=124'>125</a>\u001b[0m \u001b[39mfor\u001b[39;00m attn, ff \u001b[39min\u001b[39;00m layers:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000007?line=125'>126</a>\u001b[0m     x \u001b[39m=\u001b[39m attn(x, context \u001b[39m=\u001b[39;49m context) \u001b[39m+\u001b[39m x\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000007?line=126'>127</a>\u001b[0m     x \u001b[39m=\u001b[39m ff(x) \u001b[39m+\u001b[39m x\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000007?line=127'>128</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\repos\\belugas\\notebooks\\bce_vit_concat.ipynb Cell 9'\u001b[0m in \u001b[0;36mLayerScale.forward\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000007?line=43'>44</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000007?line=44'>45</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn(x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\repos\\belugas\\notebooks\\bce_vit_concat.ipynb Cell 9'\u001b[0m in \u001b[0;36mPreNorm.forward\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000007?line=51'>52</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000007?line=52'>53</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\repos\\belugas\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/repos/belugas/env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\repos\\belugas\\notebooks\\bce_vit_concat.ipynb Cell 9'\u001b[0m in \u001b[0;36mAttention.forward\u001b[1;34m(self, x, context)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000007?line=92'>93</a>\u001b[0m context \u001b[39m=\u001b[39m x \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exists(context) \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mcat((x, context), dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000007?line=94'>95</a>\u001b[0m qkv \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_q(x), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_kv(context)\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, dim \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000007?line=95'>96</a>\u001b[0m q, k, v \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39;49m(\u001b[39mlambda\u001b[39;49;00m t: rearrange(t, \u001b[39m'\u001b[39;49m\u001b[39mb n (h d) -> b h n d\u001b[39;49m\u001b[39m'\u001b[39;49m, h \u001b[39m=\u001b[39;49m h), qkv)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000007?line=97'>98</a>\u001b[0m dots \u001b[39m=\u001b[39m einsum(\u001b[39m'\u001b[39m\u001b[39mb h i d, b h j d -> b h i j\u001b[39m\u001b[39m'\u001b[39m, q, k) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/belugas/notebooks/bce_vit_concat.ipynb#ch0000007?line=99'>100</a>\u001b[0m dots \u001b[39m=\u001b[39m einsum(\u001b[39m'\u001b[39m\u001b[39mb h i j, h g -> b g i j\u001b[39m\u001b[39m'\u001b[39m, dots, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmix_heads_pre_attn)    \u001b[39m# talking heads, pre-softmax\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error occurred during asynchronous operation processing: Cannot log infinite or NaN value to attribute epoch5/valid/map\n",
      "Info (NVML): The operating system has blocked the request.. GPU usage metrics may not be reported. For more information, see https://docs-legacy.neptune.ai/logging-and-managing-experiment-results/logging-experiment-data.html#hardware-consumption \n",
      "Exception in thread NeptuneReporting:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vcall\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py\", line 1009, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\neptune\\new\\internal\\threading\\daemon.py\", line 53, in run\n",
      "    self.work()\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\neptune\\new\\internal\\hardware\\hardware_metric_reporting_job.py\", line 119, in work\n",
      "    metric_reports = self._metric_reporter.report(time.time())\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\neptune\\internal\\hardware\\metrics\\reports\\metric_reporter.py\", line 32, in report\n",
      "    return [\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\neptune\\internal\\hardware\\metrics\\reports\\metric_reporter.py\", line 37, in <listcomp>\n",
      "    for x in [\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\neptune\\internal\\hardware\\metrics\\reports\\metric_reporter.py\", line 38, in <listcomp>\n",
      "    self.__metric_value_for_gauge(gauge, timestamp)\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\neptune\\internal\\hardware\\metrics\\reports\\metric_reporter.py\", line 48, in __metric_value_for_gauge\n",
      "    value = gauge.value()\n",
      "  File \"c:\\repos\\belugas\\env\\lib\\site-packages\\neptune\\internal\\hardware\\gauges\\gpu.py\", line 48, in value\n",
      "    return self.__gpu_monitor.get_card_used_memory_in_bytes(\n",
      "TypeError: unsupported operand type(s) for /: 'NoneType' and 'float'\n"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    epoch_loss, epoch_acc = 0, 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for anchor, pos, neg in tqdm(train_train_dataloader):\n",
    "\n",
    "        optimizer.zero_grad(True)\n",
    "        \n",
    "        anchor = anchor.to(device, non_blocking=True, dtype=input_dtype)\n",
    "        pos = pos.to(device, non_blocking=True, dtype=input_dtype)\n",
    "        neg = neg.to(device, non_blocking=True, dtype=input_dtype)\n",
    "        \n",
    "        query = torch.cat([anchor, anchor], dim=0)\n",
    "        reference = torch.cat([pos, neg], dim=0)\n",
    "        labels = torch.cat([torch.ones(pos.shape[0],1), torch.zeros(neg.shape[0],1)], dim=0).to(device)\n",
    "\n",
    "        logits = model(query=query, reference=reference)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        run['running/loss'].log(loss)\n",
    "        \n",
    "        # accuracy\n",
    "        preds = torch.nn.Sigmoid()(logits).round().detach().cpu().numpy()\n",
    "        acc = accuracy_score(labels.detach().cpu().numpy(), preds)\n",
    "        run['running/acc'].log(acc)\n",
    "        \n",
    "        epoch_loss += loss\n",
    "        epoch_acc += acc\n",
    "        \n",
    "    run['epoch/train/loss'].log(epoch_loss / len(train_train_dataloader))\n",
    "    run['epoch/train/acc'].log(epoch_acc / len(train_train_dataloader))\n",
    "    \n",
    "    if epoch_i % 20 == 0:\n",
    "        m_ap = map_score(valid_eval_dataloader, model)\n",
    "        run['epoch5/valid/map'].log(m_ap)\n",
    "        \n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, f'../artifacts/net.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, f'../submission/net.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal batch size for inference\n",
    "\"\"\"\n",
    "5vcpus\n",
    "52 ram\n",
    "12 ram gpu\n",
    "\n",
    "model to eval, optim bactch size calculate with tqdm\n",
    "\"\"\"\n",
    "model.eval()\n",
    "\n",
    "TEST_INFER_BS_INIT = 10\n",
    "TEST_INFER_BS_ML = 10\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "class DummyTest(torch.utils.data.Dataset):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def __len__(self):\n",
    "    return 7000000\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return torch.zeros((3,224,224)), torch.zeros((3,224,224))\n",
    "\n",
    "BS = TEST_INFER_BS_INIT = 5\n",
    "while True:\n",
    "  i = 0\n",
    "  BS = BS + TEST_INFER_BS_ML\n",
    "  dataloader = torch.utils.data.DataLoader(\n",
    "                          DummyTest(), \n",
    "                          batch_size=BS,\n",
    "                          shuffle=False, \n",
    "                          num_workers=0,\n",
    "                          pin_memory=True\n",
    "                      )   \n",
    "\n",
    "  for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "    query = batch[0].to(device, non_blocking=True, dtype=torch.float32)\n",
    "    reference = batch[1].to(device, non_blocking=True, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "      logits, attn, q_cls, r_cls = model(query=query, reference=reference)\n",
    "      i+=1\n",
    "    \n",
    "    if i == 50:\n",
    "      print(BS)\n",
    "      print(torch.cuda.mem_get_info(device=0))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6465060864, 8589737984)\n"
     ]
    }
   ],
   "source": [
    "fp16 = True \n",
    "input_dtype = torch.float16 if fp16 else torch.float32\n",
    "\n",
    "scaler =  torch.cuda.amp.GradScaler(enabled=fp16)\n",
    "\n",
    "model = crossvit_base_224().to(device)\n",
    "input = torch.zeros((2,3,224,224), dtype=input_dtype, device=device)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "labels = torch.ones((2,1), dtype=input_dtype, device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-4)\n",
    "\n",
    "warmup, reps = 30, 10\n",
    "\n",
    "for i in range(0, warmup):\n",
    "    \n",
    "        with torch.cuda.amp.autocast(enabled = fp16):\n",
    "            logits, attn, q_cls, r_cls = model(query=input, reference=input)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    profile_memory=True,\n",
    ") as prof:\n",
    "    print(torch.cuda.mem_get_info(0))\n",
    "    for i in range(0, reps):\n",
    "        \n",
    "        with torch.cuda.amp.autocast(enabled = fp16):\n",
    "            \n",
    "            logits, attn, q_cls, r_cls = model(query=input, reference=input)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          aten::reshape         5.00%     566.319ms        11.90%        1.346s     153.356us     544.024ms         4.84%        1.311s     149.324us           0 b           0 b     542.46 Mb           0 b          8780  \n",
      "                                            aten::copy_         4.72%     534.034ms         4.72%     534.034ms      78.650us     525.057ms         4.67%     525.057ms      77.328us           0 b           0 b           0 b           0 b          6790  \n",
      "                                         aten::_to_copy         4.56%     516.087ms        10.33%        1.170s     227.134us     503.597ms         4.48%        1.151s     223.463us           0 b           0 b       2.03 Gb           0 b          5150  \n",
      "                                       aten::as_strided         4.20%     475.203ms         4.20%     475.203ms      29.297us     453.851ms         4.04%     453.851ms      27.981us           0 b           0 b           0 b           0 b         16220  \n",
      "                                               aten::mm         4.04%     457.166ms         4.04%     457.166ms     155.499us     510.154ms         4.54%     510.154ms     173.522us           0 b           0 b       1.28 Gb       1.28 Gb          2940  \n",
      "                                          aten::permute         3.78%     427.307ms         5.55%     628.754ms      92.737us     439.648ms         3.91%     627.262ms      92.517us           0 b           0 b           0 b           0 b          6780  \n",
      "                                            aten::empty         3.60%     407.873ms         3.60%     407.873ms      74.159us     403.471ms         3.59%     403.471ms      73.358us           0 b           0 b       1.35 Gb       1.35 Gb          5500  \n",
      "                                        aten::transpose         3.07%     347.453ms         4.57%     517.742ms      87.902us     344.904ms         3.07%     519.313ms      88.169us           0 b           0 b           0 b           0 b          5890  \n",
      "                                    aten::empty_strided         2.93%     331.262ms         2.93%     331.262ms      50.420us     331.970ms         2.96%     331.970ms      50.528us           0 b           0 b       2.72 Gb       2.72 Gb          6570  \n",
      "                                                aten::t         2.80%     316.616ms         6.63%     750.914ms     152.315us     320.957ms         2.86%     755.833ms     153.313us           0 b           0 b           0 b           0 b          4930  \n",
      "                                   aten::_reshape_alias         2.50%     283.267ms         2.50%     283.267ms      37.469us     283.366ms         2.52%     283.366ms      37.482us           0 b           0 b           0 b           0 b          7560  \n",
      "                                               aten::to         2.50%     282.847ms        12.83%        1.453s     280.964us     295.169ms         2.63%        1.446s     279.691us           0 b           0 b       2.03 Gb           0 b          5170  \n",
      "                                             aten::add_         2.13%     240.690ms         2.13%     240.690ms      81.590us     244.176ms         2.17%     244.176ms      82.772us           0 b           0 b           0 b           0 b          2950  \n",
      "                                           aten::linear         2.09%     236.620ms        19.31%        2.186s       1.104ms     238.288ms         2.12%        2.180s       1.101ms           0 b           0 b       1.57 Gb           0 b          1980  \n",
      "                                              aten::bmm         2.00%     226.344ms         2.87%     324.686ms     193.265us     220.517ms         1.96%     317.646ms     189.075us           0 b           0 b     852.90 Mb     642.09 Mb          1680  \n",
      "                                       aten::empty_like         1.98%     223.804ms         4.65%     526.345ms     147.024us     220.125ms         1.96%     521.439ms     145.653us           0 b           0 b       1.79 Gb           0 b          3580  \n",
      "autograd::engine::evaluate_function: PermuteBackward...         1.94%     219.509ms         6.43%     727.753ms     215.312us     220.368ms         1.96%     725.153ms     214.542us           0 b           0 b           0 b           0 b          3380  \n",
      "                                           aten::einsum         1.77%     200.538ms         9.10%        1.030s       2.147ms     224.760ms         2.00%        1.032s       2.150ms           0 b           0 b     652.13 Mb           0 b           480  \n",
      "autograd::engine::evaluate_function: ToCopyBackward0...         1.77%     200.472ms         9.14%        1.034s     438.266us     193.696ms         1.72%        1.032s     437.249us           0 b           0 b     725.12 Mb    -703.96 Mb          2360  \n",
      "                                       PermuteBackward0         1.73%     195.666ms         4.49%     508.244ms     150.368us     188.534ms         1.68%     504.785ms     149.345us           0 b           0 b           0 b           0 b          3380  \n",
      "                 struct torch::autograd::AccumulateGrad         1.70%     192.112ms         3.38%     382.221ms     159.259us     187.491ms         1.67%     381.166ms     158.819us           0 b           0 b    -973.02 Mb    -973.02 Mb          2400  \n",
      "                                             aten::view         1.64%     185.266ms         1.64%     185.266ms      33.624us     184.722ms         1.64%     184.722ms      33.525us           0 b           0 b           0 b           0 b          5510  \n",
      "                                            MmBackward0         1.63%     184.982ms         8.02%     907.421ms     925.940us     187.506ms         1.67%     942.223ms     961.452us           0 b           0 b     746.62 Mb           0 b           980  \n",
      "autograd::engine::evaluate_function: ReshapeAliasBac...         1.36%     154.437ms         5.63%     636.799ms     289.454us     153.435ms         1.37%     635.240ms     288.745us           0 b           0 b           0 b    -161.60 Mb          2200  \n",
      "autograd::engine::evaluate_function: struct torch::a...         1.32%     149.725ms         4.70%     531.946ms     221.644us     140.895ms         1.25%     522.061ms     217.525us           0 b           0 b    -973.02 Mb           0 b          2400  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 11.319s\n",
      "Self CUDA time total: 11.233s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 1, 197])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layers x bs x (q,ref) x cross_attn_depth x n_heads x 1 x tokens(inc cls)\n",
    "attn[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 1, 197])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(attn))\n",
    "print(len(attn[0]))\n",
    "print(len(attn[0][0]))\n",
    "attn[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e460829be586a745d810aec71d83684bd38b76dd3b8d2db700ccf14d30953fce"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
